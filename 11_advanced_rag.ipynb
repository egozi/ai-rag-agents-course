{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Advanced RAG - Evaluation, Optimization & Production\n",
    "\n",
    "This notebook covers advanced techniques to improve RAG systems beyond the basic pipeline:\n",
    "\n",
    "1. **RAG Evaluation** - Measuring retrieval and generation quality\n",
    "2. **Query Transformation** - HyDE and query decomposition\n",
    "3. **Re-ranking** - Cross-encoder re-ranking for better precision\n",
    "4. **Hybrid Search** - Combining BM25 (keyword) with vector search\n",
    "5. **Failure Analysis** - Debugging RAG systems\n",
    "\n",
    "**Prerequisites:** Modules 9 (RAG Foundations) and 10 (RAG Pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb openai python-dotenv sentence-transformers rank-bm25 matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/amir/source/.env\")\n",
    "\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "client = OpenAI()\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Base\n",
    "\n",
    "Let's create a knowledge base of AI/ML documents to work with throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base: paragraphs about AI/ML topics with metadata\n",
    "documents = [\n",
    "    {\"id\": \"doc_0\", \"text\": \"Neural networks are computing systems inspired by biological neural networks in the brain. They consist of layers of interconnected nodes (neurons) that process information. Each connection has a weight that adjusts during training. Deep learning refers to neural networks with many hidden layers, enabling them to learn hierarchical representations of data.\", \"category\": \"deep_learning\", \"topic\": \"neural_networks\"},\n",
    "    {\"id\": \"doc_1\", \"text\": \"Convolutional Neural Networks (CNNs) are specialized for processing grid-like data such as images. They use convolutional layers that apply filters to detect features like edges, textures, and patterns. Pooling layers reduce spatial dimensions while preserving important features. CNNs have revolutionized computer vision, achieving superhuman performance on image classification tasks like ImageNet.\", \"category\": \"deep_learning\", \"topic\": \"cnn\"},\n",
    "    {\"id\": \"doc_2\", \"text\": \"Recurrent Neural Networks (RNNs) process sequential data by maintaining a hidden state that captures information from previous time steps. Long Short-Term Memory (LSTM) networks address the vanishing gradient problem with gating mechanisms. GRUs are a simplified variant. RNNs have been largely superseded by Transformers for most NLP tasks due to parallelization advantages.\", \"category\": \"deep_learning\", \"topic\": \"rnn\"},\n",
    "    {\"id\": \"doc_3\", \"text\": \"The Transformer architecture, introduced in 'Attention Is All You Need' (2017), relies entirely on self-attention mechanisms. It processes all positions in parallel, unlike RNNs. The architecture consists of encoder and decoder stacks, each with multi-head attention and feed-forward layers. Transformers are the foundation of modern LLMs like GPT, BERT, and T5.\", \"category\": \"deep_learning\", \"topic\": \"transformers\"},\n",
    "    {\"id\": \"doc_4\", \"text\": \"Transfer learning allows a model trained on one task to be fine-tuned for another. In NLP, pre-trained language models like BERT are fine-tuned on downstream tasks such as sentiment analysis, named entity recognition, and question answering. This approach dramatically reduces the amount of labeled data needed and improves performance on specialized tasks.\", \"category\": \"nlp\", \"topic\": \"transfer_learning\"},\n",
    "    {\"id\": \"doc_5\", \"text\": \"BERT (Bidirectional Encoder Representations from Transformers) uses masked language modeling for pre-training, randomly masking 15% of tokens and predicting them. Unlike GPT, BERT processes text bidirectionally, making it excellent for understanding tasks. BERT-base has 110M parameters and BERT-large has 340M parameters.\", \"category\": \"nlp\", \"topic\": \"bert\"},\n",
    "    {\"id\": \"doc_6\", \"text\": \"GPT (Generative Pre-trained Transformer) models use autoregressive (left-to-right) language modeling. GPT-3 has 175 billion parameters and demonstrated strong few-shot learning abilities. GPT-4 is multimodal, accepting both text and image inputs. The GPT family uses decoder-only transformer architecture with causal attention masking.\", \"category\": \"nlp\", \"topic\": \"gpt\"},\n",
    "    {\"id\": \"doc_7\", \"text\": \"Retrieval-Augmented Generation (RAG) combines a retriever that finds relevant documents with a generator that produces answers. RAG addresses LLM limitations including knowledge cutoff, hallucination, and lack of domain-specific knowledge. The retriever typically uses dense embeddings and vector similarity search to find relevant passages.\", \"category\": \"rag\", \"topic\": \"rag_overview\"},\n",
    "    {\"id\": \"doc_8\", \"text\": \"Vector databases store high-dimensional embedding vectors and support efficient similarity search. Popular options include ChromaDB (lightweight, local), Pinecone (cloud-native, managed), FAISS (Facebook's library for billion-scale search), and Weaviate (open-source, hybrid search). The choice depends on scale, deployment model, and feature requirements.\", \"category\": \"rag\", \"topic\": \"vector_databases\"},\n",
    "    {\"id\": \"doc_9\", \"text\": \"Chunking strategies significantly impact RAG quality. Fixed-size chunking splits text at character boundaries with overlap. Sentence-based chunking preserves semantic units. Recursive character splitting tries multiple separators (paragraphs, sentences, words). Semantic chunking groups sentences by embedding similarity. The optimal chunk size balances context (larger) with precision (smaller), typically 200-500 tokens.\", \"category\": \"rag\", \"topic\": \"chunking\"},\n",
    "    {\"id\": \"doc_10\", \"text\": \"Embedding models convert text into dense vector representations that capture semantic meaning. Sentence-BERT and its successors (E5, GTE, BGE) are trained with contrastive learning for semantic similarity. OpenAI's text-embedding-ada-002 and text-embedding-3-small provide high-quality embeddings via API. The choice of embedding model significantly affects retrieval quality.\", \"category\": \"rag\", \"topic\": \"embeddings\"},\n",
    "    {\"id\": \"doc_11\", \"text\": \"Reinforcement Learning from Human Feedback (RLHF) aligns LLMs with human preferences. The process involves supervised fine-tuning on demonstration data, training a reward model on human preference comparisons, and optimizing the LLM policy using PPO. RLHF was crucial in making ChatGPT helpful and safe. DPO (Direct Preference Optimization) offers a simpler alternative.\", \"category\": \"training\", \"topic\": \"rlhf\"},\n",
    "    {\"id\": \"doc_12\", \"text\": \"Prompt engineering is the practice of designing effective inputs for LLMs. Key techniques include zero-shot prompting, few-shot prompting with examples, chain-of-thought reasoning for complex problems, and role prompting to set behavior. Structured output formats like JSON can be enforced through careful prompt design and system messages.\", \"category\": \"nlp\", \"topic\": \"prompt_engineering\"},\n",
    "    {\"id\": \"doc_13\", \"text\": \"AI agents are autonomous systems that use LLMs to reason, plan, and take actions through tools. The ReAct pattern interleaves reasoning (Thought) with actions (Action/Observation). Multi-agent systems coordinate multiple specialized agents for complex tasks. LangGraph provides a framework for building agent workflows as state machines with conditional routing.\", \"category\": \"agents\", \"topic\": \"ai_agents\"},\n",
    "    {\"id\": \"doc_14\", \"text\": \"Fine-tuning adapts a pre-trained model to specific tasks or domains. Full fine-tuning updates all parameters but requires significant compute. Parameter-efficient methods like LoRA (Low-Rank Adaptation) freeze the base model and train small adapter matrices, reducing compute by 10-100x. QLoRA combines quantization with LoRA for even greater efficiency.\", \"category\": \"training\", \"topic\": \"fine_tuning\"},\n",
    "    {\"id\": \"doc_15\", \"text\": \"Attention mechanisms allow models to focus on relevant parts of the input. Self-attention computes query-key-value interactions within a sequence. Cross-attention allows one sequence to attend to another (e.g., decoder attending to encoder). Multi-head attention runs multiple attention computations in parallel, each learning different relationship types.\", \"category\": \"deep_learning\", \"topic\": \"attention\"},\n",
    "    {\"id\": \"doc_16\", \"text\": \"Tokenization converts text into numerical tokens for model processing. Subword tokenization methods like BPE (Byte Pair Encoding) and WordPiece balance vocabulary size with coverage. SentencePiece provides language-agnostic tokenization. Typical vocabulary sizes range from 30K (BERT) to 100K+ (GPT-4). Tokenization affects model performance, multilingual capability, and inference cost.\", \"category\": \"nlp\", \"topic\": \"tokenization\"},\n",
    "    {\"id\": \"doc_17\", \"text\": \"Evaluation metrics for RAG systems include retrieval metrics (precision@k, recall@k, MRR) and generation metrics (faithfulness, relevance, answer correctness). The RAGAS framework provides automated evaluation combining context relevance, answer faithfulness, and answer relevance. Human evaluation remains the gold standard for assessing RAG system quality.\", \"category\": \"rag\", \"topic\": \"evaluation\"},\n",
    "    {\"id\": \"doc_18\", \"text\": \"Cosine similarity measures the angle between two vectors, ranging from -1 to 1. It is the most common distance metric for comparing embeddings because it is invariant to vector magnitude. Dot product is faster but affected by magnitude. Euclidean distance measures absolute distance. For normalized vectors, cosine similarity equals dot product.\", \"category\": \"rag\", \"topic\": \"distance_metrics\"},\n",
    "    {\"id\": \"doc_19\", \"text\": \"Generative Adversarial Networks (GANs) consist of a generator and discriminator trained adversarially. The generator creates synthetic data while the discriminator distinguishes real from fake. GANs produce high-quality images but suffer from training instability and mode collapse. Diffusion models have largely replaced GANs for image generation due to better training stability and quality.\", \"category\": \"deep_learning\", \"topic\": \"gans\"}\n",
    "]\n",
    "\n",
    "# Embed and store in ChromaDB\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"ai_knowledge_base\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "ids = [doc[\"id\"] for doc in documents]\n",
    "metadatas = [{\"category\": doc[\"category\"], \"topic\": doc[\"topic\"]} for doc in documents]\n",
    "\n",
    "embeddings = embedding_model.encode(texts).tolist()\n",
    "\n",
    "collection.add(\n",
    "    documents=texts,\n",
    "    embeddings=embeddings,\n",
    "    ids=ids,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "print(f\"Knowledge base loaded: {collection.count()} documents\")\n",
    "print(f\"Categories: {set(doc['category'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Evaluation Metrics\n",
    "\n",
    "Before improving RAG, we need to **measure** how good it is. There are two types of metrics:\n",
    "\n",
    "### Retrieval Metrics\n",
    "- **Precision@k**: Of the k documents retrieved, how many are relevant?\n",
    "- **Recall@k**: Of all relevant documents, how many did we retrieve?\n",
    "- **MRR (Mean Reciprocal Rank)**: How high is the first relevant result ranked?\n",
    "\n",
    "### Generation Metrics\n",
    "- **Faithfulness**: Is the answer supported by the retrieved context?\n",
    "- **Relevance**: Does the answer address the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation dataset: questions with known relevant document IDs\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"question\": \"How do transformers work?\",\n",
    "        \"relevant_ids\": [\"doc_3\", \"doc_15\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is RAG and why is it needed?\",\n",
    "        \"relevant_ids\": [\"doc_7\", \"doc_8\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are GPT models trained?\",\n",
    "        \"relevant_ids\": [\"doc_6\", \"doc_11\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the best chunking strategies for RAG?\",\n",
    "        \"relevant_ids\": [\"doc_9\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between BERT and GPT?\",\n",
    "        \"relevant_ids\": [\"doc_5\", \"doc_6\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do embedding models work for search?\",\n",
    "        \"relevant_ids\": [\"doc_10\", \"doc_18\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is fine-tuning and LoRA?\",\n",
    "        \"relevant_ids\": [\"doc_14\", \"doc_4\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do AI agents use tools?\",\n",
    "        \"relevant_ids\": [\"doc_13\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Evaluation dataset: {len(eval_dataset)} questions\")\n",
    "for item in eval_dataset:\n",
    "    print(f\"  Q: {item['question']}\")\n",
    "    print(f\"     Relevant: {item['relevant_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement retrieval evaluation metrics\n",
    "\n",
    "def precision_at_k(retrieved_ids, relevant_ids, k):\n",
    "    \"\"\"What fraction of the top-k retrieved documents are relevant?\"\"\"\n",
    "    top_k = retrieved_ids[:k]\n",
    "    relevant_in_top_k = len(set(top_k) & set(relevant_ids))\n",
    "    return relevant_in_top_k / k\n",
    "\n",
    "def recall_at_k(retrieved_ids, relevant_ids, k):\n",
    "    \"\"\"What fraction of all relevant documents appear in the top-k?\"\"\"\n",
    "    top_k = retrieved_ids[:k]\n",
    "    relevant_in_top_k = len(set(top_k) & set(relevant_ids))\n",
    "    return relevant_in_top_k / len(relevant_ids) if relevant_ids else 0\n",
    "\n",
    "def reciprocal_rank(retrieved_ids, relevant_ids):\n",
    "    \"\"\"1/rank of the first relevant document.\"\"\"\n",
    "    for i, doc_id in enumerate(retrieved_ids):\n",
    "        if doc_id in relevant_ids:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "# Demo with a sample query\n",
    "sample_retrieved = [\"doc_3\", \"doc_2\", \"doc_15\", \"doc_5\", \"doc_6\"]\n",
    "sample_relevant = [\"doc_3\", \"doc_15\"]\n",
    "\n",
    "for k in [1, 3, 5]:\n",
    "    p = precision_at_k(sample_retrieved, sample_relevant, k)\n",
    "    r = recall_at_k(sample_retrieved, sample_relevant, k)\n",
    "    print(f\"k={k}: Precision@{k}={p:.3f}, Recall@{k}={r:.3f}\")\n",
    "\n",
    "rr = reciprocal_rank(sample_retrieved, sample_relevant)\n",
    "print(f\"Reciprocal Rank: {rr:.3f} (first relevant doc at position 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, collection, k=5):\n",
    "    \"\"\"Retrieve top-k documents from ChromaDB.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=k\n",
    "    )\n",
    "    return results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0]\n",
    "\n",
    "def evaluate_retrieval(eval_data, collection, k=5):\n",
    "    \"\"\"Evaluate retrieval across all questions.\"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    for item in eval_data:\n",
    "        retrieved_ids, _, _ = retrieve(item[\"question\"], collection, k)\n",
    "        \n",
    "        p = precision_at_k(retrieved_ids, item[\"relevant_ids\"], k)\n",
    "        r = recall_at_k(retrieved_ids, item[\"relevant_ids\"], k)\n",
    "        rr = reciprocal_rank(retrieved_ids, item[\"relevant_ids\"])\n",
    "        \n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        mrr_scores.append(rr)\n",
    "    \n",
    "    return {\n",
    "        f\"precision@{k}\": np.mean(precisions),\n",
    "        f\"recall@{k}\": np.mean(recalls),\n",
    "        \"mrr\": np.mean(mrr_scores)\n",
    "    }\n",
    "\n",
    "# Baseline evaluation\n",
    "baseline_metrics = evaluate_retrieval(eval_dataset, collection, k=3)\n",
    "print(\"Baseline Retrieval Metrics (k=3):\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build a RAG Evaluation Pipeline\n",
    "\n",
    "**Your Task:** Implement a comprehensive evaluation pipeline that measures both retrieval quality and a simple faithfulness check.\n",
    "\n",
    "**Steps:**\n",
    "1. Run retrieval for each question in the eval dataset\n",
    "2. Compute precision@k and MRR\n",
    "3. For faithfulness: check if key terms from the retrieved docs appear in the generated answer\n",
    "\n",
    "**Hints:**\n",
    "- Use the `evaluate_retrieval()` function as a starting point\n",
    "- For faithfulness, a simple keyword overlap ratio works as a rough approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_pipeline(eval_data, collection, k=3):\n",
    "    \"\"\"\n",
    "    Evaluate both retrieval and generation quality.\n",
    "    \n",
    "    Returns:\n",
    "        dict with precision@k, recall@k, mrr, and avg_faithfulness\n",
    "    \"\"\"\n",
    "    # TODO: Implement evaluation pipeline\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for item in eval_data:\n",
    "        # 1. Retrieve documents\n",
    "        retrieved_ids, retrieved_docs, _ = None, None, None  # Your code\n",
    "        \n",
    "        # 2. Compute retrieval metrics\n",
    "        p_at_k = None  # Your code\n",
    "        rr = None  # Your code\n",
    "        \n",
    "        # 3. Generate answer using retrieved context\n",
    "        context = None  # Your code: join retrieved_docs\n",
    "        answer = None  # Your code: call LLM with context + question\n",
    "        \n",
    "        # 4. Simple faithfulness: fraction of context keywords found in answer\n",
    "        faithfulness = None  # Your code\n",
    "        \n",
    "        results.append({\"p_at_k\": p_at_k, \"rr\": rr, \"faithfulness\": faithfulness})\n",
    "    \n",
    "    return {\n",
    "        f\"precision@{k}\": np.mean([r[\"p_at_k\"] for r in results]),\n",
    "        \"mrr\": np.mean([r[\"rr\"] for r in results]),\n",
    "        \"avg_faithfulness\": np.mean([r[\"faithfulness\"] for r in results])\n",
    "    }\n",
    "\n",
    "# Test\n",
    "# metrics = evaluate_rag_pipeline(eval_dataset[:3], collection, k=3)\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    \"\"\"Generate an answer using OpenAI given context.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Answer the question based on the provided context. Be concise.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def simple_faithfulness(context, answer):\n",
    "    \"\"\"Simple faithfulness: fraction of important context words found in answer.\"\"\"\n",
    "    context_words = set(context.lower().split())\n",
    "    answer_words = set(answer.lower().split())\n",
    "    # Filter to meaningful words (length > 4 to skip stopwords)\n",
    "    context_keywords = {w for w in context_words if len(w) > 4}\n",
    "    if not context_keywords:\n",
    "        return 1.0\n",
    "    overlap = context_keywords & answer_words\n",
    "    return len(overlap) / len(context_keywords)\n",
    "\n",
    "def evaluate_rag_pipeline(eval_data, collection, k=3):\n",
    "    \"\"\"Evaluate both retrieval and generation quality - SOLUTION.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for item in eval_data:\n",
    "        # 1. Retrieve documents\n",
    "        retrieved_ids, retrieved_docs, _ = retrieve(item[\"question\"], collection, k)\n",
    "        \n",
    "        # 2. Retrieval metrics\n",
    "        p_at_k = precision_at_k(retrieved_ids, item[\"relevant_ids\"], k)\n",
    "        rr = reciprocal_rank(retrieved_ids, item[\"relevant_ids\"])\n",
    "        \n",
    "        # 3. Generate answer\n",
    "        context = \"\\n\\n\".join(retrieved_docs)\n",
    "        answer = generate_answer(item[\"question\"], context)\n",
    "        \n",
    "        # 4. Faithfulness\n",
    "        faith = simple_faithfulness(context, answer)\n",
    "        \n",
    "        results.append({\"p_at_k\": p_at_k, \"rr\": rr, \"faithfulness\": faith,\n",
    "                       \"question\": item[\"question\"], \"answer\": answer[:100]})\n",
    "        print(f\"Q: {item['question']}\")\n",
    "        print(f\"  P@{k}={p_at_k:.2f}, RR={rr:.2f}, Faith={faith:.2f}\")\n",
    "        print(f\"  A: {answer[:100]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        f\"precision@{k}\": np.mean([r[\"p_at_k\"] for r in results]),\n",
    "        \"mrr\": np.mean([r[\"rr\"] for r in results]),\n",
    "        \"avg_faithfulness\": np.mean([r[\"faithfulness\"] for r in results])\n",
    "    }\n",
    "\n",
    "metrics = evaluate_rag_pipeline(eval_dataset[:4], collection, k=3)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Overall Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Query Transformation: HyDE\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** improves retrieval by:\n",
    "1. Asking the LLM to generate a **hypothetical answer** to the question\n",
    "2. Embedding that hypothetical answer (instead of the raw question)\n",
    "3. Using the hypothetical answer's embedding for retrieval\n",
    "\n",
    "**Intuition:** The hypothetical answer is stylistically closer to the actual documents in the knowledge base than a short question would be. This bridges the \"query-document gap.\"\n",
    "\n",
    "```\n",
    "Standard:  \"How do transformers work?\" → embed question → search\n",
    "HyDE:      \"How do transformers work?\" → LLM generates paragraph → embed paragraph → search\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_retrieve(question, collection, k=5):\n",
    "    \"\"\"\n",
    "    HyDE: Generate hypothetical document, embed it, use for retrieval.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate hypothetical answer\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Write a detailed paragraph answering the question. Write as if this is from a textbook. Do not say 'I don't know'.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    hypothetical_doc = response.choices[0].message.content\n",
    "    \n",
    "    # Step 2: Embed the hypothetical document (not the question!)\n",
    "    hyde_embedding = embedding_model.encode([hypothetical_doc]).tolist()\n",
    "    \n",
    "    # Step 3: Search with hypothetical document's embedding\n",
    "    results = collection.query(\n",
    "        query_embeddings=hyde_embedding,\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    return results[\"ids\"][0], results[\"documents\"][0], results[\"distances\"][0], hypothetical_doc\n",
    "\n",
    "# Compare standard vs HyDE retrieval\n",
    "test_question = \"What approaches exist for making LLMs generate better outputs?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STANDARD RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "std_ids, std_docs, std_dists = retrieve(test_question, collection, k=3)\n",
    "for i, (doc_id, doc, dist) in enumerate(zip(std_ids, std_docs, std_dists)):\n",
    "    print(f\"\\n{i+1}. [{doc_id}] (dist={dist:.3f})\")\n",
    "    print(f\"   {doc[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HyDE RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "hyde_ids, hyde_docs, hyde_dists, hypo = hyde_retrieve(test_question, collection, k=3)\n",
    "print(f\"\\nHypothetical document:\\n{hypo[:200]}...\\n\")\n",
    "for i, (doc_id, doc, dist) in enumerate(zip(hyde_ids, hyde_docs, hyde_dists)):\n",
    "    print(f\"{i+1}. [{doc_id}] (dist={dist:.3f})\")\n",
    "    print(f\"   {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement HyDE and Compare Retrieval Quality\n",
    "\n",
    "**Your Task:** Evaluate HyDE vs standard retrieval across the full evaluation dataset.\n",
    "\n",
    "**Steps:**\n",
    "1. Run both standard and HyDE retrieval for each question\n",
    "2. Compute precision@3 and MRR for both\n",
    "3. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hyde_vs_standard(eval_data, collection, k=3):\n",
    "    \"\"\"\n",
    "    Compare standard retrieval vs HyDE on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison\n",
    "    \n",
    "    standard_results = []\n",
    "    hyde_results = []\n",
    "    \n",
    "    for item in eval_data:\n",
    "        # Standard retrieval\n",
    "        std_ids = None  # Your code\n",
    "        std_p = None    # Your code: precision@k\n",
    "        std_rr = None   # Your code: reciprocal rank\n",
    "        \n",
    "        # HyDE retrieval\n",
    "        hyde_ids = None  # Your code\n",
    "        hyde_p = None    # Your code\n",
    "        hyde_rr = None   # Your code\n",
    "        \n",
    "        standard_results.append({\"p\": std_p, \"rr\": std_rr})\n",
    "        hyde_results.append({\"p\": hyde_p, \"rr\": hyde_rr})\n",
    "    \n",
    "    # Print comparison\n",
    "    pass  # Your code\n",
    "\n",
    "# compare_hyde_vs_standard(eval_dataset[:4], collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hyde_vs_standard(eval_data, collection, k=3):\n",
    "    \"\"\"Compare standard retrieval vs HyDE - SOLUTION.\"\"\"\n",
    "    standard_results = []\n",
    "    hyde_results = []\n",
    "    \n",
    "    for item in eval_data:\n",
    "        question = item[\"question\"]\n",
    "        relevant = item[\"relevant_ids\"]\n",
    "        \n",
    "        # Standard retrieval\n",
    "        std_ids, _, _ = retrieve(question, collection, k)\n",
    "        std_p = precision_at_k(std_ids, relevant, k)\n",
    "        std_rr = reciprocal_rank(std_ids, relevant)\n",
    "        \n",
    "        # HyDE retrieval\n",
    "        hyde_ids, _, _, _ = hyde_retrieve(question, collection, k)\n",
    "        hyde_p = precision_at_k(hyde_ids, relevant, k)\n",
    "        hyde_rr = reciprocal_rank(hyde_ids, relevant)\n",
    "        \n",
    "        standard_results.append({\"p\": std_p, \"rr\": std_rr})\n",
    "        hyde_results.append({\"p\": hyde_p, \"rr\": hyde_rr})\n",
    "        \n",
    "        better = \"HyDE\" if hyde_p > std_p else (\"Standard\" if std_p > hyde_p else \"Tie\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"  Standard P@{k}={std_p:.2f} | HyDE P@{k}={hyde_p:.2f} | Winner: {better}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Average P@{k} - Standard: {np.mean([r['p'] for r in standard_results]):.3f}\")\n",
    "    print(f\"Average P@{k} - HyDE:     {np.mean([r['p'] for r in hyde_results]):.3f}\")\n",
    "    print(f\"Average MRR  - Standard: {np.mean([r['rr'] for r in standard_results]):.3f}\")\n",
    "    print(f\"Average MRR  - HyDE:     {np.mean([r['rr'] for r in hyde_results]):.3f}\")\n",
    "\n",
    "compare_hyde_vs_standard(eval_dataset[:4], collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Re-ranking with Cross-Encoders\n",
    "\n",
    "Retrieval uses **bi-encoders** (encode query and documents separately) which are fast but less accurate. **Cross-encoders** process the query and document together, giving much more accurate relevance scores but are slower.\n",
    "\n",
    "**Pipeline:** Bi-encoder retrieves top-20 candidates → Cross-encoder re-ranks → Return top-5\n",
    "\n",
    "```\n",
    "Stage 1 (fast):  Bi-encoder retrieves ~20 candidates from millions\n",
    "Stage 2 (accurate):  Cross-encoder re-scores 20 candidates\n",
    "Final: Return top-5 after re-ranking\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a cross-encoder model for re-ranking\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def retrieve_and_rerank(query, collection, initial_k=10, final_k=3):\n",
    "    \"\"\"\n",
    "    Two-stage retrieval: bi-encoder retrieval + cross-encoder re-ranking.\n",
    "    \"\"\"\n",
    "    # Stage 1: Bi-encoder retrieval (fast, broad)\n",
    "    retrieved_ids, retrieved_docs, distances = retrieve(query, collection, k=initial_k)\n",
    "    \n",
    "    # Stage 2: Cross-encoder re-ranking (slow, accurate)\n",
    "    # Cross-encoder takes (query, document) pairs and scores them\n",
    "    pairs = [[query, doc] for doc in retrieved_docs]\n",
    "    cross_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Sort by cross-encoder score (higher is better)\n",
    "    ranked_indices = np.argsort(cross_scores)[::-1]\n",
    "    \n",
    "    reranked_ids = [retrieved_ids[i] for i in ranked_indices[:final_k]]\n",
    "    reranked_docs = [retrieved_docs[i] for i in ranked_indices[:final_k]]\n",
    "    reranked_scores = [cross_scores[i] for i in ranked_indices[:final_k]]\n",
    "    \n",
    "    return reranked_ids, reranked_docs, reranked_scores\n",
    "\n",
    "# Compare standard vs re-ranked results\n",
    "query = \"How do transformers use attention?\"\n",
    "\n",
    "print(\"BEFORE RE-RANKING (bi-encoder only):\")\n",
    "ids, docs, dists = retrieve(query, collection, k=5)\n",
    "for i, (doc_id, dist) in enumerate(zip(ids, dists)):\n",
    "    topic = next(d[\"topic\"] for d in documents if d[\"id\"] == doc_id)\n",
    "    print(f\"  {i+1}. [{doc_id}] topic={topic}, dist={dist:.3f}\")\n",
    "\n",
    "print(\"\\nAFTER RE-RANKING (bi-encoder + cross-encoder):\")\n",
    "ids_rr, docs_rr, scores_rr = retrieve_and_rerank(query, collection, initial_k=10, final_k=5)\n",
    "for i, (doc_id, score) in enumerate(zip(ids_rr, scores_rr)):\n",
    "    topic = next(d[\"topic\"] for d in documents if d[\"id\"] == doc_id)\n",
    "    print(f\"  {i+1}. [{doc_id}] topic={topic}, cross-score={score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Evaluate Re-ranking Impact\n",
    "\n",
    "**Your Task:** Add cross-encoder re-ranking to the retrieval pipeline and measure the improvement.\n",
    "\n",
    "**Steps:**\n",
    "1. Run standard retrieval (bi-encoder only) on eval dataset\n",
    "2. Run re-ranked retrieval on the same questions\n",
    "3. Compare precision@3 and MRR for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranking(eval_data, collection, initial_k=10, final_k=3):\n",
    "    \"\"\"\n",
    "    Compare retrieval with and without re-ranking.\n",
    "    \"\"\"\n",
    "    # TODO: Implement evaluation\n",
    "    \n",
    "    for item in eval_data:\n",
    "        # Standard retrieval\n",
    "        std_ids = None  # Your code\n",
    "        \n",
    "        # Re-ranked retrieval\n",
    "        reranked_ids = None  # Your code\n",
    "        \n",
    "        # Compare metrics\n",
    "        pass  # Your code\n",
    "\n",
    "# evaluate_reranking(eval_dataset[:4], collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranking(eval_data, collection, initial_k=10, final_k=3):\n",
    "    \"\"\"Compare retrieval with and without re-ranking - SOLUTION.\"\"\"\n",
    "    std_metrics = {\"precision\": [], \"mrr\": []}\n",
    "    rr_metrics = {\"precision\": [], \"mrr\": []}\n",
    "    \n",
    "    for item in eval_data:\n",
    "        relevant = item[\"relevant_ids\"]\n",
    "        \n",
    "        # Standard retrieval (top final_k only)\n",
    "        std_ids, _, _ = retrieve(item[\"question\"], collection, k=final_k)\n",
    "        std_metrics[\"precision\"].append(precision_at_k(std_ids, relevant, final_k))\n",
    "        std_metrics[\"mrr\"].append(reciprocal_rank(std_ids, relevant))\n",
    "        \n",
    "        # Re-ranked retrieval\n",
    "        rr_ids, _, _ = retrieve_and_rerank(item[\"question\"], collection, initial_k, final_k)\n",
    "        rr_metrics[\"precision\"].append(precision_at_k(rr_ids, relevant, final_k))\n",
    "        rr_metrics[\"mrr\"].append(reciprocal_rank(rr_ids, relevant))\n",
    "        \n",
    "        print(f\"Q: {item['question']}\")\n",
    "        print(f\"  Standard: P@{final_k}={std_metrics['precision'][-1]:.2f}\")\n",
    "        print(f\"  Reranked: P@{final_k}={rr_metrics['precision'][-1]:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"Standard - Avg P@{final_k}: {np.mean(std_metrics['precision']):.3f}, MRR: {np.mean(std_metrics['mrr']):.3f}\")\n",
    "    print(f\"Reranked - Avg P@{final_k}: {np.mean(rr_metrics['precision']):.3f}, MRR: {np.mean(rr_metrics['mrr']):.3f}\")\n",
    "\n",
    "evaluate_reranking(eval_dataset, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Hybrid Search: BM25 + Vector Search\n",
    "\n",
    "**BM25** is a keyword-based ranking algorithm (like a smart version of TF-IDF). It excels at exact keyword matches.\n",
    "\n",
    "**Vector search** excels at semantic meaning but can miss exact keywords.\n",
    "\n",
    "**Hybrid search** combines both for the best of both worlds.\n",
    "\n",
    "We combine rankings using **Reciprocal Rank Fusion (RRF)**:\n",
    "\n",
    "$$\\text{RRF}(d) = \\sum_{r \\in \\text{rankers}} \\frac{1}{k + \\text{rank}_r(d)}$$\n",
    "\n",
    "where k is typically 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 setup\n",
    "tokenized_docs = [doc.lower().split() for doc in texts]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def bm25_retrieve(query, k=5):\n",
    "    \"\"\"Retrieve using BM25 keyword matching.\"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "    return (\n",
    "        [ids[i] for i in top_k_indices],\n",
    "        [texts[i] for i in top_k_indices],\n",
    "        [scores[i] for i in top_k_indices]\n",
    "    )\n",
    "\n",
    "def reciprocal_rank_fusion(rankings, k=60):\n",
    "    \"\"\"\n",
    "    Combine multiple rankings using Reciprocal Rank Fusion.\n",
    "    \n",
    "    Args:\n",
    "        rankings: list of lists of doc_ids (each list is a ranking)\n",
    "        k: constant (typically 60)\n",
    "    Returns:\n",
    "        list of doc_ids sorted by fused score\n",
    "    \"\"\"\n",
    "    fused_scores = {}\n",
    "    for ranking in rankings:\n",
    "        for rank, doc_id in enumerate(ranking):\n",
    "            if doc_id not in fused_scores:\n",
    "                fused_scores[doc_id] = 0\n",
    "            fused_scores[doc_id] += 1.0 / (k + rank + 1)\n",
    "    \n",
    "    sorted_docs = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [doc_id for doc_id, score in sorted_docs]\n",
    "\n",
    "def hybrid_search(query, collection, k=5):\n",
    "    \"\"\"Combine BM25 and vector search with RRF.\"\"\"\n",
    "    # Get both rankings\n",
    "    vector_ids, _, _ = retrieve(query, collection, k=10)\n",
    "    bm25_ids, _, _ = bm25_retrieve(query, k=10)\n",
    "    \n",
    "    # Fuse rankings\n",
    "    fused_ids = reciprocal_rank_fusion([vector_ids, bm25_ids])[:k]\n",
    "    fused_docs = [texts[ids.index(doc_id)] for doc_id in fused_ids]\n",
    "    \n",
    "    return fused_ids, fused_docs\n",
    "\n",
    "# Compare all three approaches\n",
    "query = \"What vector databases are available for storing embeddings?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "print(\"BM25 (keyword):\")\n",
    "bm25_ids, _, bm25_scores = bm25_retrieve(query, k=3)\n",
    "for i, (doc_id, score) in enumerate(zip(bm25_ids, bm25_scores)):\n",
    "    print(f\"  {i+1}. [{doc_id}] score={score:.2f}\")\n",
    "\n",
    "print(\"\\nVector Search (semantic):\")\n",
    "vec_ids, _, vec_dists = retrieve(query, collection, k=3)\n",
    "for i, (doc_id, dist) in enumerate(zip(vec_ids, vec_dists)):\n",
    "    print(f\"  {i+1}. [{doc_id}] dist={dist:.3f}\")\n",
    "\n",
    "print(\"\\nHybrid (BM25 + Vector + RRF):\")\n",
    "hybrid_ids, _ = hybrid_search(query, collection, k=3)\n",
    "for i, doc_id in enumerate(hybrid_ids):\n",
    "    print(f\"  {i+1}. [{doc_id}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Evaluate Hybrid Search\n",
    "\n",
    "**Your Task:** Compare BM25-only, vector-only, and hybrid search across the evaluation dataset.\n",
    "\n",
    "**Steps:**\n",
    "1. Run all three retrieval methods on each question\n",
    "2. Compute precision@3 and MRR for each\n",
    "3. Create a bar chart comparing the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods(eval_data, collection, k=3):\n",
    "    \"\"\"\n",
    "    Compare BM25, vector search, and hybrid search.\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison\n",
    "    \n",
    "    methods = {\"BM25\": [], \"Vector\": [], \"Hybrid\": []}\n",
    "    \n",
    "    for item in eval_data:\n",
    "        relevant = item[\"relevant_ids\"]\n",
    "        \n",
    "        # BM25\n",
    "        bm25_result_ids = None  # Your code\n",
    "        \n",
    "        # Vector\n",
    "        vec_result_ids = None  # Your code\n",
    "        \n",
    "        # Hybrid\n",
    "        hybrid_result_ids = None  # Your code\n",
    "        \n",
    "        # Compute precision@k for each\n",
    "        pass  # Your code\n",
    "    \n",
    "    # Plot comparison\n",
    "    pass  # Your code: bar chart\n",
    "\n",
    "# compare_search_methods(eval_dataset, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods(eval_data, collection, k=3):\n",
    "    \"\"\"Compare BM25, vector search, and hybrid search - SOLUTION.\"\"\"\n",
    "    methods = {\n",
    "        \"BM25\": {\"precision\": [], \"mrr\": []},\n",
    "        \"Vector\": {\"precision\": [], \"mrr\": []},\n",
    "        \"Hybrid\": {\"precision\": [], \"mrr\": []}\n",
    "    }\n",
    "    \n",
    "    for item in eval_data:\n",
    "        relevant = item[\"relevant_ids\"]\n",
    "        \n",
    "        # BM25\n",
    "        bm25_result_ids, _, _ = bm25_retrieve(item[\"question\"], k=k)\n",
    "        methods[\"BM25\"][\"precision\"].append(precision_at_k(bm25_result_ids, relevant, k))\n",
    "        methods[\"BM25\"][\"mrr\"].append(reciprocal_rank(bm25_result_ids, relevant))\n",
    "        \n",
    "        # Vector\n",
    "        vec_result_ids, _, _ = retrieve(item[\"question\"], collection, k=k)\n",
    "        methods[\"Vector\"][\"precision\"].append(precision_at_k(vec_result_ids, relevant, k))\n",
    "        methods[\"Vector\"][\"mrr\"].append(reciprocal_rank(vec_result_ids, relevant))\n",
    "        \n",
    "        # Hybrid\n",
    "        hybrid_result_ids, _ = hybrid_search(item[\"question\"], collection, k=k)\n",
    "        methods[\"Hybrid\"][\"precision\"].append(precision_at_k(hybrid_result_ids, relevant, k))\n",
    "        methods[\"Hybrid\"][\"mrr\"].append(reciprocal_rank(hybrid_result_ids, relevant))\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{'Method':<10} {'Avg P@'+str(k):<12} {'Avg MRR':<10}\")\n",
    "    print(\"-\" * 32)\n",
    "    for method, scores in methods.items():\n",
    "        avg_p = np.mean(scores[\"precision\"])\n",
    "        avg_mrr = np.mean(scores[\"mrr\"])\n",
    "        print(f\"{method:<10} {avg_p:<12.3f} {avg_mrr:<10.3f}\")\n",
    "    \n",
    "    # Bar chart\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    method_names = list(methods.keys())\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    \n",
    "    for ax, metric_name in zip(axes, [f\"precision\", \"mrr\"]):\n",
    "        values = [np.mean(methods[m][metric_name]) for m in method_names]\n",
    "        bars = ax.bar(method_names, values, color=colors)\n",
    "        ax.set_title(f\"Average {metric_name.upper()}\" if metric_name == \"mrr\" else f\"Average Precision@{k}\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        for bar, val in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                   f\"{val:.3f}\", ha='center', fontsize=11)\n",
    "    \n",
    "    plt.suptitle(\"Search Method Comparison\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_search_methods(eval_dataset, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Failure Analysis & Debugging RAG\n",
    "\n",
    "RAG systems can fail at multiple points. Here's a systematic debugging approach:\n",
    "\n",
    "### Common Failure Modes\n",
    "\n",
    "| Failure | Symptom | Root Cause | Fix |\n",
    "|---------|---------|------------|-----|\n",
    "| Wrong chunks retrieved | Answer about wrong topic | Embedding model mismatch, poor chunking | Better embeddings, adjust chunk size |\n",
    "| Right chunks, wrong answer | Plausible but incorrect answer | LLM hallucination, prompt issue | Better prompt, lower temperature |\n",
    "| Chunks too large | Answer misses details | Large chunks dilute signal | Smaller chunks with overlap |\n",
    "| Chunks too small | Missing context | Small chunks lose coherence | Larger chunks, parent-child strategy |\n",
    "| Query mismatch | No relevant results | User query != document language | Query expansion, HyDE |\n",
    "\n",
    "### Debugging Checklist\n",
    "\n",
    "1. **Check retrieval first** - Are the right documents being found?\n",
    "2. **Check chunk quality** - Are chunks coherent and informative?\n",
    "3. **Check the prompt** - Is the context + question formatted well?\n",
    "4. **Check the answer** - Is the LLM following instructions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_rag_query(question, collection, k=3):\n",
    "    \"\"\"Debug a single RAG query step by step.\"\"\"\n",
    "    print(f\"DEBUGGING RAG QUERY\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Check retrieval\n",
    "    print(\"\\n[1] RETRIEVAL RESULTS:\")\n",
    "    retrieved_ids, retrieved_docs, distances = retrieve(question, collection, k)\n",
    "    for i, (doc_id, doc, dist) in enumerate(zip(retrieved_ids, retrieved_docs, distances)):\n",
    "        meta = next(d for d in documents if d[\"id\"] == doc_id)\n",
    "        print(f\"  #{i+1} [{doc_id}] cat={meta['category']}, topic={meta['topic']}, dist={dist:.3f}\")\n",
    "        print(f\"      {doc[:80]}...\")\n",
    "    \n",
    "    # Step 2: Check context quality\n",
    "    print(\"\\n[2] CONTEXT QUALITY:\")\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    print(f\"  Total context length: {len(context)} chars, {len(context.split())} words\")\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    print(\"\\n[3] GENERATED ANSWER:\")\n",
    "    answer = generate_answer(question, context)\n",
    "    print(f\"  {answer}\")\n",
    "    \n",
    "    # Step 4: Check faithfulness\n",
    "    print(\"\\n[4] FAITHFULNESS CHECK:\")\n",
    "    faith = simple_faithfulness(context, answer)\n",
    "    print(f\"  Keyword overlap: {faith:.2f}\")\n",
    "    if faith < 0.1:\n",
    "        print(\"  WARNING: Low faithfulness - answer may not be grounded in context!\")\n",
    "    \n",
    "    return {\"ids\": retrieved_ids, \"answer\": answer, \"faithfulness\": faith}\n",
    "\n",
    "# Debug a query that might fail\n",
    "debug_rag_query(\"What is the difference between GANs and diffusion models for image generation?\", collection)\n",
    "print(\"\\n\" + \"#\" * 60 + \"\\n\")\n",
    "debug_rag_query(\"How does quantum computing relate to machine learning?\", collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Evaluation first**: Always measure before optimizing. Use precision@k, recall@k, and MRR for retrieval. Use faithfulness for generation.\n",
    "\n",
    "2. **HyDE**: Generate a hypothetical answer, embed that instead of the query. Bridges the query-document gap.\n",
    "\n",
    "3. **Cross-encoder re-ranking**: Two-stage retrieval (bi-encoder → cross-encoder) dramatically improves precision at minimal latency cost.\n",
    "\n",
    "4. **Hybrid search**: Combining BM25 (keyword) with vector search (semantic) via Reciprocal Rank Fusion gives the best of both worlds.\n",
    "\n",
    "5. **Debug systematically**: Check retrieval → chunk quality → prompt → answer, in that order.\n",
    "\n",
    "### Techniques Summary\n",
    "\n",
    "| Technique | When to Use | Complexity | Impact |\n",
    "|-----------|------------|------------|--------|\n",
    "| HyDE | When queries are short/vague | Low | Medium |\n",
    "| Cross-encoder re-ranking | When precision matters | Medium | High |\n",
    "| Hybrid search (BM25+vector) | When users use specific terms | Medium | High |\n",
    "| Query decomposition | Complex multi-part questions | Low | Medium |\n",
    "\n",
    "### References\n",
    "\n",
    "- Paper: Gao et al. \"Precise Zero-Shot Dense Retrieval without Relevance Labels\" (HyDE, 2022)\n",
    "- Framework: RAGAS (ragas.io) for automated RAG evaluation\n",
    "- Paper: Nogueira & Cho \"Passage Re-ranking with BERT\" (2019)\n",
    "- Library: rank-bm25 for BM25 implementation\n",
    "- Library: sentence-transformers CrossEncoder for re-ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}