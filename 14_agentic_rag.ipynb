{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 14: Agentic RAG (Capstone)\n",
    "\n",
    "This capstone module combines everything from the course: RAG (Modules 9-11) + Agents (Modules 12-13) into **Agentic RAG** systems.\n",
    "\n",
    "**What you'll build:**\n",
    "1. Retrieval as an agent tool (agent decides WHEN to retrieve)\n",
    "2. Multi-source retrieval (agent chooses WHICH source)\n",
    "3. Corrective RAG (CRAG) - evaluate and re-retrieve if needed\n",
    "4. Self-RAG - decide when retrieval is needed\n",
    "5. Full agentic RAG pipeline\n",
    "\n",
    "**Why Agentic RAG?**\n",
    "\n",
    "Static RAG pipelines always retrieve, always use the same source, and never check quality. Agentic RAG adds intelligence:\n",
    "\n",
    "```\n",
    "Static RAG:   Question → Always Retrieve → Always Generate\n",
    "Agentic RAG:  Question → Decide IF retrieval needed → Choose WHICH source → \n",
    "              Retrieve → Check quality → Re-retrieve if bad → Generate → Self-check\n",
    "```\n",
    "\n",
    "**Prerequisites:** Modules 9-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai python-dotenv chromadb sentence-transformers langchain langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import TypedDict, List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/amir/source/.env\")\n",
    "\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = OpenAI()\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base about AI/ML topics\n",
    "documents = [\n",
    "    {\"id\": \"doc_0\", \"text\": \"Neural networks are computing systems inspired by biological neural networks. They consist of layers of interconnected nodes that process information. Deep learning uses neural networks with many hidden layers to learn hierarchical representations of data. Common architectures include CNNs for images and RNNs for sequences.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_1\", \"text\": \"The Transformer architecture, introduced in 'Attention Is All You Need' (2017), relies entirely on self-attention mechanisms and processes all positions in parallel. It consists of encoder and decoder stacks with multi-head attention and feed-forward layers. Transformers are the foundation of BERT, GPT, and T5.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_2\", \"text\": \"BERT uses masked language modeling for pre-training, randomly masking 15% of tokens and predicting them. It processes text bidirectionally, making it excellent for understanding tasks like classification and NER. BERT-base has 110M parameters.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_3\", \"text\": \"GPT models use autoregressive language modeling, predicting the next token left-to-right. GPT-3 has 175B parameters and demonstrated few-shot learning. GPT-4 is multimodal. The GPT family uses decoder-only transformer architecture.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_4\", \"text\": \"Retrieval-Augmented Generation (RAG) combines a retriever with a generator to ground LLM outputs in retrieved evidence. RAG addresses hallucination and knowledge cutoff. The retriever uses dense embeddings and vector similarity search.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_5\", \"text\": \"Vector databases like ChromaDB, Pinecone, and FAISS store high-dimensional embeddings and support efficient similarity search. ChromaDB is lightweight and local, Pinecone is cloud-native, and FAISS handles billion-scale search.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_6\", \"text\": \"Prompt engineering techniques include zero-shot prompting, few-shot with examples, chain-of-thought reasoning, and role prompting. Structured output can be enforced through system messages. Good prompts are specific, provide context, and include examples.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_7\", \"text\": \"AI agents are autonomous systems that use LLMs to reason, plan, and take actions through tools. The ReAct pattern interleaves reasoning with actions. Multi-agent systems coordinate specialized agents. LangGraph models agent workflows as state machines.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_8\", \"text\": \"Fine-tuning adapts pre-trained models to specific tasks. LoRA (Low-Rank Adaptation) freezes the base model and trains small adapter matrices, reducing compute by 10-100x. QLoRA combines quantization with LoRA for even more efficiency.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_9\", \"text\": \"RLHF (Reinforcement Learning from Human Feedback) aligns LLMs with human preferences through supervised fine-tuning, reward model training, and PPO optimization. DPO offers a simpler alternative without a separate reward model.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_10\", \"text\": \"Evaluation metrics for RAG include precision@k, recall@k, MRR for retrieval, and faithfulness and relevance for generation. The RAGAS framework automates RAG evaluation. Human evaluation remains the gold standard.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_11\", \"text\": \"Chunking strategies for RAG include fixed-size splitting, sentence-based chunking, recursive character splitting, and semantic chunking. Optimal chunk size balances context with precision, typically 200-500 tokens. Overlap between chunks preserves continuity.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_12\", \"text\": \"Embedding models like Sentence-BERT, E5, and OpenAI's text-embedding-3-small convert text into dense vectors capturing semantic meaning. They are trained with contrastive learning. The choice of embedding model significantly affects retrieval quality.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_13\", \"text\": \"Attention mechanisms allow models to focus on relevant parts of the input. Self-attention computes query-key-value interactions within a sequence. Multi-head attention runs multiple attention computations in parallel, each learning different relationship patterns.\", \"source\": \"textbook\"},\n",
    "    {\"id\": \"doc_14\", \"text\": \"Diffusion models generate images by learning to reverse a gradual noising process. Models like Stable Diffusion and DALL-E 3 produce high-quality images from text prompts. They have largely replaced GANs due to better training stability.\", \"source\": \"textbook\"}\n",
    "]\n",
    "\n",
    "# Embed and store\n",
    "chroma_client = chromadb.Client()\n",
    "kb_collection = chroma_client.create_collection(name=\"ai_knowledge_base\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "doc_ids = [doc[\"id\"] for doc in documents]\n",
    "embeddings = embedding_model.encode(texts).tolist()\n",
    "\n",
    "kb_collection.add(\n",
    "    documents=texts,\n",
    "    embeddings=embeddings,\n",
    "    ids=doc_ids,\n",
    "    metadatas=[{\"source\": doc[\"source\"]} for doc in documents]\n",
    ")\n",
    "\n",
    "print(f\"Knowledge base loaded: {kb_collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(messages, temperature=0.0, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Call OpenAI chat completions.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def kb_search(query, k=3):\n",
    "    \"\"\"Search the knowledge base.\"\"\"\n",
    "    query_emb = embedding_model.encode([query]).tolist()\n",
    "    results = kb_collection.query(query_embeddings=query_emb, n_results=k)\n",
    "    return results[\"documents\"][0], results[\"ids\"][0], results[\"distances\"][0]\n",
    "\n",
    "def mock_web_search(query):\n",
    "    \"\"\"Simulated web search (returns hardcoded results for demo).\"\"\"\n",
    "    web_results = {\n",
    "        \"latest\": \"As of 2024, the latest developments in AI include multimodal models like GPT-4o, open-source models like Llama 3, and advances in AI agents and reasoning.\",\n",
    "        \"news\": \"Recent AI news: OpenAI released GPT-4o with native multimodal capabilities. Anthropic launched Claude 3.5 Sonnet. Meta open-sourced Llama 3.1 405B.\",\n",
    "        \"default\": f\"Web search results for '{query}': This information is current as of 2024 and may include recent developments not covered in the knowledge base.\"\n",
    "    }\n",
    "    for key in web_results:\n",
    "        if key in query.lower():\n",
    "            return web_results[key]\n",
    "    return web_results[\"default\"]\n",
    "\n",
    "def calculator(expression):\n",
    "    \"\"\"Safe calculator for mathematical expressions.\"\"\"\n",
    "    allowed = set('0123456789+-*/.() ')\n",
    "    if all(c in allowed for c in expression):\n",
    "        return str(eval(expression))\n",
    "    return \"Error: Invalid expression\"\n",
    "\n",
    "print(\"Helper functions ready!\")\n",
    "print(f\"KB search test: {kb_search('transformers', k=1)[0][0][:60]}...\")\n",
    "print(f\"Calculator test: 2^10 = {calculator('2**10')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Retrieval as an Agent Tool\n",
    "\n",
    "The key insight of agentic RAG: **the agent decides WHEN to retrieve**. Not every question needs retrieval - some can be answered from the LLM's parametric knowledge.\n",
    "\n",
    "We'll use OpenAI function calling to let the LLM decide which tools to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools for function calling\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"knowledge_base_search\",\n",
    "            \"description\": \"Search the AI/ML knowledge base for information about neural networks, transformers, RAG, embeddings, agents, and related topics. Use this when the question is about AI/ML concepts covered in the course.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The search query\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"web_search\",\n",
    "            \"description\": \"Search the web for recent or current information. Use this for questions about latest news, current events, or topics not in the knowledge base.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The search query\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Perform mathematical calculations. Use for any arithmetic, percentages, or numerical computations.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\"type\": \"string\", \"description\": \"Math expression (e.g., '2 * 3 + 4')\"}\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def execute_tool(tool_name, arguments):\n",
    "    \"\"\"Execute a tool and return the result.\"\"\"\n",
    "    if tool_name == \"knowledge_base_search\":\n",
    "        docs, ids, dists = kb_search(arguments[\"query\"], k=3)\n",
    "        return \"\\n\\n\".join(docs)\n",
    "    elif tool_name == \"web_search\":\n",
    "        return mock_web_search(arguments[\"query\"])\n",
    "    elif tool_name == \"calculator\":\n",
    "        return calculator(arguments[\"expression\"])\n",
    "    return \"Unknown tool\"\n",
    "\n",
    "def agentic_rag(question, verbose=True):\n",
    "    \"\"\"\n",
    "    Agentic RAG: LLM decides which tools to use.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant with access to tools. Use tools when needed to answer questions accurately. You can use multiple tools if needed. If you can answer from your own knowledge confidently, you don't need to use tools.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Question: {question}\")\n",
    "    \n",
    "    # LLM decides what to do\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    msg = response.choices[0].message\n",
    "    \n",
    "    # If LLM wants to use tools\n",
    "    if msg.tool_calls:\n",
    "        messages.append(msg)\n",
    "        \n",
    "        for tool_call in msg.tool_calls:\n",
    "            tool_name = tool_call.function.name\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Tool: {tool_name}({arguments})\")\n",
    "            \n",
    "            result = execute_tool(tool_name, arguments)\n",
    "            \n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": result\n",
    "            })\n",
    "        \n",
    "        # Get final answer with tool results\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        answer = final_response.choices[0].message.content\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"  No tools used (answered from parametric knowledge)\")\n",
    "        answer = msg.content\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Answer: {answer[:200]}...\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with different types of questions\n",
    "test_questions = [\n",
    "    \"What is the transformer architecture?\",          # Should use KB\n",
    "    \"What are the latest AI developments in 2024?\",   # Should use web\n",
    "    \"What is 175 billion divided by 1000?\",            # Should use calculator\n",
    "    \"What is the capital of France?\",                  # Should answer directly\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    agentic_rag(q)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Agent with 3 Tools - Autonomous Selection\n",
    "\n",
    "**Your Task:** Test the agentic RAG system with 5 diverse questions that require different tools, and analyze the agent's tool selection behavior.\n",
    "\n",
    "**Steps:**\n",
    "1. Create 5 questions: 2 requiring KB search, 1 requiring web, 1 requiring calculator, 1 requiring no tools\n",
    "2. Run each through `agentic_rag` with verbose=True\n",
    "3. Track which tools were selected for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create 5 diverse test questions and analyze tool selection\n",
    "\n",
    "test_questions = [\n",
    "    None,  # Your code: KB search question\n",
    "    None,  # Your code: KB search question\n",
    "    None,  # Your code: Web search question\n",
    "    None,  # Your code: Calculator question\n",
    "    None,  # Your code: No-tool question\n",
    "]\n",
    "\n",
    "# Run and analyze\n",
    "for q in test_questions:\n",
    "    if q:\n",
    "        answer = agentic_rag(q)\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"How does RAG address the hallucination problem in LLMs?\",\n",
    "    \"What are the different chunking strategies used in RAG systems?\",\n",
    "    \"What are the latest news about open-source AI models?\",\n",
    "    \"If a model has 175 billion parameters and each parameter uses 2 bytes (float16), how many gigabytes of memory does it need?\",\n",
    "    \"What does the acronym API stand for?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    answer = agentic_rag(q)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Query Routing\n",
    "\n",
    "Before retrieval, we can classify the query to route it to the most appropriate source.\n",
    "\n",
    "```\n",
    "Question → Classify → Route:\n",
    "  ├─ \"factual_kb\"    → Search knowledge base\n",
    "  ├─ \"recent_events\" → Search the web  \n",
    "  ├─ \"calculation\"   → Use calculator\n",
    "  └─ \"general\"       → Answer from LLM knowledge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(question):\n",
    "    \"\"\"Classify query to determine routing.\"\"\"\n",
    "    response = llm_call([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"Classify the user's question into exactly one category. Respond with ONLY the category name.\n",
    "\n",
    "Categories:\n",
    "- factual_kb: Questions about AI/ML concepts, neural networks, transformers, RAG, embeddings, agents\n",
    "- recent_events: Questions about recent news, latest developments, current events\n",
    "- calculation: Questions requiring mathematical computation\n",
    "- general: General knowledge questions not about AI/ML\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ])\n",
    "    return response.strip().lower()\n",
    "\n",
    "def routed_rag(question, verbose=True):\n",
    "    \"\"\"RAG with explicit query routing.\"\"\"\n",
    "    category = classify_query(question)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"  Route: {category}\")\n",
    "    \n",
    "    if category == \"factual_kb\":\n",
    "        docs, _, _ = kb_search(question, k=3)\n",
    "        context = \"\\n\\n\".join(docs)\n",
    "        answer = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Answer based on the provided context. Be concise and accurate.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "        ])\n",
    "    elif category == \"recent_events\":\n",
    "        web_result = mock_web_search(question)\n",
    "        answer = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Answer based on these web search results.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Search results:\\n{web_result}\\n\\nQuestion: {question}\"}\n",
    "        ])\n",
    "    elif category == \"calculation\":\n",
    "        answer = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Extract the math expression and compute the result. Show your work.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ])\n",
    "    else:\n",
    "        answer = llm_call([\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Answer: {answer[:150]}...\\n\")\n",
    "    return answer, category\n",
    "\n",
    "# Test routing\n",
    "routing_tests = [\n",
    "    \"How does self-attention work in transformers?\",\n",
    "    \"What AI models were released this year?\",\n",
    "    \"What is 256 * 768 * 12?\",\n",
    "    \"What is the population of Tokyo?\"\n",
    "]\n",
    "\n",
    "for q in routing_tests:\n",
    "    routed_rag(q)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Corrective RAG (CRAG)\n",
    "\n",
    "**CRAG** adds a critical step: after retrieval, **evaluate whether the retrieved documents are actually relevant** before generating an answer.\n",
    "\n",
    "```\n",
    "Question → Retrieve → Evaluate Relevance\n",
    "  ├─ RELEVANT     → Generate answer from context\n",
    "  ├─ AMBIGUOUS    → Re-retrieve with refined query, then generate\n",
    "  └─ NOT RELEVANT → Fall back to web search or parametric knowledge\n",
    "```\n",
    "\n",
    "This prevents the model from generating answers based on irrelevant retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(question, document):\n",
    "    \"\"\"Use LLM to judge if a document is relevant to the question.\"\"\"\n",
    "    response = llm_call([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a relevance evaluator. Given a question and a document, determine if the document contains information useful for answering the question.\n",
    "\n",
    "Respond with ONLY one word:\n",
    "- RELEVANT: The document directly addresses the question\n",
    "- PARTIAL: The document is somewhat related but doesn't fully answer\n",
    "- IRRELEVANT: The document is not related to the question\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nDocument: {document}\"}\n",
    "    ])\n",
    "    return response.strip().upper()\n",
    "\n",
    "def corrective_rag(question, verbose=True):\n",
    "    \"\"\"\n",
    "    CRAG: Retrieve → Evaluate → Correct if needed → Generate\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Q: {question}\")\n",
    "    \n",
    "    # Step 1: Initial retrieval\n",
    "    docs, ids, dists = kb_search(question, k=3)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  [Retrieve] Got {len(docs)} documents\")\n",
    "    \n",
    "    # Step 2: Evaluate relevance of each document\n",
    "    relevant_docs = []\n",
    "    for doc, doc_id in zip(docs, ids):\n",
    "        relevance = evaluate_relevance(question, doc)\n",
    "        if verbose:\n",
    "            print(f\"  [Evaluate] {doc_id}: {relevance}\")\n",
    "        if relevance in [\"RELEVANT\", \"PARTIAL\"]:\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    # Step 3: Corrective action based on evaluation\n",
    "    if len(relevant_docs) >= 2:\n",
    "        # Good retrieval - proceed with generation\n",
    "        if verbose:\n",
    "            print(f\"  [Correct] Sufficient relevant docs ({len(relevant_docs)}) - generating answer\")\n",
    "        context = \"\\n\\n\".join(relevant_docs)\n",
    "        source = \"knowledge_base\"\n",
    "    elif len(relevant_docs) == 1:\n",
    "        # Partial - try to supplement with a refined query\n",
    "        if verbose:\n",
    "            print(f\"  [Correct] Only 1 relevant doc - refining query and re-retrieving\")\n",
    "        refined_query = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Rephrase this question to be more specific for a search engine. Return only the rephrased question.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ])\n",
    "        extra_docs, _, _ = kb_search(refined_query, k=2)\n",
    "        relevant_docs.extend(extra_docs)\n",
    "        context = \"\\n\\n\".join(relevant_docs)\n",
    "        source = \"knowledge_base (refined)\"\n",
    "    else:\n",
    "        # No relevant docs - fall back to web search\n",
    "        if verbose:\n",
    "            print(f\"  [Correct] No relevant docs - falling back to web search\")\n",
    "        web_result = mock_web_search(question)\n",
    "        context = web_result\n",
    "        source = \"web_search\"\n",
    "    \n",
    "    # Step 4: Generate answer\n",
    "    answer = llm_call([\n",
    "        {\"role\": \"system\", \"content\": \"Answer the question based on the provided context. Be concise.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "    ])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  [Source] {source}\")\n",
    "        print(f\"  [Answer] {answer[:200]}\")\n",
    "    \n",
    "    return answer, source\n",
    "\n",
    "# Test CRAG with different types of questions\n",
    "print(\"=\" * 60)\n",
    "corrective_rag(\"What is the difference between BERT and GPT architectures?\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "corrective_rag(\"What is the weather like in New York today?\")  # Should fall back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Corrective RAG with Re-retrieval\n",
    "\n",
    "**Your Task:** Extend the CRAG pipeline to handle the case where initial retrieval returns partially relevant results by re-retrieving with a decomposed query.\n",
    "\n",
    "**Steps:**\n",
    "1. Retrieve documents for the question\n",
    "2. Evaluate each document's relevance\n",
    "3. If results are poor: decompose the question into sub-questions, retrieve for each\n",
    "4. Combine all relevant documents and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrective_rag_v2(question, verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced CRAG with query decomposition for re-retrieval.\n",
    "    \"\"\"\n",
    "    # TODO: Implement enhanced CRAG\n",
    "    \n",
    "    # 1. Initial retrieval\n",
    "    docs, ids, dists = None, None, None  # Your code\n",
    "    \n",
    "    # 2. Evaluate relevance\n",
    "    relevant_docs = []  # Your code: filter relevant docs\n",
    "    \n",
    "    # 3. If insufficient relevant docs, decompose query\n",
    "    if len(relevant_docs) < 2:\n",
    "        # Decompose into sub-questions\n",
    "        sub_questions = None  # Your code: use LLM to break question into parts\n",
    "        \n",
    "        # Re-retrieve for each sub-question\n",
    "        pass  # Your code\n",
    "    \n",
    "    # 4. Generate answer from all collected context\n",
    "    answer = None  # Your code\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test\n",
    "# corrective_rag_v2(\"Compare how BERT and GPT handle attention masking and what that means for their use cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrective_rag_v2(question, verbose=True):\n",
    "    \"\"\"Enhanced CRAG with query decomposition - SOLUTION.\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Q: {question}\")\n",
    "    \n",
    "    # 1. Initial retrieval\n",
    "    docs, ids, dists = kb_search(question, k=3)\n",
    "    if verbose:\n",
    "        print(f\"  [Retrieve] Got {len(docs)} docs: {ids}\")\n",
    "    \n",
    "    # 2. Evaluate relevance\n",
    "    relevant_docs = []\n",
    "    for doc, doc_id in zip(docs, ids):\n",
    "        rel = evaluate_relevance(question, doc)\n",
    "        if verbose:\n",
    "            print(f\"  [Evaluate] {doc_id}: {rel}\")\n",
    "        if rel in [\"RELEVANT\", \"PARTIAL\"]:\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    # 3. If insufficient, decompose and re-retrieve\n",
    "    if len(relevant_docs) < 2:\n",
    "        if verbose:\n",
    "            print(f\"  [Correct] Only {len(relevant_docs)} relevant - decomposing query\")\n",
    "        \n",
    "        sub_q_response = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Break this complex question into 2-3 simpler sub-questions. Return each on a new line, nothing else.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ])\n",
    "        sub_questions = [q.strip().lstrip('0123456789.-) ') for q in sub_q_response.strip().split('\\n') if q.strip()]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  [Decompose] Sub-questions: {sub_questions}\")\n",
    "        \n",
    "        for sub_q in sub_questions:\n",
    "            sub_docs, _, _ = kb_search(sub_q, k=2)\n",
    "            for doc in sub_docs:\n",
    "                if doc not in relevant_docs:\n",
    "                    relevant_docs.append(doc)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  [Re-retrieve] Now have {len(relevant_docs)} documents\")\n",
    "    \n",
    "    # 4. Generate\n",
    "    if not relevant_docs:\n",
    "        context = mock_web_search(question)\n",
    "        source = \"web_fallback\"\n",
    "    else:\n",
    "        context = \"\\n\\n\".join(relevant_docs[:5])\n",
    "        source = \"knowledge_base\"\n",
    "    \n",
    "    answer = llm_call([\n",
    "        {\"role\": \"system\", \"content\": \"Answer the question based on the provided context. Be thorough.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "    ])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  [Source] {source}\")\n",
    "        print(f\"  [Answer] {answer[:300]}\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "corrective_rag_v2(\"Compare how BERT and GPT handle attention and what chunking strategies work best for RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Self-RAG\n",
    "\n",
    "**Self-RAG** goes further than CRAG: the model decides at each step whether it needs retrieval, and after generating, checks if its answer is faithful to the sources.\n",
    "\n",
    "```\n",
    "Question → [Need retrieval?]\n",
    "  ├─ YES → Retrieve → Generate → [Is answer faithful?]\n",
    "  │                                 ├─ YES → Return answer\n",
    "  │                                 └─ NO  → Regenerate with better prompt\n",
    "  └─ NO  → Generate from knowledge → [Is answer confident?]\n",
    "                                       ├─ YES → Return answer\n",
    "                                       └─ NO  → Retrieve and try again\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def needs_retrieval(question):\n",
    "    \"\"\"Decide if a question needs external retrieval.\"\"\"\n",
    "    response = llm_call([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"Determine if this question requires looking up specific information from a knowledge base about AI/ML, or if it can be answered from general knowledge.\n",
    "\n",
    "Respond with ONLY one word:\n",
    "- RETRIEVE: Needs specific facts, definitions, or technical details about AI/ML\n",
    "- GENERATE: Can be answered from general knowledge\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ])\n",
    "    return response.strip().upper() == \"RETRIEVE\"\n",
    "\n",
    "def check_faithfulness(question, context, answer):\n",
    "    \"\"\"Check if the answer is faithful to the retrieved context.\"\"\"\n",
    "    response = llm_call([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"Evaluate if the answer is faithful to (supported by) the provided context. The answer should not contain claims that aren't in the context.\n",
    "\n",
    "Respond with ONLY one word:\n",
    "- FAITHFUL: Answer is fully supported by context\n",
    "- UNFAITHFUL: Answer contains claims not in context\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}\"}\n",
    "    ])\n",
    "    return response.strip().upper() == \"FAITHFUL\"\n",
    "\n",
    "def self_rag(question, max_retries=2, verbose=True):\n",
    "    \"\"\"\n",
    "    Self-RAG: Decide when to retrieve, self-check generation.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Q: {question}\")\n",
    "    \n",
    "    # Step 1: Decide if retrieval is needed\n",
    "    should_retrieve = needs_retrieval(question)\n",
    "    if verbose:\n",
    "        print(f\"  [Decision] {'RETRIEVE' if should_retrieve else 'GENERATE directly'}\")\n",
    "    \n",
    "    if should_retrieve:\n",
    "        docs, ids, _ = kb_search(question, k=3)\n",
    "        context = \"\\n\\n\".join(docs)\n",
    "        \n",
    "        for attempt in range(max_retries + 1):\n",
    "            # Generate answer\n",
    "            if attempt == 0:\n",
    "                prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "            else:\n",
    "                prompt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nIMPORTANT: Only use information from the context above. Do not add any information not present in the context.\"\n",
    "            \n",
    "            answer = llm_call([\n",
    "                {\"role\": \"system\", \"content\": \"Answer based strictly on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ])\n",
    "            \n",
    "            # Check faithfulness\n",
    "            is_faithful = check_faithfulness(question, context, answer)\n",
    "            if verbose:\n",
    "                print(f\"  [Attempt {attempt+1}] Faithful: {is_faithful}\")\n",
    "            \n",
    "            if is_faithful:\n",
    "                break\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  [Answer] {answer[:200]}\")\n",
    "        return answer\n",
    "    else:\n",
    "        # Generate without retrieval\n",
    "        answer = llm_call([\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ])\n",
    "        if verbose:\n",
    "            print(f\"  [Answer] {answer[:200]}\")\n",
    "        return answer\n",
    "\n",
    "# Test Self-RAG\n",
    "print(\"=\" * 60)\n",
    "self_rag(\"What is the difference between encoder-only and decoder-only transformers?\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "self_rag(\"What color is the sky?\")  # Should not retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Full Agentic RAG Pipeline with State Machine\n",
    "\n",
    "Now let's combine everything into a complete pipeline modeled as a state machine.\n",
    "\n",
    "```\n",
    "States:\n",
    "  classify → route → retrieve → evaluate → generate → check → END\n",
    "                  ↘ web_search ↗         ↘ re_retrieve ↗\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAGPipeline:\n",
    "    \"\"\"\n",
    "    Full agentic RAG pipeline combining routing, retrieval,\n",
    "    evaluation, correction, and self-checking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trace = []  # Record of all steps taken\n",
    "    \n",
    "    def _log(self, step, detail):\n",
    "        self.trace.append({\"step\": step, \"detail\": detail})\n",
    "    \n",
    "    def classify(self, question):\n",
    "        \"\"\"Classify the query type.\"\"\"\n",
    "        category = classify_query(question)\n",
    "        self._log(\"classify\", f\"Category: {category}\")\n",
    "        return category\n",
    "    \n",
    "    def retrieve(self, question, k=3):\n",
    "        \"\"\"Retrieve from knowledge base.\"\"\"\n",
    "        docs, ids, dists = kb_search(question, k=k)\n",
    "        self._log(\"retrieve\", f\"Got {len(docs)} docs: {ids}\")\n",
    "        return docs, ids\n",
    "    \n",
    "    def evaluate(self, question, docs, ids):\n",
    "        \"\"\"Evaluate retrieved document relevance.\"\"\"\n",
    "        relevant = []\n",
    "        for doc, doc_id in zip(docs, ids):\n",
    "            rel = evaluate_relevance(question, doc)\n",
    "            self._log(\"evaluate\", f\"{doc_id}: {rel}\")\n",
    "            if rel in [\"RELEVANT\", \"PARTIAL\"]:\n",
    "                relevant.append(doc)\n",
    "        return relevant\n",
    "    \n",
    "    def generate(self, question, context):\n",
    "        \"\"\"Generate answer from context.\"\"\"\n",
    "        answer = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Answer the question based on the provided context. Cite specific facts from the context.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "        ])\n",
    "        self._log(\"generate\", f\"Generated {len(answer)} chars\")\n",
    "        return answer\n",
    "    \n",
    "    def quality_check(self, question, context, answer):\n",
    "        \"\"\"Check answer quality.\"\"\"\n",
    "        is_faithful = check_faithfulness(question, context, answer)\n",
    "        self._log(\"quality_check\", f\"Faithful: {is_faithful}\")\n",
    "        return is_faithful\n",
    "    \n",
    "    def run(self, question, verbose=True):\n",
    "        \"\"\"Execute the full agentic RAG pipeline.\"\"\"\n",
    "        self.trace = []\n",
    "        self._log(\"start\", f\"Question: {question}\")\n",
    "        \n",
    "        # Step 1: Classify\n",
    "        category = self.classify(question)\n",
    "        \n",
    "        # Step 2: Route\n",
    "        if category == \"factual_kb\":\n",
    "            # Retrieve from KB\n",
    "            docs, ids = self.retrieve(question)\n",
    "            relevant_docs = self.evaluate(question, docs, ids)\n",
    "            \n",
    "            # Corrective step\n",
    "            if len(relevant_docs) < 1:\n",
    "                self._log(\"correct\", \"No relevant docs - trying web search\")\n",
    "                context = mock_web_search(question)\n",
    "            elif len(relevant_docs) < 2:\n",
    "                self._log(\"correct\", \"Few relevant docs - supplementing with refined query\")\n",
    "                refined = llm_call([\n",
    "                    {\"role\": \"system\", \"content\": \"Rephrase for better search. Return only the query.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ])\n",
    "                extra_docs, _ = self.retrieve(refined, k=2)\n",
    "                relevant_docs.extend(extra_docs)\n",
    "                context = \"\\n\\n\".join(relevant_docs)\n",
    "            else:\n",
    "                context = \"\\n\\n\".join(relevant_docs)\n",
    "            \n",
    "            # Generate\n",
    "            answer = self.generate(question, context)\n",
    "            \n",
    "            # Quality check\n",
    "            if not self.quality_check(question, context, answer):\n",
    "                self._log(\"regenerate\", \"Failed quality check - regenerating\")\n",
    "                answer = self.generate(question + \" (Only use facts from the context, nothing else)\", context)\n",
    "        \n",
    "        elif category == \"recent_events\":\n",
    "            context = mock_web_search(question)\n",
    "            answer = self.generate(question, context)\n",
    "        \n",
    "        elif category == \"calculation\":\n",
    "            answer = llm_call([\n",
    "                {\"role\": \"system\", \"content\": \"Solve the math problem step by step.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ])\n",
    "            self._log(\"calculate\", \"Direct LLM calculation\")\n",
    "        \n",
    "        else:\n",
    "            answer = llm_call([{\"role\": \"user\", \"content\": question}])\n",
    "            self._log(\"direct\", \"Answered from parametric knowledge\")\n",
    "        \n",
    "        self._log(\"end\", f\"Answer length: {len(answer)}\")\n",
    "        \n",
    "        # Print trace\n",
    "        if verbose:\n",
    "            print(f\"\\nQuestion: {question}\")\n",
    "            print(f\"\\nPipeline trace:\")\n",
    "            for step in self.trace:\n",
    "                print(f\"  [{step['step']}] {step['detail']}\")\n",
    "            print(f\"\\nAnswer: {answer[:300]}\")\n",
    "        \n",
    "        return answer, self.trace\n",
    "\n",
    "# Test the full pipeline\n",
    "pipeline = AgenticRAGPipeline()\n",
    "\n",
    "pipeline.run(\"How does multi-head attention work in transformers?\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "pipeline.run(\"What is 1024 * 768?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Complete Agentic RAG for Technical Documentation\n",
    "\n",
    "**Your Task:** Extend the `AgenticRAGPipeline` to handle multi-step questions by decomposing them.\n",
    "\n",
    "**Steps:**\n",
    "1. Detect if a question has multiple parts\n",
    "2. Decompose into sub-questions\n",
    "3. Run each sub-question through the pipeline\n",
    "4. Synthesize a combined answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_agentic_rag(question, verbose=True):\n",
    "    \"\"\"\n",
    "    Handle multi-step questions by decomposition.\n",
    "    \"\"\"\n",
    "    # TODO: Implement multi-step agentic RAG\n",
    "    \n",
    "    # 1. Check if question needs decomposition\n",
    "    needs_decomposition = None  # Your code: use LLM to decide\n",
    "    \n",
    "    if needs_decomposition:\n",
    "        # 2. Decompose\n",
    "        sub_questions = None  # Your code\n",
    "        \n",
    "        # 3. Answer each sub-question\n",
    "        sub_answers = []  # Your code\n",
    "        \n",
    "        # 4. Synthesize\n",
    "        final_answer = None  # Your code\n",
    "    else:\n",
    "        pipeline = AgenticRAGPipeline()\n",
    "        final_answer, _ = pipeline.run(question, verbose=verbose)\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "# multi_step_agentic_rag(\"Compare BERT and GPT architectures, and explain which is better for RAG systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_agentic_rag(question, verbose=True):\n",
    "    \"\"\"Handle multi-step questions - SOLUTION.\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Original question: {question}\\n\")\n",
    "    \n",
    "    # 1. Check if decomposition is needed\n",
    "    check = llm_call([\n",
    "        {\"role\": \"system\", \"content\": \"Does this question contain multiple distinct parts that should be answered separately? Respond YES or NO only.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]).strip().upper()\n",
    "    \n",
    "    needs_decomposition = check == \"YES\"\n",
    "    \n",
    "    if needs_decomposition:\n",
    "        if verbose:\n",
    "            print(\"[Decomposing question into parts]\\n\")\n",
    "        \n",
    "        # 2. Decompose\n",
    "        sub_q_text = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Break this into 2-3 independent sub-questions. One per line, nothing else.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ])\n",
    "        sub_questions = [q.strip().lstrip('0123456789.-) ') for q in sub_q_text.strip().split('\\n') if q.strip()]\n",
    "        \n",
    "        if verbose:\n",
    "            for i, sq in enumerate(sub_questions):\n",
    "                print(f\"  Sub-Q {i+1}: {sq}\")\n",
    "            print()\n",
    "        \n",
    "        # 3. Answer each\n",
    "        sub_answers = []\n",
    "        pipeline = AgenticRAGPipeline()\n",
    "        for sq in sub_questions:\n",
    "            answer, _ = pipeline.run(sq, verbose=verbose)\n",
    "            sub_answers.append({\"question\": sq, \"answer\": answer})\n",
    "            if verbose:\n",
    "                print(\"-\" * 40)\n",
    "        \n",
    "        # 4. Synthesize\n",
    "        parts = \"\\n\\n\".join([f\"Q: {sa['question']}\\nA: {sa['answer']}\" for sa in sub_answers])\n",
    "        final_answer = llm_call([\n",
    "            {\"role\": \"system\", \"content\": \"Synthesize these sub-answers into one coherent, comprehensive response to the original question.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Original question: {question}\\n\\nSub-answers:\\n{parts}\"}\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = AgenticRAGPipeline()\n",
    "        final_answer, _ = pipeline.run(question, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"FINAL ANSWER:\\n{final_answer}\")\n",
    "    \n",
    "    return final_answer\n",
    "\n",
    "multi_step_agentic_rag(\"Compare BERT and GPT architectures, and explain which is better for RAG systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 (Capstone): End-to-End AI Assistant\n",
    "\n",
    "**Your Task:** Build a complete AI assistant that combines:\n",
    "- RAG from the knowledge base\n",
    "- Web search fallback\n",
    "- Mathematical computation\n",
    "- Multi-step reasoning\n",
    "- Self-checking for quality\n",
    "\n",
    "Test with 5 diverse queries that exercise different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_assistant(question):\n",
    "    \"\"\"\n",
    "    Complete AI assistant combining all agentic RAG capabilities.\n",
    "    \n",
    "    TODO: Combine the best of:\n",
    "    - agentic_rag() for tool selection\n",
    "    - corrective_rag() for quality checking\n",
    "    - multi_step_agentic_rag() for complex questions\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test with 5 diverse queries\n",
    "capstone_queries = [\n",
    "    None,  # Your code: technical AI question\n",
    "    None,  # Your code: comparison question\n",
    "    None,  # Your code: calculation question\n",
    "    None,  # Your code: recent events question\n",
    "    None,  # Your code: multi-step reasoning question\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_assistant(question, verbose=True):\n",
    "    \"\"\"Complete AI assistant - SOLUTION.\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"USER: {question}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # Use multi-step agentic RAG as the backbone\n",
    "    # It handles decomposition, routing, retrieval, evaluation, and generation\n",
    "    answer = multi_step_agentic_rag(question, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"ASSISTANT: {answer}\")\n",
    "        print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with 5 diverse queries\n",
    "capstone_queries = [\n",
    "    \"How do transformer models use attention mechanisms to process text?\",\n",
    "    \"Compare the training approaches of BERT and GPT models\",\n",
    "    \"If a transformer has 12 attention heads and each head has dimension 64, what is the total model dimension?\",\n",
    "    \"What are the latest developments in open-source AI?\",\n",
    "    \"Explain how RAG systems use embeddings for retrieval, and what evaluation metrics are used to measure their quality\"\n",
    "]\n",
    "\n",
    "for query in capstone_queries:\n",
    "    ai_assistant(query)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Course Summary\n",
    "\n",
    "Congratulations on completing the AI RAG & Agents course!\n",
    "\n",
    "### Your Journey\n",
    "\n",
    "| Module | Topic | Key Skill |\n",
    "|--------|-------|----------|\n",
    "| 1 | AI/ML Fundamentals | Linear regression, gradient descent, evaluation metrics |\n",
    "| 2 | Neural Networks | Perceptrons, backpropagation, PyTorch, MNIST |\n",
    "| 3 | NLP Fundamentals | Tokenization, TF-IDF, text classification |\n",
    "| 4 | Word Vectors | GloVe, word analogies, dense representations |\n",
    "| 5 | Modern Embeddings | API embeddings, semantic similarity, clustering |\n",
    "| 6 | Text Generation | GPT-2, sampling strategies, temperature |\n",
    "| 7a/b | Transformers | Self-attention, multi-head attention, encoder/decoder |\n",
    "| 8 | Prompt Engineering | Zero/few-shot, CoT, structured output |\n",
    "| 9 | RAG Foundations | Chunking, vector databases, semantic search |\n",
    "| 10 | RAG Pipeline | End-to-end Q&A, stuff/map-reduce, source attribution |\n",
    "| 11 | Advanced RAG | Evaluation, HyDE, re-ranking, hybrid search |\n",
    "| 12 | Agents Introduction | Tool use, ReAct, LangChain agents |\n",
    "| 13 | Advanced Agents | Multi-agent systems, memory, planning, LangGraph |\n",
    "| **14** | **Agentic RAG** | **Combining RAG + agents for intelligent retrieval** |\n",
    "\n",
    "### Where to Go From Here\n",
    "\n",
    "- **Production RAG**: LlamaIndex, Haystack for production-grade pipelines\n",
    "- **Fine-tuning**: LoRA/QLoRA for domain-specific models\n",
    "- **Evaluation**: RAGAS, DeepEval for automated quality assessment\n",
    "- **Advanced Agents**: CrewAI, AutoGen for complex multi-agent systems\n",
    "- **Build real applications**: Customer support bots, research assistants, code analysis tools\n",
    "\n",
    "### References\n",
    "\n",
    "- Paper: Yan et al. \"Corrective Retrieval Augmented Generation\" (CRAG, 2024)\n",
    "- Paper: Asai et al. \"Self-RAG: Learning to Retrieve, Generate, and Critique\" (2023)\n",
    "- Docs: LangGraph \"Agentic RAG\" tutorial\n",
    "- Paper: Lewis et al. \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}