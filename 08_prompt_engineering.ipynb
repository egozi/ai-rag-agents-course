{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "# Module 8: Prompt Engineering\n",
    "\n",
    "Prompt engineering is the art and science of communicating effectively with large language models (LLMs). It is the **interface between humans and AI** -- the way we translate our intent into instructions that a model can act on.\n",
    "\n",
    "Why does prompt engineering matter?\n",
    "\n",
    "- **Same model, vastly different results**: A well-crafted prompt can turn a mediocre output into an excellent one, without changing the model or fine-tuning.\n",
    "- **Cost efficiency**: Better prompts mean fewer retries, less post-processing, and lower API costs.\n",
    "- **Reliability**: Systematic prompt engineering produces consistent, predictable outputs -- critical for production systems.\n",
    "- **Unlocking capabilities**: Models have latent abilities (reasoning, structured output, role-play) that only emerge with the right prompting techniques.\n",
    "\n",
    "### What you'll learn\n",
    "\n",
    "1. **Prompt anatomy** -- system, user, and assistant messages\n",
    "2. **Zero-shot prompting** -- direct instructions without examples\n",
    "3. **Few-shot prompting** -- teaching by example\n",
    "4. **Chain-of-thought (CoT) prompting** -- eliciting step-by-step reasoning\n",
    "5. **Role prompting** -- setting persona and expertise\n",
    "6. **Structured output** -- getting reliable JSON, tables, and formatted data\n",
    "7. **Common failures and mitigations** -- hallucinations, instruction-following issues\n",
    "8. **Iterative refinement** -- the workflow for evolving prompts\n",
    "9. **Prompt templates** -- reusable, parameterized prompts\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0002-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv(\"/home/amir/source/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def chat(messages, model=\"gpt-4o-mini\", temperature=0.7):\n",
    "    \"\"\"Helper function to call the OpenAI Chat Completions API.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content' keys.\n",
    "        model: Model identifier (default: gpt-4o-mini).\n",
    "        temperature: Sampling temperature (0 = deterministic, higher = more creative).\n",
    "    \n",
    "    Returns:\n",
    "        The assistant's response as a string.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Quick test\n",
    "print(chat([{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0003-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prompt Anatomy\n",
    "\n",
    "The OpenAI Chat Completions API (and similar APIs) use a **messages** format with three role types:\n",
    "\n",
    "| Role | Purpose | When to use |\n",
    "|------|---------|-------------|\n",
    "| **system** | Sets behavior, role, constraints, and tone | Once, at the start of the conversation |\n",
    "| **user** | The actual request or question | Every turn |\n",
    "| **assistant** | The model's response (or a synthetic example) | Few-shot examples, multi-turn context |\n",
    "\n",
    "Think of it like directing an actor:\n",
    "- **System message** = the character description and stage directions\n",
    "- **User message** = the scene prompt\n",
    "- **Assistant message** = the actor's previous lines (for continuity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: All three message types in action\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a concise technical writer. Always respond in exactly 2 sentences.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is a neural network?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = chat(messages)\n",
    "print(\"Response:\", response)\n",
    "print(\"---\")\n",
    "\n",
    "# Now continue the conversation with the assistant's prior response\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "messages.append({\"role\": \"user\", \"content\": \"How is it trained?\"})\n",
    "\n",
    "response2 = chat(messages)\n",
    "print(\"Follow-up:\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system message dramatically changes behavior\n",
    "# Same question, different system messages\n",
    "\n",
    "question = \"Explain what a database index is.\"\n",
    "\n",
    "# Version 1: Expert audience\n",
    "expert_response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a database expert speaking to senior engineers. Be technical and precise.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "])\n",
    "\n",
    "# Version 2: Beginner audience\n",
    "beginner_response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a patient teacher explaining to a 10-year-old. Use simple analogies.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "])\n",
    "\n",
    "print(\"=== Expert version ===\")\n",
    "print(expert_response)\n",
    "print()\n",
    "print(\"=== Beginner version ===\")\n",
    "print(beginner_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0004-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Zero-Shot Prompting\n",
    "\n",
    "**Zero-shot prompting** means giving the model a direct instruction **without any examples**. The model relies entirely on its pre-trained knowledge to understand and complete the task.\n",
    "\n",
    "This is the simplest form of prompting and works surprisingly well for many tasks.\n",
    "\n",
    "**Tips for effective zero-shot prompts:**\n",
    "- Be specific about the desired output format\n",
    "- Use clear, unambiguous language\n",
    "- Specify constraints (length, style, format)\n",
    "- Tell the model what NOT to do if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot: Sentiment Classification\n",
    "\n",
    "review = \"The battery life is incredible and the camera quality exceeded my expectations, but the phone heats up during gaming.\"\n",
    "\n",
    "response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a sentiment analysis classifier.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify the sentiment of the following review as POSITIVE, NEGATIVE, or MIXED.\\n\\nReview: {review}\\n\\nSentiment:\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(f\"Review: {review}\")\n",
    "print(f\"Sentiment: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot: Text Summarization\n",
    "\n",
    "article = \"\"\"\n",
    "Researchers at MIT have developed a new AI system that can predict protein structures \n",
    "with unprecedented accuracy. The system, called ProteinFlow, uses a novel graph neural \n",
    "network architecture that models amino acid interactions at multiple scales simultaneously. \n",
    "In benchmarks against existing methods including AlphaFold, ProteinFlow achieved a 15% \n",
    "improvement in prediction accuracy for proteins with more than 500 residues. The team \n",
    "believes this could accelerate drug discovery by reducing the time needed to understand \n",
    "protein-drug interactions from months to hours. The research was published in Nature \n",
    "Methods and the code has been released as open source.\n",
    "\"\"\"\n",
    "\n",
    "summary = chat([\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize the following article in exactly 2 sentences:\\n\\n{article}\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot: Translation\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet.\"\n",
    "\n",
    "for language in [\"French\", \"Japanese\", \"Spanish\"]:\n",
    "    translation = chat([\n",
    "        {\"role\": \"user\", \"content\": f\"Translate the following English text to {language}. Output only the translation, nothing else.\\n\\n{text}\"}\n",
    "    ], temperature=0)\n",
    "    print(f\"{language}: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0005-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Few-Shot Prompting\n",
    "\n",
    "**Few-shot prompting** provides the model with **2-3 examples** of the desired input-output behavior before presenting the actual task. This is one of the most powerful techniques because:\n",
    "\n",
    "- It **demonstrates** the expected format and style\n",
    "- It reduces **ambiguity** in the instruction\n",
    "- It acts as **implicit fine-tuning** at inference time\n",
    "\n",
    "The examples can be provided either as user/assistant message pairs or within a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot: Sentiment Classification (same task as zero-shot above)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sentiment classifier. Respond with exactly one word: POSITIVE, NEGATIVE, or MIXED.\"},\n",
    "    # Example 1\n",
    "    {\"role\": \"user\", \"content\": \"Review: This laptop is amazing! Fast, lightweight, and the screen is gorgeous.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"POSITIVE\"},\n",
    "    # Example 2\n",
    "    {\"role\": \"user\", \"content\": \"Review: Terrible product. Broke after one week and customer service was unhelpful.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "    # Example 3\n",
    "    {\"role\": \"user\", \"content\": \"Review: The food was great but the service was slow and the restaurant was too noisy.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"MIXED\"},\n",
    "    # Actual task\n",
    "    {\"role\": \"user\", \"content\": \"Review: The battery life is incredible and the camera quality exceeded my expectations, but the phone heats up during gaming.\"}\n",
    "]\n",
    "\n",
    "response = chat(messages, temperature=0)\n",
    "print(f\"Few-shot sentiment: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot: Custom entity extraction\n",
    "# This task is hard for zero-shot because the output format is very specific\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Extract product names and their associated sentiment from reviews. Format: PRODUCT: sentiment\"},\n",
    "    # Example 1\n",
    "    {\"role\": \"user\", \"content\": \"I love my new MacBook Pro but the Magic Mouse is uncomfortable.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"MacBook Pro: positive\\nMagic Mouse: negative\"},\n",
    "    # Example 2\n",
    "    {\"role\": \"user\", \"content\": \"The AirPods Max sound quality is decent for the price. My old Sony WH-1000XM4 were better though.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"AirPods Max: neutral\\nSony WH-1000XM4: positive\"},\n",
    "    # Actual task\n",
    "    {\"role\": \"user\", \"content\": \"Switched from Slack to Microsoft Teams and I'm really struggling. At least the Outlook integration works well.\"}\n",
    "]\n",
    "\n",
    "response = chat(messages, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing zero-shot vs few-shot on the same task:\n",
    "# Classifying whether a headline is about technology, sports, or politics\n",
    "\n",
    "headlines = [\n",
    "    \"New Quantum Chip Breaks Speed Record for Complex Calculations\",\n",
    "    \"City Council Approves $2B Infrastructure Bill After Marathon Debate\",\n",
    "    \"Underdog Team Clinches Championship in Overtime Thriller\",\n",
    "]\n",
    "\n",
    "print(\"=== Zero-shot ===\")\n",
    "for h in headlines:\n",
    "    resp = chat([\n",
    "        {\"role\": \"user\", \"content\": f\"Classify this headline into one category: TECHNOLOGY, SPORTS, or POLITICS.\\n\\nHeadline: {h}\\n\\nCategory:\"}\n",
    "    ], temperature=0)\n",
    "    print(f\"  {h[:50]}... -> {resp}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Few-shot ===\")\n",
    "for h in headlines:\n",
    "    resp = chat([\n",
    "        {\"role\": \"system\", \"content\": \"Classify headlines. Respond with one word only.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Headline: Apple Unveils New M4 Processor at WWDC Keynote\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"TECHNOLOGY\"},\n",
    "        {\"role\": \"user\", \"content\": \"Headline: Senate Passes Bipartisan Climate Legislation\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"POLITICS\"},\n",
    "        {\"role\": \"user\", \"content\": \"Headline: World Cup Final Draws Record 1.5 Billion Viewers\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SPORTS\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Headline: {h}\"}\n",
    "    ], temperature=0)\n",
    "    print(f\"  {h[:50]}... -> {resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0005-0005-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 1: Zero-Shot vs Few-Shot Comparison on Classification\n",
    "\n",
    "Classify 5 customer reviews as **POSITIVE**, **NEGATIVE**, or **NEUTRAL** using both zero-shot and few-shot prompting. Compare the results.\n",
    "\n",
    "The reviews below have ground-truth labels. Your goal is to see which approach produces more accurate classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-0006-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: TODO\n",
    "\n",
    "# Test data with ground truth labels\n",
    "test_reviews = [\n",
    "    {\"text\": \"Absolutely love this product! Best purchase I've made all year.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"The item arrived damaged and the return process was a nightmare.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"It works as described. Nothing special but gets the job done.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Waste of money. Stopped working after two days.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Pretty good overall. The quality is decent for the price point.\", \"label\": \"POSITIVE\"},\n",
    "]\n",
    "\n",
    "# TODO: Implement zero-shot classification\n",
    "# For each review, call chat() with a zero-shot prompt and collect the predicted label.\n",
    "zero_shot_predictions = None  # Replace with a list of predicted labels\n",
    "\n",
    "# TODO: Implement few-shot classification\n",
    "# For each review, call chat() with a few-shot prompt (include 2-3 examples as\n",
    "# user/assistant pairs) and collect the predicted label.\n",
    "few_shot_predictions = None  # Replace with a list of predicted labels\n",
    "\n",
    "# TODO: Compare accuracy\n",
    "# Calculate accuracy for each approach by comparing predictions to ground truth labels.\n",
    "zero_shot_accuracy = None  # Replace with calculated accuracy\n",
    "few_shot_accuracy = None  # Replace with calculated accuracy\n",
    "\n",
    "# TODO: Print a comparison table\n",
    "# Print each review, its ground truth, zero-shot prediction, and few-shot prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0005-0007-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-0008-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Solution\n",
    "\n",
    "test_reviews = [\n",
    "    {\"text\": \"Absolutely love this product! Best purchase I've made all year.\", \"label\": \"POSITIVE\"},\n",
    "    {\"text\": \"The item arrived damaged and the return process was a nightmare.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"It works as described. Nothing special but gets the job done.\", \"label\": \"NEUTRAL\"},\n",
    "    {\"text\": \"Waste of money. Stopped working after two days.\", \"label\": \"NEGATIVE\"},\n",
    "    {\"text\": \"Pretty good overall. The quality is decent for the price point.\", \"label\": \"POSITIVE\"},\n",
    "]\n",
    "\n",
    "# --- Zero-shot ---\n",
    "zero_shot_predictions = []\n",
    "for review in test_reviews:\n",
    "    resp = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a sentiment classifier. Respond with exactly one word: POSITIVE, NEGATIVE, or NEUTRAL.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the sentiment of this review:\\n\\n{review['text']}\"}\n",
    "    ], temperature=0)\n",
    "    # Extract just the label (strip whitespace and take first word in case of extra text)\n",
    "    pred = resp.strip().split()[0].upper()\n",
    "    zero_shot_predictions.append(pred)\n",
    "\n",
    "# --- Few-shot ---\n",
    "few_shot_predictions = []\n",
    "for review in test_reviews:\n",
    "    resp = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a sentiment classifier. Respond with exactly one word: POSITIVE, NEGATIVE, or NEUTRAL.\"},\n",
    "        # Example 1\n",
    "        {\"role\": \"user\", \"content\": \"Review: This is hands down the best phone I have ever owned.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"POSITIVE\"},\n",
    "        # Example 2\n",
    "        {\"role\": \"user\", \"content\": \"Review: Terrible experience. The product broke immediately and support ignored me.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        # Example 3\n",
    "        {\"role\": \"user\", \"content\": \"Review: It's okay. Does what it says, nothing more nothing less.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEUTRAL\"},\n",
    "        # Actual review\n",
    "        {\"role\": \"user\", \"content\": f\"Review: {review['text']}\"}\n",
    "    ], temperature=0)\n",
    "    pred = resp.strip().split()[0].upper()\n",
    "    few_shot_predictions.append(pred)\n",
    "\n",
    "# --- Compare accuracy ---\n",
    "ground_truth = [r[\"label\"] for r in test_reviews]\n",
    "zero_shot_correct = sum(1 for gt, pred in zip(ground_truth, zero_shot_predictions) if gt == pred)\n",
    "few_shot_correct = sum(1 for gt, pred in zip(ground_truth, few_shot_predictions) if gt == pred)\n",
    "\n",
    "zero_shot_accuracy = zero_shot_correct / len(test_reviews)\n",
    "few_shot_accuracy = few_shot_correct / len(test_reviews)\n",
    "\n",
    "# --- Print comparison table ---\n",
    "print(f\"{'Review (truncated)':<55} {'Truth':<10} {'Zero-shot':<12} {'Few-shot':<10}\")\n",
    "print(\"-\" * 87)\n",
    "for review, zs, fs in zip(test_reviews, zero_shot_predictions, few_shot_predictions):\n",
    "    text = review['text'][:52] + \"...\" if len(review['text']) > 52 else review['text']\n",
    "    match_zs = \"ok\" if zs == review['label'] else \"WRONG\"\n",
    "    match_fs = \"ok\" if fs == review['label'] else \"WRONG\"\n",
    "    print(f\"{text:<55} {review['label']:<10} {zs:<6}{match_zs:<6} {fs:<6}{match_fs}\")\n",
    "\n",
    "print()\n",
    "print(f\"Zero-shot accuracy: {zero_shot_accuracy:.0%} ({zero_shot_correct}/{len(test_reviews)})\")\n",
    "print(f\"Few-shot accuracy:  {few_shot_accuracy:.0%} ({few_shot_correct}/{len(test_reviews)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "**Chain-of-thought prompting** encourages the model to **show its reasoning step by step** before arriving at a final answer. This technique was introduced by Wei et al. (2022) and dramatically improves performance on tasks requiring:\n",
    "\n",
    "- Mathematical reasoning\n",
    "- Multi-step logic\n",
    "- Word problems\n",
    "- Common-sense reasoning\n",
    "\n",
    "There are two variants:\n",
    "1. **Zero-shot CoT**: Simply add \"Let's think step by step\" to the prompt\n",
    "2. **Few-shot CoT**: Provide examples that include the reasoning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0006-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Standard prompting vs CoT on a math word problem\n",
    "\n",
    "problem = \"\"\"A store sells apples for $2 each and oranges for $3 each. \n",
    "If Sarah buys 4 apples and 3 oranges, and she pays with a $20 bill, \n",
    "how much change does she receive?\"\"\"\n",
    "\n",
    "# Standard prompting\n",
    "standard_response = chat([\n",
    "    {\"role\": \"user\", \"content\": f\"{problem}\\n\\nAnswer:\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== Standard Prompting ===\")\n",
    "print(standard_response)\n",
    "print()\n",
    "\n",
    "# Zero-shot CoT: Just add \"Let's think step by step\"\n",
    "cot_response = chat([\n",
    "    {\"role\": \"user\", \"content\": f\"{problem}\\n\\nLet's think step by step.\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== Zero-shot CoT ===\")\n",
    "print(cot_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0006-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A harder problem where CoT really shines\n",
    "\n",
    "hard_problem = \"\"\"A farmer has a rectangular field that is 120 meters long and 80 meters wide.\n",
    "He wants to build a fence around the entire field, plus a fence down the middle \n",
    "dividing it into two equal halves (parallel to the shorter side).\n",
    "If fencing costs $15 per meter, how much will the total fencing cost?\"\"\"\n",
    "\n",
    "# Standard\n",
    "print(\"=== Standard ===\")\n",
    "resp = chat([{\"role\": \"user\", \"content\": f\"{hard_problem}\\n\\nProvide just the final answer.\"}], temperature=0)\n",
    "print(resp)\n",
    "print()\n",
    "\n",
    "# CoT\n",
    "print(\"=== Chain-of-Thought ===\")\n",
    "resp = chat([{\"role\": \"user\", \"content\": f\"{hard_problem}\\n\\nLet's think step by step.\"}], temperature=0)\n",
    "print(resp)\n",
    "print()\n",
    "\n",
    "# The correct answer:\n",
    "# Perimeter = 2*(120+80) = 400m\n",
    "# Middle fence (parallel to shorter side = 80m) = 80m\n",
    "# Total fencing = 400 + 80 = 480m\n",
    "# Cost = 480 * $15 = $7,200\n",
    "print(\"Correct answer: $7,200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0006-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot CoT: Providing reasoning examples\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Solve math word problems step by step. Show your reasoning, then give the final answer on the last line as 'ANSWER: <number>'.\"},\n",
    "    # Example with reasoning\n",
    "    {\"role\": \"user\", \"content\": \"If a train travels at 60 km/h for 2.5 hours, how far does it go?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Step 1: I need to find distance using the formula: distance = speed x time.\\nStep 2: speed = 60 km/h, time = 2.5 hours\\nStep 3: distance = 60 x 2.5 = 150 km\\n\\nANSWER: 150 km\"},\n",
    "    # Another example\n",
    "    {\"role\": \"user\", \"content\": \"A shirt costs $40. It's on sale for 25% off. What's the sale price?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Step 1: Calculate the discount amount: 25% of $40 = 0.25 x 40 = $10\\nStep 2: Subtract the discount from the original price: $40 - $10 = $30\\n\\nANSWER: $30\"},\n",
    "    # Actual problem\n",
    "    {\"role\": \"user\", \"content\": \"A water tank holds 500 liters. It is currently 60% full. If water is added at a rate of 20 liters per minute, how many minutes until the tank is completely full?\"}\n",
    "]\n",
    "\n",
    "response = chat(messages, temperature=0)\n",
    "print(\"=== Few-shot CoT ===\")\n",
    "print(response)\n",
    "print()\n",
    "print(\"Correct answer: 10 minutes (need 200 liters at 20 L/min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-0005-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 2: Chain-of-Thought for Math Word Problems\n",
    "\n",
    "Test 5 math problems with **standard prompting** vs **chain-of-thought prompting**. Compare the accuracy of each approach.\n",
    "\n",
    "Use `temperature=0` for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0006-0006-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: TODO\n",
    "\n",
    "math_problems = [\n",
    "    {\n",
    "        \"question\": \"A bookstore sells 3 books for $12 each and 2 books for $8 each. What is the total cost?\",\n",
    "        \"answer\": 52\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If you have 156 marbles and give away 1/3 of them, then receive 20 more, how many do you have?\",\n",
    "        \"answer\": 124\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A car travels 180 miles in 3 hours. If it then speeds up by 20 mph, how far will it travel in the next 2 hours?\",\n",
    "        \"answer\": 160\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A rectangular garden is 15m long and 8m wide. If you want to put a 1m wide path around the entire garden, what is the area of just the path?\",\n",
    "        \"answer\": 52\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Three friends split a dinner bill. The total was $87. They each add a 20% tip on the original total. How much does each person pay in total (bill + tip)?\",\n",
    "        \"answer\": 34.8\n",
    "    },\n",
    "]\n",
    "\n",
    "# TODO: For each problem, get a response using standard prompting.\n",
    "# Instruct the model to reply with just the numeric answer.\n",
    "standard_results = None  # Replace with list of responses\n",
    "\n",
    "# TODO: For each problem, get a response using CoT prompting.\n",
    "# Add \"Let's think step by step\" and ask for the final answer on the last line.\n",
    "cot_results = None  # Replace with list of responses\n",
    "\n",
    "# TODO: Extract numeric answers from both and compare to ground truth.\n",
    "# Print a comparison showing which approach got each problem right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-0007-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0006-0008-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Solution\n",
    "import re\n",
    "\n",
    "math_problems = [\n",
    "    {\n",
    "        \"question\": \"A bookstore sells 3 books for $12 each and 2 books for $8 each. What is the total cost?\",\n",
    "        \"answer\": 52\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If you have 156 marbles and give away 1/3 of them, then receive 20 more, how many do you have?\",\n",
    "        \"answer\": 124\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A car travels 180 miles in 3 hours. If it then speeds up by 20 mph, how far will it travel in the next 2 hours?\",\n",
    "        \"answer\": 160\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A rectangular garden is 15m long and 8m wide. If you want to put a 1m wide path around the entire garden, what is the area of just the path?\",\n",
    "        \"answer\": 52\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Three friends split a dinner bill. The total was $87. They each add a 20% tip on the original total. How much does each person pay in total (bill + tip)?\",\n",
    "        \"answer\": 34.8\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Extract the last number from a string (likely the final answer).\"\"\"\n",
    "    numbers = re.findall(r'[\\d,]+\\.?\\d*', text.replace(',', ''))\n",
    "    if numbers:\n",
    "        return float(numbers[-1])\n",
    "    return None\n",
    "\n",
    "\n",
    "# Standard prompting\n",
    "standard_results = []\n",
    "for p in math_problems:\n",
    "    resp = chat([\n",
    "        {\"role\": \"user\", \"content\": f\"{p['question']}\\n\\nRespond with only the numeric answer.\"}\n",
    "    ], temperature=0)\n",
    "    standard_results.append(resp)\n",
    "\n",
    "# CoT prompting\n",
    "cot_results = []\n",
    "for p in math_problems:\n",
    "    resp = chat([\n",
    "        {\"role\": \"user\", \"content\": f\"{p['question']}\\n\\nLet's think step by step. After your reasoning, provide the final answer on the last line as ANSWER: <number>.\"}\n",
    "    ], temperature=0)\n",
    "    cot_results.append(resp)\n",
    "\n",
    "# Compare\n",
    "print(f\"{'Problem':<6} {'Correct':<10} {'Standard':<12} {'CoT':<12} {'Std OK?':<10} {'CoT OK?'}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "std_correct = 0\n",
    "cot_correct = 0\n",
    "\n",
    "for i, p in enumerate(math_problems):\n",
    "    std_num = extract_number(standard_results[i])\n",
    "    cot_num = extract_number(cot_results[i])\n",
    "    \n",
    "    std_ok = abs(std_num - p[\"answer\"]) < 0.1 if std_num else False\n",
    "    cot_ok = abs(cot_num - p[\"answer\"]) < 0.1 if cot_num else False\n",
    "    \n",
    "    if std_ok:\n",
    "        std_correct += 1\n",
    "    if cot_ok:\n",
    "        cot_correct += 1\n",
    "    \n",
    "    print(f\"{i+1:<6} {p['answer']:<10} {str(std_num):<12} {str(cot_num):<12} {'Yes' if std_ok else 'No':<10} {'Yes' if cot_ok else 'No'}\")\n",
    "\n",
    "print()\n",
    "print(f\"Standard accuracy: {std_correct}/{len(math_problems)} ({std_correct/len(math_problems):.0%})\")\n",
    "print(f\"CoT accuracy:      {cot_correct}/{len(math_problems)} ({cot_correct/len(math_problems):.0%})\")\n",
    "\n",
    "# Show full CoT for one problem\n",
    "print(\"\\n=== Example CoT reasoning (Problem 4) ===\")\n",
    "print(cot_results[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0007-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Role Prompting\n",
    "\n",
    "**Role prompting** sets a specific **persona** in the system message. This is powerful because it implicitly brings in:\n",
    "\n",
    "- Domain-specific vocabulary and knowledge\n",
    "- Appropriate level of detail\n",
    "- The right tone and communication style\n",
    "- Relevant frameworks and mental models\n",
    "\n",
    "Think of it as casting the model in a role before it starts performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same question, different roles\n",
    "\n",
    "question = \"How should I handle errors in my Python code?\"\n",
    "\n",
    "roles = {\n",
    "    \"Senior Python Developer\": \"You are a senior Python developer with 15 years of experience. You write clean, production-quality code and follow best practices. Be specific and include code examples.\",\n",
    "    \"Patient Teacher\": \"You are a patient, encouraging programming teacher for beginners. Use simple language, relatable analogies, and avoid jargon. Keep it short.\",\n",
    "    \"Code Reviewer\": \"You are a strict code reviewer at a top tech company. Focus on what can go wrong, edge cases, and potential security issues. Be direct.\"\n",
    "}\n",
    "\n",
    "for role_name, system_msg in roles.items():\n",
    "    print(f\"=== {role_name} ===\")\n",
    "    resp = chat([\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ])\n",
    "    # Print just the first 400 chars to keep output manageable\n",
    "    print(resp[:400] + \"...\" if len(resp) > 400 else resp)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role prompting for creative tasks\n",
    "\n",
    "topic = \"the importance of testing in software development\"\n",
    "\n",
    "# As a poet\n",
    "poem = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a witty poet who writes short, clever poems about technical topics. Write a 4-line poem.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Write a poem about: {topic}\"}\n",
    "], temperature=0.9)\n",
    "\n",
    "# As a stand-up comedian\n",
    "joke = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a stand-up comedian who specializes in tech humor. Tell a short joke (2-3 sentences max).\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Tell a joke about: {topic}\"}\n",
    "], temperature=0.9)\n",
    "\n",
    "print(\"=== Poem ===\")\n",
    "print(poem)\n",
    "print()\n",
    "print(\"=== Joke ===\")\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0008-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Structured Output\n",
    "\n",
    "One of the most practical prompt engineering skills is getting the model to produce **reliably structured output** -- especially JSON. This is critical for:\n",
    "\n",
    "- Building pipelines where LLM output feeds into downstream code\n",
    "- API responses\n",
    "- Data extraction and transformation\n",
    "- Automated workflows\n",
    "\n",
    "**Key techniques:**\n",
    "1. Specify the exact JSON schema in the system message\n",
    "2. Show an example of the desired output\n",
    "3. Tell the model to output **only** JSON (no extra text)\n",
    "4. Always validate with `json.loads()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting JSON output\n",
    "\n",
    "text = \"John Smith ordered 3 laptops at $999 each on January 15, 2025. The shipping address is 123 Main St, Springfield, IL 62701.\"\n",
    "\n",
    "response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"\"\"Extract structured data from text. Return ONLY valid JSON, no other text.\n",
    "Use this exact schema:\n",
    "{\n",
    "  \"customer_name\": \"string\",\n",
    "  \"items\": [{\"product\": \"string\", \"quantity\": number, \"unit_price\": number}],\n",
    "  \"total\": number,\n",
    "  \"date\": \"YYYY-MM-DD\",\n",
    "  \"shipping_address\": \"string\"\n",
    "}\"\"\"}, \n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"Raw response:\")\n",
    "print(response)\n",
    "print()\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    # Handle case where model wraps JSON in markdown code blocks\n",
    "    cleaned = response.strip()\n",
    "    if cleaned.startswith(\"```\"):\n",
    "        cleaned = cleaned.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0]\n",
    "    \n",
    "    data = json.loads(cleaned)\n",
    "    print(\"Parsed successfully!\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON parse error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting tabular output\n",
    "\n",
    "response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You format data as clean markdown tables. Only output the table, nothing else.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compare Python, JavaScript, and Rust across these dimensions: typing (static/dynamic), speed, learning curve, and primary use case. Keep entries brief (1-3 words each).\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list output with specific formatting\n",
    "\n",
    "response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You generate structured lists. For each item, use this exact format:\n",
    "[NUMBER]. TERM -- DEFINITION (one sentence)\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"List the 5 most important machine learning concepts for beginners.\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0008-0005-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 3: Design Prompts for Reliable JSON Output\n",
    "\n",
    "Create a prompt that extracts structured data from invoice text and returns valid JSON. The JSON should contain: `name`, `date`, `items` (list), and `total_amount`.\n",
    "\n",
    "Test it on multiple invoice texts and validate that the output is always parseable JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008-0006-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: TODO\n",
    "\n",
    "invoices = [\n",
    "    \"Invoice #1042 for Acme Corp, dated March 5, 2025. Items: 10 widgets at $5.99 each, 3 gadgets at $24.99 each. Total: $134.87.\",\n",
    "    \"Bill to: Jane Doe. Date: 2025-02-28. Services rendered: Website redesign ($2,500), SEO audit ($800), Content writing ($1,200). Grand total: $4,500.00\",\n",
    "    \"Receipt from Cloud Services Inc. on 01/15/2025 -- Monthly hosting $49.99, SSL certificate $12.00, Domain renewal $15.00. Amount due: $76.99.\",\n",
    "]\n",
    "\n",
    "# TODO: Create a system message that instructs the model to extract invoice data as JSON.\n",
    "# The JSON schema should be:\n",
    "# {\n",
    "#   \"name\": \"customer or company name\",\n",
    "#   \"date\": \"YYYY-MM-DD\",\n",
    "#   \"items\": [{\"description\": \"string\", \"amount\": number}],\n",
    "#   \"total_amount\": number\n",
    "# }\n",
    "system_message = None  # Replace with your system message string\n",
    "\n",
    "# TODO: Loop through invoices, call chat(), parse JSON, and print results.\n",
    "# Handle potential parsing errors gracefully.\n",
    "# Track how many invoices produced valid JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0008-0007-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008-0008-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Solution\n",
    "\n",
    "invoices = [\n",
    "    \"Invoice #1042 for Acme Corp, dated March 5, 2025. Items: 10 widgets at $5.99 each, 3 gadgets at $24.99 each. Total: $134.87.\",\n",
    "    \"Bill to: Jane Doe. Date: 2025-02-28. Services rendered: Website redesign ($2,500), SEO audit ($800), Content writing ($1,200). Grand total: $4,500.00\",\n",
    "    \"Receipt from Cloud Services Inc. on 01/15/2025 -- Monthly hosting $49.99, SSL certificate $12.00, Domain renewal $15.00. Amount due: $76.99.\",\n",
    "]\n",
    "\n",
    "system_message = \"\"\"You are a data extraction assistant. Extract invoice information from the provided text and return ONLY valid JSON.\n",
    "\n",
    "Use this exact schema (no additional fields, no missing fields):\n",
    "{\n",
    "  \"name\": \"customer or company name (string)\",\n",
    "  \"date\": \"YYYY-MM-DD (string)\",\n",
    "  \"items\": [\n",
    "    {\"description\": \"item description (string)\", \"amount\": <total for this line item as a number>}\n",
    "  ],\n",
    "  \"total_amount\": <total amount as a number>\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Output ONLY the JSON object. No markdown, no explanation, no code blocks.\n",
    "- All amounts should be numbers (not strings), without dollar signs.\n",
    "- Dates must be in YYYY-MM-DD format.\n",
    "- If a line item has quantity and unit price, compute the line total for the \"amount\" field.\"\"\"\n",
    "\n",
    "valid_count = 0\n",
    "\n",
    "for i, invoice_text in enumerate(invoices):\n",
    "    print(f\"=== Invoice {i+1} ===\")\n",
    "    print(f\"Input: {invoice_text[:80]}...\")\n",
    "    print()\n",
    "    \n",
    "    response = chat([\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": invoice_text}\n",
    "    ], temperature=0)\n",
    "    \n",
    "    # Clean up potential markdown code blocks\n",
    "    cleaned = response.strip()\n",
    "    if cleaned.startswith(\"```\"):\n",
    "        cleaned = cleaned.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(cleaned)\n",
    "        valid_count += 1\n",
    "        print(f\"  Valid JSON!\")\n",
    "        print(f\"  Name: {data['name']}\")\n",
    "        print(f\"  Date: {data['date']}\")\n",
    "        print(f\"  Items: {len(data['items'])}\")\n",
    "        for item in data['items']:\n",
    "            print(f\"    - {item['description']}: ${item['amount']}\")\n",
    "        print(f\"  Total: ${data['total_amount']}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"  PARSE ERROR: {e}\")\n",
    "        print(f\"  Raw response: {response[:200]}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nResults: {valid_count}/{len(invoices)} invoices produced valid JSON ({valid_count/len(invoices):.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0009-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Common Failures & Mitigations\n",
    "\n",
    "Even well-crafted prompts can fail. Understanding common failure modes helps you build more robust prompts.\n",
    "\n",
    "### Failure 1: Hallucination\n",
    "The model confidently states things that are **factually incorrect**.\n",
    "\n",
    "**Mitigations:**\n",
    "- Ask the model to cite sources or say \"I don't know\"\n",
    "- Add: \"Only use information that is definitely true. If unsure, say so.\"\n",
    "- Use retrieval-augmented generation (RAG) to ground responses in real data\n",
    "\n",
    "### Failure 2: Instruction Following Failures\n",
    "The model ignores or misinterprets part of the prompt.\n",
    "\n",
    "**Mitigations:**\n",
    "- Break complex instructions into numbered steps\n",
    "- Use delimiters (```, ---, XML tags) to separate sections\n",
    "- Repeat critical constraints\n",
    "- Test with adversarial inputs\n",
    "\n",
    "### Failure 3: Verbosity / Off-topic Responses\n",
    "The model produces much more text than needed or goes off on tangents.\n",
    "\n",
    "**Mitigations:**\n",
    "- Specify exact length: \"in 2 sentences\", \"in under 50 words\"\n",
    "- Add: \"Be concise. Do not include preamble or explanation.\"\n",
    "- Use structured output format to constrain the response\n",
    "\n",
    "### Failure 4: Context Window Limits\n",
    "For very long inputs, the model may lose track of information in the middle.\n",
    "\n",
    "**Mitigations:**\n",
    "- Put the most important information at the beginning or end\n",
    "- Summarize long documents before querying\n",
    "- Use chunking strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Hallucination risk\n",
    "\n",
    "# Bad: Encourages hallucination\n",
    "bad_response = chat([\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about the scientific research published by Dr. James Wellington of Stanford on quantum gravity in 2023.\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== Without guardrails (may hallucinate) ===\")\n",
    "print(bad_response[:300])\n",
    "print()\n",
    "\n",
    "# Better: Includes honesty constraints\n",
    "good_response = chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a research assistant. Only state facts you are confident about. If you are not sure about something, explicitly say 'I'm not certain about this' or 'I don't have verified information on this'. Never fabricate details.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about the scientific research published by Dr. James Wellington of Stanford on quantum gravity in 2023.\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== With honesty guardrails ===\")\n",
    "print(good_response[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Improving instruction following with delimiters and numbered steps\n",
    "\n",
    "# Bad: Vague instruction\n",
    "bad_prompt = \"Analyze this text and give me the key points and also translate it to French and summarize it.\"\n",
    "\n",
    "# Good: Structured with clear delimiters\n",
    "good_prompt = \"\"\"Perform the following 3 tasks on the text enclosed in <text> tags.\n",
    "\n",
    "<text>\n",
    "Machine learning is transforming healthcare by enabling early disease detection through \n",
    "medical imaging analysis. Recent studies show AI systems can identify certain cancers \n",
    "with accuracy comparable to experienced radiologists.\n",
    "</text>\n",
    "\n",
    "Tasks:\n",
    "1. KEY POINTS: List the 2-3 main points as bullet points.\n",
    "2. FRENCH TRANSLATION: Translate the original text to French.\n",
    "3. ONE-LINE SUMMARY: Summarize in exactly one sentence.\n",
    "\n",
    "Format your response with clear headers for each task.\"\"\"\n",
    "\n",
    "response = chat([{\"role\": \"user\", \"content\": good_prompt}], temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Iterative Refinement Workflow\n",
    "\n",
    "Great prompts are rarely written on the first try. The best prompt engineers follow an **iterative process**:\n",
    "\n",
    "1. **Start simple** -- Write a basic prompt\n",
    "2. **Test** -- Run it on several representative inputs\n",
    "3. **Analyze failures** -- Identify where it goes wrong and why\n",
    "4. **Refine** -- Add constraints, examples, or restructure\n",
    "5. **Repeat** -- Until quality meets your threshold\n",
    "\n",
    "Let's walk through this process for a real task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Extract action items from meeting notes\n",
    "# We'll iterate through 4 versions of the prompt\n",
    "\n",
    "meeting_notes = \"\"\"Team standup - Feb 10, 2025\n",
    "Attendees: Alice, Bob, Charlie\n",
    "\n",
    "Alice mentioned the API integration is behind schedule by 2 days. She'll reach out \n",
    "to the vendor today for updated documentation. Bob said he'll review the pull request \n",
    "for the auth module by EOD Wednesday. Charlie raised a concern about test coverage -- \n",
    "currently at 72%, target is 85%. He'll write unit tests for the payment module this week.\n",
    "Alice also noted we need someone to update the deployment runbook before Friday's release.\n",
    "Bob volunteered to handle that.\"\"\"\n",
    "\n",
    "# ---- Version 1: Naive ----\n",
    "v1 = chat([\n",
    "    {\"role\": \"user\", \"content\": f\"What are the action items from these meeting notes?\\n\\n{meeting_notes}\"}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== V1: Naive prompt ===\")\n",
    "print(v1)\n",
    "print()\n",
    "print(\"Issues: Output format is inconsistent, may miss details like deadlines.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Version 2: Add structure ----\n",
    "v2 = chat([\n",
    "    {\"role\": \"system\", \"content\": \"Extract action items from meeting notes. For each action item, include: WHO is responsible, WHAT they need to do, and WHEN it's due.\"},\n",
    "    {\"role\": \"user\", \"content\": meeting_notes}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== V2: Added structure (who/what/when) ===\")\n",
    "print(v2)\n",
    "print()\n",
    "print(\"Better! But format still varies. Let's enforce a specific format.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Version 3: Enforce format with example ----\n",
    "v3 = chat([\n",
    "    {\"role\": \"system\", \"content\": \"\"\"Extract action items from meeting notes. Return ONLY a numbered list.\n",
    "Each item MUST follow this exact format:\n",
    "N. [OWNER] -- ACTION -- Due: DEADLINE\n",
    "\n",
    "If no deadline is mentioned, write \"Due: Not specified\".\n",
    "Do not include any other text before or after the list.\"\"\"}, \n",
    "    {\"role\": \"user\", \"content\": meeting_notes}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== V3: Enforced format ===\")\n",
    "print(v3)\n",
    "print()\n",
    "print(\"Great format! But let's also add priority and make it JSON for downstream use.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0005-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Version 4: JSON output with priority ----\n",
    "v4 = chat([\n",
    "    {\"role\": \"system\", \"content\": \"\"\"Extract action items from meeting notes. Return ONLY a valid JSON array.\n",
    "\n",
    "Each action item must have these fields:\n",
    "- \"owner\": person responsible (string)\n",
    "- \"action\": what needs to be done (string, concise)\n",
    "- \"deadline\": when it's due (string, or \"unspecified\")\n",
    "- \"priority\": \"high\" if time-sensitive or blocking, \"medium\" otherwise\n",
    "\n",
    "Return ONLY the JSON array. No markdown formatting, no explanation.\"\"\"}, \n",
    "    {\"role\": \"user\", \"content\": meeting_notes}\n",
    "], temperature=0)\n",
    "\n",
    "print(\"=== V4: JSON with priority ===\")\n",
    "\n",
    "# Clean and parse\n",
    "cleaned = v4.strip()\n",
    "if cleaned.startswith(\"```\"):\n",
    "    cleaned = cleaned.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "\n",
    "try:\n",
    "    action_items = json.loads(cleaned)\n",
    "    print(json.dumps(action_items, indent=2))\n",
    "    print(f\"\\nExtracted {len(action_items)} action items. Valid JSON.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to parse. Raw response:\")\n",
    "    print(v4)\n",
    "\n",
    "print()\n",
    "print(\"This is production-ready! Each version improved on specific weaknesses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-0006-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 4: Prompt Evaluation Harness\n",
    "\n",
    "Create a mini evaluation framework:\n",
    "1. Define 5 prompt variants for the same task (summarizing a paragraph)\n",
    "2. Run each prompt on 5 test inputs\n",
    "3. Score outputs on a simple metric (e.g., length within target range)\n",
    "4. Print a comparison table showing which prompt variant performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0007-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: TODO\n",
    "\n",
    "# Test paragraphs to summarize\n",
    "test_paragraphs = [\n",
    "    \"Artificial intelligence has transformed the healthcare industry in remarkable ways. From early disease detection through medical imaging to drug discovery acceleration, AI systems are helping doctors make better decisions. Machine learning algorithms can now analyze thousands of medical images in minutes, identifying patterns that might take human experts hours to find. This has led to earlier diagnoses and improved patient outcomes across multiple specialties.\",\n",
    "    \"The global shift toward remote work has fundamentally changed how companies operate. Organizations have had to invest heavily in digital infrastructure, collaboration tools, and cybersecurity measures. While many employees appreciate the flexibility, some struggle with isolation and blurred work-life boundaries. Companies are now experimenting with hybrid models that aim to combine the best of both in-office and remote environments.\",\n",
    "    \"Climate change continues to be one of the most pressing challenges facing humanity. Rising global temperatures are causing more frequent extreme weather events, from devastating wildfires to unprecedented flooding. Scientists warn that without significant reductions in greenhouse gas emissions, many coastal cities could face severe flooding by 2050. International cooperation and technological innovation are both critical to addressing this crisis.\",\n",
    "    \"The electric vehicle market has experienced explosive growth over the past five years. Major automakers have committed billions of dollars to EV development, and charging infrastructure is expanding rapidly. Battery technology improvements have increased range while reducing costs, making EVs more accessible to average consumers. However, challenges remain around raw material sourcing, grid capacity, and recycling of spent batteries.\",\n",
    "    \"Quantum computing represents a paradigm shift in computational capability. Unlike classical computers that use bits, quantum computers use qubits that can exist in multiple states simultaneously. This allows them to solve certain problems exponentially faster than traditional machines. While still in early stages, quantum computing shows promise for cryptography, materials science, financial modeling, and drug discovery.\"\n",
    "]\n",
    "\n",
    "# TODO: Define 5 different prompt variants for summarization.\n",
    "# Each should aim to produce a 1-2 sentence summary, but use different techniques.\n",
    "prompt_variants = None  # Replace with a list of 5 system message strings\n",
    "\n",
    "# TODO: Run each variant on all test paragraphs and collect results.\n",
    "# results should be a dict: {variant_name: [summary1, summary2, ..., summary5]}\n",
    "results = None\n",
    "\n",
    "# TODO: Score each output. A good score means:\n",
    "# - Length is between 20 and 60 words (target range for a 1-2 sentence summary)\n",
    "# - Score 1 if within range, 0 if outside\n",
    "\n",
    "# TODO: Print a comparison table with average scores per variant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-0008-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0009-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Solution\n",
    "\n",
    "test_paragraphs = [\n",
    "    \"Artificial intelligence has transformed the healthcare industry in remarkable ways. From early disease detection through medical imaging to drug discovery acceleration, AI systems are helping doctors make better decisions. Machine learning algorithms can now analyze thousands of medical images in minutes, identifying patterns that might take human experts hours to find. This has led to earlier diagnoses and improved patient outcomes across multiple specialties.\",\n",
    "    \"The global shift toward remote work has fundamentally changed how companies operate. Organizations have had to invest heavily in digital infrastructure, collaboration tools, and cybersecurity measures. While many employees appreciate the flexibility, some struggle with isolation and blurred work-life boundaries. Companies are now experimenting with hybrid models that aim to combine the best of both in-office and remote environments.\",\n",
    "    \"Climate change continues to be one of the most pressing challenges facing humanity. Rising global temperatures are causing more frequent extreme weather events, from devastating wildfires to unprecedented flooding. Scientists warn that without significant reductions in greenhouse gas emissions, many coastal cities could face severe flooding by 2050. International cooperation and technological innovation are both critical to addressing this crisis.\",\n",
    "    \"The electric vehicle market has experienced explosive growth over the past five years. Major automakers have committed billions of dollars to EV development, and charging infrastructure is expanding rapidly. Battery technology improvements have increased range while reducing costs, making EVs more accessible to average consumers. However, challenges remain around raw material sourcing, grid capacity, and recycling of spent batteries.\",\n",
    "    \"Quantum computing represents a paradigm shift in computational capability. Unlike classical computers that use bits, quantum computers use qubits that can exist in multiple states simultaneously. This allows them to solve certain problems exponentially faster than traditional machines. While still in early stages, quantum computing shows promise for cryptography, materials science, financial modeling, and drug discovery.\"\n",
    "]\n",
    "\n",
    "# 5 prompt variants -- different strategies for the same summarization task\n",
    "prompt_variants = {\n",
    "    \"V1-Basic\": \"Summarize the following text.\",\n",
    "    \"V2-Constrained\": \"Summarize the following text in exactly 1-2 sentences. Be concise.\",\n",
    "    \"V3-WordLimit\": \"Summarize the following text in 20-50 words. Do not exceed 50 words.\",\n",
    "    \"V4-RoleBased\": \"You are a news editor writing headlines and brief summaries. Summarize the following text in 1-2 crisp sentences suitable for a news digest.\",\n",
    "    \"V5-Template\": \"Summarize the following text using this template: '[Topic] is [key development], resulting in [impact/consequence].' Fill in the bracketed parts. One sentence only.\"\n",
    "}\n",
    "\n",
    "# Run all variants on all paragraphs\n",
    "results = {}\n",
    "for variant_name, system_msg in prompt_variants.items():\n",
    "    summaries = []\n",
    "    for paragraph in test_paragraphs:\n",
    "        resp = chat([\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": paragraph}\n",
    "        ], temperature=0)\n",
    "        summaries.append(resp)\n",
    "    results[variant_name] = summaries\n",
    "\n",
    "# Score: word count within target range (20-60 words)\n",
    "TARGET_MIN = 20\n",
    "TARGET_MAX = 60\n",
    "\n",
    "print(f\"{'Variant':<16} {'Avg Words':<12} {'In Range':<12} {'Score':<8} {'Word Counts'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for variant_name, summaries in results.items():\n",
    "    word_counts = [len(s.split()) for s in summaries]\n",
    "    in_range = sum(1 for wc in word_counts if TARGET_MIN <= wc <= TARGET_MAX)\n",
    "    avg_words = sum(word_counts) / len(word_counts)\n",
    "    score = in_range / len(summaries)\n",
    "    \n",
    "    print(f\"{variant_name:<16} {avg_words:<12.1f} {in_range}/{len(summaries):<10} {score:<8.0%} {word_counts}\")\n",
    "\n",
    "# Show the best variant's outputs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "best_variant = max(results.keys(), key=lambda v: sum(\n",
    "    1 for s in results[v] if TARGET_MIN <= len(s.split()) <= TARGET_MAX\n",
    "))\n",
    "print(f\"Best variant: {best_variant}\")\n",
    "print(\"\\nSample outputs:\")\n",
    "for i, summary in enumerate(results[best_variant][:3]):\n",
    "    print(f\"  [{i+1}] ({len(summary.split())} words) {summary}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0011-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Prompt Templates & Parameterization\n",
    "\n",
    "In production systems, you rarely write one-off prompts. Instead, you create **reusable templates** that accept parameters. This ensures:\n",
    "\n",
    "- **Consistency** across many invocations\n",
    "- **Maintainability** -- change the template in one place\n",
    "- **Testability** -- systematically test with different inputs\n",
    "\n",
    "Python f-strings and `.format()` are the simplest approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple template with f-strings\n",
    "\n",
    "def summarize(text, length=\"1-2 sentences\", audience=\"general\"):\n",
    "    \"\"\"Reusable summarization prompt template.\"\"\"\n",
    "    system_msg = f\"\"\"You are a summarization assistant. Write summaries for a {audience} audience.\n",
    "Always respond in {length}. Be concise and informative.\"\"\"\n",
    "    \n",
    "    return chat([\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize:\\n\\n{text}\"}\n",
    "    ], temperature=0)\n",
    "\n",
    "\n",
    "sample_text = \"\"\"The James Webb Space Telescope has captured unprecedented images of distant \n",
    "galaxies, revealing structures that formed just 300 million years after the Big Bang. \n",
    "These observations challenge existing models of galaxy formation and suggest that the \n",
    "early universe was more complex than previously thought.\"\"\"\n",
    "\n",
    "print(\"=== General audience, 1-2 sentences ===\")\n",
    "print(summarize(sample_text))\n",
    "print()\n",
    "\n",
    "print(\"=== Expert audience, 1 sentence ===\")\n",
    "print(summarize(sample_text, length=\"exactly 1 sentence\", audience=\"astrophysics researcher\"))\n",
    "print()\n",
    "\n",
    "print(\"=== Child audience, 2-3 sentences ===\")\n",
    "print(summarize(sample_text, length=\"2-3 simple sentences\", audience=\"10-year-old child\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced template: a reusable analyzer\n",
    "\n",
    "ANALYSIS_TEMPLATE = \"\"\"Analyze the following {content_type} and provide:\n",
    "1. MAIN TOPIC: One sentence describing the primary subject.\n",
    "2. KEY POINTS: 2-3 bullet points with the most important information.\n",
    "3. TONE: The overall tone (e.g., formal, casual, urgent, informative).\n",
    "4. TARGET AUDIENCE: Who this content is written for.\n",
    "{extra_instructions}\n",
    "\n",
    "Content to analyze:\n",
    "---\n",
    "{content}\n",
    "---\"\"\"\n",
    "\n",
    "\n",
    "def analyze_content(content, content_type=\"text\", extra_instructions=\"\"):\n",
    "    \"\"\"Reusable content analysis template.\"\"\"\n",
    "    prompt = ANALYSIS_TEMPLATE.format(\n",
    "        content_type=content_type,\n",
    "        content=content,\n",
    "        extra_instructions=extra_instructions\n",
    "    )\n",
    "    return chat([{\"role\": \"user\", \"content\": prompt}], temperature=0)\n",
    "\n",
    "\n",
    "# Use the template\n",
    "email = \"\"\"Hi team, just a heads up that we're pushing the release date to next Friday \n",
    "due to the critical bug found in the payment module. Please prioritize fixing issue #342 \n",
    "and update your sprint boards accordingly. Let's sync at tomorrow's standup.\"\"\"\n",
    "\n",
    "print(analyze_content(\n",
    "    content=email,\n",
    "    content_type=\"internal team email\",\n",
    "    extra_instructions=\"5. URGENCY: Rate from 1-5 how urgent this communication is.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template pattern: Building a prompt library\n",
    "\n",
    "PROMPT_LIBRARY = {\n",
    "    \"classify_sentiment\": {\n",
    "        \"system\": \"You are a sentiment classifier. Respond with exactly one word: POSITIVE, NEGATIVE, or NEUTRAL.\",\n",
    "        \"user\": \"Classify the sentiment of this text:\\n\\n{text}\"\n",
    "    },\n",
    "    \"extract_keywords\": {\n",
    "        \"system\": \"Extract the {count} most important keywords from the text. Return them as a comma-separated list. No other text.\",\n",
    "        \"user\": \"{text}\"\n",
    "    },\n",
    "    \"rewrite_tone\": {\n",
    "        \"system\": \"Rewrite the given text in a {tone} tone. Keep the same meaning but change the style. Output only the rewritten text.\",\n",
    "        \"user\": \"{text}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def run_prompt(template_name, **kwargs):\n",
    "    \"\"\"Run a prompt from the library with the given parameters.\"\"\"\n",
    "    template = PROMPT_LIBRARY[template_name]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": template[\"system\"].format(**kwargs)},\n",
    "        {\"role\": \"user\", \"content\": template[\"user\"].format(**kwargs)}\n",
    "    ]\n",
    "    return chat(messages, temperature=0)\n",
    "\n",
    "\n",
    "sample = \"The new software update completely broke my workflow. Three hours wasted trying to fix compatibility issues.\"\n",
    "\n",
    "print(\"Sentiment:\", run_prompt(\"classify_sentiment\", text=sample))\n",
    "print(\"Keywords:\", run_prompt(\"extract_keywords\", text=sample, count=5))\n",
    "print(\"Formal rewrite:\", run_prompt(\"rewrite_tone\", text=sample, tone=\"formal and professional\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0012-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary & References\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Prompt anatomy matters**: The system message sets behavior; user messages provide the task; assistant messages enable few-shot learning and multi-turn context.\n",
    "\n",
    "2. **Zero-shot works for simple tasks**: Direct, specific instructions can get surprisingly good results without examples.\n",
    "\n",
    "3. **Few-shot improves consistency**: Providing 2-3 examples dramatically improves format adherence and edge-case handling.\n",
    "\n",
    "4. **Chain-of-thought unlocks reasoning**: Adding \"Let's think step by step\" or providing reasoning examples significantly improves accuracy on math, logic, and multi-step problems.\n",
    "\n",
    "5. **Roles shape output**: Setting a persona in the system message implicitly adjusts vocabulary, depth, tone, and style.\n",
    "\n",
    "6. **Structured output requires explicit schemas**: Specify the exact JSON schema, provide examples, and always validate with `json.loads()`.\n",
    "\n",
    "7. **Iterate systematically**: Start simple, test on representative inputs, identify failure modes, and refine. Track quality at each iteration.\n",
    "\n",
    "8. **Use templates in production**: Parameterized prompts ensure consistency, maintainability, and testability.\n",
    "\n",
    "### References\n",
    "\n",
    "**Papers:**\n",
    "- Wei et al., \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" (2022). [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)\n",
    "- Brown et al., \"Language Models are Few-Shot Learners\" (GPT-3, 2020). [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)\n",
    "- Kojima et al., \"Large Language Models are Zero-Shot Reasoners\" (2022). [arXiv:2205.11916](https://arxiv.org/abs/2205.11916)\n",
    "\n",
    "**Guides:**\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering Guide](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)\n",
    "- [learnprompting.org](https://learnprompting.org/) -- Community-driven prompt engineering tutorials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}