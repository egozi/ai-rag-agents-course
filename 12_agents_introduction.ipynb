{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Introduction to AI Agents\n",
    "\n",
    "## What Are AI Agents?\n",
    "\n",
    "AI agents are **autonomous systems** that use Large Language Models (LLMs) as their reasoning engine to **decide which actions to take** in order to achieve a goal. Unlike simple LLM calls where you send a prompt and get a response, agents can:\n",
    "\n",
    "- **Use tools** to interact with the outside world (search the web, run code, query databases)\n",
    "- **Make decisions** about which tool to use and when\n",
    "- **Iterate** on their approach based on intermediate results\n",
    "- **Chain multiple steps** together to solve complex problems\n",
    "\n",
    "### Simple LLM Call vs. Agent\n",
    "\n",
    "| Simple LLM Call | AI Agent |\n",
    "|---|---|\n",
    "| Single input, single output | Multi-step reasoning |\n",
    "| No external tools | Can call tools (APIs, databases, code) |\n",
    "| Stateless | Maintains context across steps |\n",
    "| Deterministic flow | Dynamic flow based on LLM decisions |\n",
    "| Limited to training data | Can fetch real-time information |\n",
    "\n",
    "### Historical Note\n",
    "\n",
    "This repository contains `langchain_agent.ipynb` which demonstrates an earlier approach using deprecated LangChain APIs (`LLMChain`, `Tool.from_function`, `initialize_agent`). In this module, we use the **modern LangChain APIs** (`@tool` decorator, `create_react_agent`, `AgentExecutor`) which are more robust and better supported.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the required packages and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai python-dotenv langchain langchain-openai langchain-community requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_react_agent, create_tool_calling_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"/home/amir/source/.env\")\n",
    "\n",
    "print(\"Environment loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client (for the raw API demo)\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Initialize LangChain LLM (for agent demos)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Clients initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Agent Loop\n",
    "\n",
    "At the heart of every AI agent is a simple loop:\n",
    "\n",
    "```\n",
    "                +------------------+\n",
    "                |                  |\n",
    "                v                  |\n",
    "  +----------+     +----------+   |\n",
    "  | OBSERVE  |---->|  THINK   |   |\n",
    "  +----------+     +----------+   |\n",
    "       ^                |         |\n",
    "       |                v         |\n",
    "  +----------+     +----------+   |\n",
    "  | OBSERVE  |<----+   ACT    +---+\n",
    "  | (result) |     +----------+\n",
    "  +----------+\n",
    "       |\n",
    "       v\n",
    "  +----------+\n",
    "  |  FINAL   |\n",
    "  | ANSWER   |\n",
    "  +----------+\n",
    "```\n",
    "\n",
    "1. **Observe**: The agent receives input (user query or tool result)\n",
    "2. **Think**: The LLM reasons about what to do next\n",
    "3. **Act**: The agent calls a tool or returns a final answer\n",
    "4. **Observe** (again): If a tool was called, the agent observes the result and loops back to Think\n",
    "\n",
    "This cycle continues until the agent decides it has enough information to provide a final answer.\n",
    "\n",
    "### Demo: Manual Agent Loop with OpenAI Function Calling\n",
    "\n",
    "Let's implement this loop manually using the OpenAI API to understand what's happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple calculator tool for OpenAI function calling\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculator\",\n",
    "            \"description\": \"Evaluate a mathematical expression. Supports basic arithmetic (+, -, *, /, **) and common math functions.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The mathematical expression to evaluate, e.g. '2 + 3 * 4'\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Safely evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        # Only allow safe math operations\n",
    "        allowed_chars = set(\"0123456789+-*/.() \")\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return f\"Error: Expression contains invalid characters.\"\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "print(\"Tool defined. Testing calculator:\")\n",
    "print(f\"  calculator('2 + 3 * 4') = {calculator('2 + 3 * 4')}\")\n",
    "print(f\"  calculator('(10 + 5) / 3') = {calculator('(10 + 5) / 3')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_loop(user_message: str) -> str:\n",
    "    \"\"\"\n",
    "    Manually implement the Observe -> Think -> Act -> Observe loop\n",
    "    using OpenAI's function calling API.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the calculator tool when you need to perform math calculations. Always show your reasoning.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    print(f\"User: {user_message}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    max_iterations = 5\n",
    "    for i in range(max_iterations):\n",
    "        print(f\"\\n--- Iteration {i + 1} ---\")\n",
    "\n",
    "        # THINK: Send messages to the LLM\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools_schema,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "\n",
    "        assistant_message = response.choices[0].message\n",
    "        messages.append(assistant_message)\n",
    "\n",
    "        # Check if the LLM wants to call a tool (ACT)\n",
    "        if assistant_message.tool_calls:\n",
    "            for tool_call in assistant_message.tool_calls:\n",
    "                func_name = tool_call.function.name\n",
    "                func_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"  [ACT] Calling tool: {func_name}({func_args})\")\n",
    "\n",
    "                # Execute the tool\n",
    "                if func_name == \"calculator\":\n",
    "                    result = calculator(func_args[\"expression\"])\n",
    "                else:\n",
    "                    result = f\"Unknown tool: {func_name}\"\n",
    "\n",
    "                print(f\"  [OBSERVE] Tool result: {result}\")\n",
    "\n",
    "                # Feed the result back to the LLM\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": result\n",
    "                })\n",
    "        else:\n",
    "            # No tool call means the agent is done (FINAL ANSWER)\n",
    "            final_answer = assistant_message.content\n",
    "            print(f\"\\n  [FINAL ANSWER] {final_answer}\")\n",
    "            return final_answer\n",
    "\n",
    "    return \"Max iterations reached without a final answer.\"\n",
    "\n",
    "\n",
    "# Test the agent loop\n",
    "run_agent_loop(\"What is 47 * 83 + 129?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complex example requiring multiple tool calls\n",
    "run_agent_loop(\"I bought 15 items at $23.50 each. After a 12% discount, what's my total? Also, what's 20% tip on that total?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** The agent loop is straightforward:\n",
    "1. The LLM receives the conversation so far and decides whether to call a tool or give a final answer.\n",
    "2. If it calls a tool, we execute it and feed the result back.\n",
    "3. We repeat until we get a final answer.\n",
    "\n",
    "Frameworks like LangChain automate this loop so you don't have to write it manually every time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Tools with LangChain\n",
    "\n",
    "The modern way to define tools in LangChain is with the `@tool` decorator. This automatically:\n",
    "- Extracts the tool name from the function name\n",
    "- Extracts the description from the docstring\n",
    "- Infers the input schema from type hints\n",
    "\n",
    "Let's build four useful tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator_tool(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression. Supports basic arithmetic like\n",
    "    addition (+), subtraction (-), multiplication (*), division (/), and\n",
    "    exponentiation (**). Example: '2 + 3 * 4' returns '14'.\"\"\"\n",
    "    try:\n",
    "        allowed_chars = set(\"0123456789+-*/.() \")\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Expression contains invalid characters.\"\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_datetime() -> str:\n",
    "    \"\"\"Get the current date and time. Returns the date in YYYY-MM-DD HH:MM:SS\n",
    "    format along with the day of the week. Useful when the user asks about\n",
    "    today's date or current time.\"\"\"\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d %H:%M:%S (%A)\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def word_counter(text: str) -> str:\n",
    "    \"\"\"Count the number of words, characters, and sentences in the given text.\n",
    "    Returns a summary with word count, character count, and sentence count.\"\"\"\n",
    "    words = len(text.split())\n",
    "    characters = len(text)\n",
    "    sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    return f\"Words: {words}, Characters: {characters}, Sentences: {sentences}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def web_fetcher(url: str) -> str:\n",
    "    \"\"\"Fetch the text content from a given URL. Returns the first 2000\n",
    "    characters of the page text. Useful for retrieving information from\n",
    "    web pages.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()\n",
    "        # Simple text extraction: strip HTML tags\n",
    "        from html.parser import HTMLParser\n",
    "        class TextExtractor(HTMLParser):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.texts = []\n",
    "                self._skip = False\n",
    "            def handle_starttag(self, tag, attrs):\n",
    "                if tag in ('script', 'style'):\n",
    "                    self._skip = True\n",
    "            def handle_endtag(self, tag):\n",
    "                if tag in ('script', 'style'):\n",
    "                    self._skip = False\n",
    "            def handle_data(self, data):\n",
    "                if not self._skip:\n",
    "                    self.texts.append(data.strip())\n",
    "        extractor = TextExtractor()\n",
    "        extractor.feed(response.text)\n",
    "        text = ' '.join(t for t in extractor.texts if t)\n",
    "        return text[:2000]\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching URL: {e}\"\n",
    "\n",
    "\n",
    "# Collect all tools\n",
    "tools = [calculator_tool, get_current_datetime, word_counter, web_fetcher]\n",
    "\n",
    "print(\"Tools created successfully!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the tool schemas that LangChain auto-generated\n",
    "for t in tools:\n",
    "    print(f\"Tool: {t.name}\")\n",
    "    print(f\"  Description: {t.description}\")\n",
    "    print(f\"  Schema: {t.args_schema.model_json_schema()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Agent Types\n",
    "\n",
    "LangChain supports several agent types. The two most important are:\n",
    "\n",
    "### ReAct Agent (`create_react_agent`)\n",
    "- Based on the **ReAct** paper (Yao et al., 2023)\n",
    "- Interleaves **Re**asoning and **Act**ing in a single prompt\n",
    "- The LLM generates explicit `Thought:`, `Action:`, and `Observation:` traces\n",
    "- Works with any LLM (including those without native function calling)\n",
    "- Best for: transparency, debugging, older models\n",
    "\n",
    "### Tool-Calling Agent (`create_tool_calling_agent`)\n",
    "- Uses the LLM's **native function/tool calling** capability (e.g., OpenAI's function calling)\n",
    "- The LLM directly outputs structured tool calls in its response\n",
    "- More efficient since the model was trained for this\n",
    "- Best for: production use with modern LLMs (GPT-4, Claude, Gemini)\n",
    "\n",
    "Let's build both using the same set of tools and compare their behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Tool-Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tool-calling agent using modern LangChain APIs\n",
    "tool_calling_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use your tools to answer questions. Always explain your reasoning.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "tool_calling_agent = create_tool_calling_agent(llm, tools, tool_calling_prompt)\n",
    "tool_calling_executor = AgentExecutor(\n",
    "    agent=tool_calling_agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"Tool-calling agent ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tool-calling agent\n",
    "result = tool_calling_executor.invoke(\n",
    "    {\"input\": \"What's today's date, and how many days are left until New Year's Eve 2025?\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a ReAct agent\n",
    "# The ReAct agent needs a specific prompt format with {tools}, {tool_names}, {input}, and {agent_scratchpad}\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "react_agent = create_react_agent(llm, tools, react_prompt)\n",
    "react_executor = AgentExecutor(\n",
    "    agent=react_agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"ReAct agent ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ReAct agent with the same question\n",
    "result = react_executor.invoke(\n",
    "    {\"input\": \"What's today's date, and how many days are left until New Year's Eve 2025?\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison Notes:**\n",
    "\n",
    "| Feature | Tool-Calling Agent | ReAct Agent |\n",
    "|---|---|---|\n",
    "| Reasoning visibility | Implicit (in model's internal reasoning) | Explicit (`Thought:` traces) |\n",
    "| Efficiency | More efficient (native API feature) | Slightly more tokens (text-based reasoning) |\n",
    "| Model compatibility | Requires models with tool-calling support | Works with any LLM |\n",
    "| Debugging | Harder to debug | Easy to follow reasoning chain |\n",
    "| Prompt format | Flexible | Requires specific `{tools}`, `{tool_names}`, `{agent_scratchpad}` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build a Weather Agent\n",
    "\n",
    "Create a mock weather tool and an agent that uses it. The tool should return hardcoded weather data for demonstration purposes.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a `get_weather` tool using the `@tool` decorator\n",
    "2. Build a tool-calling agent with both the weather tool and the calculator tool\n",
    "3. Ask the agent: \"What's the weather in Tokyo and New York? What's the temperature difference?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: YOUR CODE HERE\n",
    "\n",
    "# Step 1: Create a mock weather tool\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a given city. Returns temperature,\n",
    "    humidity, and conditions.\"\"\"\n",
    "    # TODO: Create a dictionary of mock weather data for a few cities\n",
    "    # Return weather info for the requested city, or \"City not found\" for unknown cities\n",
    "    weather_data = None  # Replace with a dict of city -> weather info\n",
    "    pass\n",
    "\n",
    "\n",
    "# Step 2: Create the agent\n",
    "weather_tools = None  # Replace with list of tools\n",
    "weather_agent = None   # Replace with create_tool_calling_agent(...)\n",
    "weather_executor = None  # Replace with AgentExecutor(...)\n",
    "\n",
    "\n",
    "# Step 3: Test the agent\n",
    "# result = weather_executor.invoke({\"input\": \"What's the weather in Tokyo and New York? What's the temperature difference?\"})\n",
    "# print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: SOLUTION\n",
    "\n",
    "# Step 1: Create a mock weather tool\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a given city. Returns temperature in\n",
    "    Fahrenheit, humidity percentage, and weather conditions.\"\"\"\n",
    "    weather_data = {\n",
    "        \"tokyo\": {\"temp_f\": 72, \"humidity\": 65, \"conditions\": \"Partly Cloudy\"},\n",
    "        \"new york\": {\"temp_f\": 58, \"humidity\": 45, \"conditions\": \"Sunny\"},\n",
    "        \"london\": {\"temp_f\": 52, \"humidity\": 80, \"conditions\": \"Rainy\"},\n",
    "        \"paris\": {\"temp_f\": 55, \"humidity\": 70, \"conditions\": \"Overcast\"},\n",
    "        \"sydney\": {\"temp_f\": 85, \"humidity\": 55, \"conditions\": \"Clear\"},\n",
    "    }\n",
    "    city_lower = city.lower().strip()\n",
    "    if city_lower in weather_data:\n",
    "        w = weather_data[city_lower]\n",
    "        return f\"Weather in {city}: {w['temp_f']}F, Humidity: {w['humidity']}%, Conditions: {w['conditions']}\"\n",
    "    return f\"Weather data not available for '{city}'. Available cities: {', '.join(weather_data.keys())}\"\n",
    "\n",
    "\n",
    "# Step 2: Create the agent\n",
    "weather_tools = [get_weather, calculator_tool]\n",
    "\n",
    "weather_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful weather assistant. Use your tools to look up weather and perform calculations.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "weather_agent = create_tool_calling_agent(llm, weather_tools, weather_prompt)\n",
    "weather_executor = AgentExecutor(\n",
    "    agent=weather_agent,\n",
    "    tools=weather_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Step 3: Test the agent\n",
    "result = weather_executor.invoke(\n",
    "    {\"input\": \"What's the weather in Tokyo and New York? What's the temperature difference?\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ReAct Pattern Deep Dive\n",
    "\n",
    "The ReAct (Reasoning + Acting) pattern is one of the most important concepts in AI agents. Let's examine how it works in detail.\n",
    "\n",
    "### The Thought-Action-Observation Trace\n",
    "\n",
    "In a ReAct agent, the LLM generates a structured trace:\n",
    "\n",
    "```\n",
    "Thought: I need to find out what today's date is first.\n",
    "Action: get_current_datetime\n",
    "Action Input: {}\n",
    "Observation: 2025-01-15 14:30:00 (Wednesday)\n",
    "\n",
    "Thought: Now I know the date. I need to calculate the number of days...\n",
    "Action: calculator_tool\n",
    "Action Input: {\"expression\": \"365 - 15\"}\n",
    "Observation: 350\n",
    "\n",
    "Thought: I now have all the information I need.\n",
    "Final Answer: Today is January 15, 2025. There are 350 days left...\n",
    "```\n",
    "\n",
    "This trace is valuable because:\n",
    "1. You can see **exactly why** the agent made each decision\n",
    "2. You can **debug** issues by examining the reasoning chain\n",
    "3. You can **audit** agent behavior for safety and correctness\n",
    "\n",
    "Let's walk through a multi-step example with verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-step ReAct example with verbose output\n",
    "# The agent needs to: get the date, do a calculation, and count words\n",
    "result = react_executor.invoke(\n",
    "    {\"input\": (\n",
    "        \"Please do the following:\\n\"\n",
    "        \"1. Tell me what today's date is\\n\"\n",
    "        \"2. Calculate 2 to the power of 16\\n\"\n",
    "        \"3. Count the words in this sentence: 'The quick brown fox jumps over the lazy dog'\"\n",
    "    )}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Final Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also access the intermediate steps if you set return_intermediate_steps=True\n",
    "react_executor_with_steps = AgentExecutor(\n",
    "    agent=react_agent,\n",
    "    tools=tools,\n",
    "    verbose=False,  # Disable verbose printing\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "\n",
    "result = react_executor_with_steps.invoke(\n",
    "    {\"input\": \"What is 123 * 456? Then count the words in the result.\"}\n",
    ")\n",
    "\n",
    "print(\"Final Answer:\", result[\"output\"])\n",
    "print(\"\\nIntermediate Steps:\")\n",
    "for i, (action, observation) in enumerate(result[\"intermediate_steps\"]):\n",
    "    print(f\"  Step {i + 1}:\")\n",
    "    print(f\"    Tool: {action.tool}\")\n",
    "    print(f\"    Input: {action.tool_input}\")\n",
    "    print(f\"    Output: {observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Error Handling\n",
    "\n",
    "In production, tools will fail. Networks go down, APIs return errors, inputs are malformed. A robust agent needs to handle these gracefully.\n",
    "\n",
    "### Strategies for Error Handling\n",
    "\n",
    "1. **Tool-level error handling**: Catch exceptions inside the tool and return an error message\n",
    "2. **Agent-level retry**: The `AgentExecutor` can be configured to handle parsing errors\n",
    "3. **Fallback behavior**: Provide alternative actions when a tool fails\n",
    "4. **Graceful degradation**: Return partial results instead of complete failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Tool-level error handling\n",
    "@tool\n",
    "def division_tool(numerator: float, denominator: float) -> str:\n",
    "    \"\"\"Divide the numerator by the denominator. Returns the result of the division.\"\"\"\n",
    "    try:\n",
    "        if denominator == 0:\n",
    "            return \"Error: Division by zero is not allowed. Please provide a non-zero denominator.\"\n",
    "        result = numerator / denominator\n",
    "        return f\"{numerator} / {denominator} = {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error performing division: {str(e)}\"\n",
    "\n",
    "\n",
    "# Example 2: Tool with timeout and retry logic\n",
    "@tool\n",
    "def reliable_web_fetcher(url: str) -> str:\n",
    "    \"\"\"Fetch content from a URL with automatic retry on failure. Tries up\n",
    "    to 3 times before giving up. Returns the first 1500 characters of text.\"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                timeout=5,\n",
    "                headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            # Simple text extraction\n",
    "            from html.parser import HTMLParser\n",
    "            class TextExtractor(HTMLParser):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "                    self.texts = []\n",
    "                    self._skip = False\n",
    "                def handle_starttag(self, tag, attrs):\n",
    "                    if tag in ('script', 'style'):\n",
    "                        self._skip = True\n",
    "                def handle_endtag(self, tag):\n",
    "                    if tag in ('script', 'style'):\n",
    "                        self._skip = False\n",
    "                def handle_data(self, data):\n",
    "                    if not self._skip:\n",
    "                        self.texts.append(data.strip())\n",
    "            extractor = TextExtractor()\n",
    "            extractor.feed(response.text)\n",
    "            text = ' '.join(t for t in extractor.texts if t)\n",
    "            return text[:1500]\n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt < max_retries - 1:\n",
    "                continue\n",
    "            return f\"Error: Request timed out after {max_retries} attempts for URL: {url}\"\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            return f\"Error: HTTP {e.response.status_code} for URL: {url}\"\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                continue\n",
    "            return f\"Error: Failed to fetch {url} after {max_retries} attempts. Last error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"Error-handling tools created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling: the agent should gracefully handle division by zero\n",
    "error_handling_tools = [division_tool, calculator_tool]\n",
    "\n",
    "error_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful math assistant. If a tool returns an error, \"\n",
    "        \"explain the error to the user and suggest alternatives.\"\n",
    "    )),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "error_agent = create_tool_calling_agent(llm, error_handling_tools, error_prompt)\n",
    "error_executor = AgentExecutor(\n",
    "    agent=error_agent,\n",
    "    tools=error_handling_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# This should trigger the division by zero error and the agent should handle it gracefully\n",
    "result = error_executor.invoke(\n",
    "    {\"input\": \"What is 100 divided by 0?\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an invalid URL to see retry + graceful degradation\n",
    "error_tools_2 = [reliable_web_fetcher, word_counter]\n",
    "\n",
    "error_prompt_2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful research assistant. If fetching a URL fails, \"\n",
    "        \"inform the user and suggest alternatives.\"\n",
    "    )),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "error_agent_2 = create_tool_calling_agent(llm, error_tools_2, error_prompt_2)\n",
    "error_executor_2 = AgentExecutor(\n",
    "    agent=error_agent_2,\n",
    "    tools=error_tools_2,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "result = error_executor_2.invoke(\n",
    "    {\"input\": \"Please fetch the content from https://thisurldoesnotexist12345.com and count its words.\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Research Assistant Agent\n",
    "\n",
    "Build a \"research assistant\" agent that can search for information and summarize it.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a `mock_web_search` tool that returns hardcoded search results\n",
    "2. Create a `summarize_text` tool that uses the LLM to summarize text\n",
    "3. Build an agent and test it with: \"Search for information about the transformer architecture and summarize the key points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: YOUR CODE HERE\n",
    "\n",
    "# Step 1: Create a mock web search tool\n",
    "@tool\n",
    "def mock_web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information about a given query. Returns relevant\n",
    "    search results as text.\"\"\"\n",
    "    # TODO: Create a dictionary mapping keywords to mock search results\n",
    "    # Check if any keyword appears in the query and return the corresponding result\n",
    "    search_results = None  # Replace with dict of keyword -> search result text\n",
    "    pass\n",
    "\n",
    "\n",
    "# Step 2: Create a summarization tool\n",
    "@tool\n",
    "def summarize_text(text: str) -> str:\n",
    "    \"\"\"Summarize the given text into 3-5 key bullet points.\"\"\"\n",
    "    # TODO: Use the OpenAI client to summarize the text\n",
    "    pass\n",
    "\n",
    "\n",
    "# Step 3: Build the agent\n",
    "research_tools = None  # Replace with list of tools\n",
    "research_agent = None   # Replace with create_tool_calling_agent(...)\n",
    "research_executor = None  # Replace with AgentExecutor(...)\n",
    "\n",
    "# Step 4: Test the agent\n",
    "# result = research_executor.invoke(\n",
    "#     {\"input\": \"Search for information about the transformer architecture and summarize the key points\"}\n",
    "# )\n",
    "# print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: SOLUTION\n",
    "\n",
    "# Step 1: Create a mock web search tool\n",
    "@tool\n",
    "def mock_web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information about a given query. Returns relevant\n",
    "    search results as text.\"\"\"\n",
    "    search_db = {\n",
    "        \"transformer\": (\n",
    "            \"The Transformer architecture was introduced in the 2017 paper 'Attention Is All You Need' \"\n",
    "            \"by Vaswani et al. Key innovations include: (1) Self-attention mechanism that allows the model \"\n",
    "            \"to weigh the importance of different parts of the input sequence. (2) Multi-head attention \"\n",
    "            \"that allows the model to attend to information from different representation subspaces. \"\n",
    "            \"(3) Positional encoding to inject sequence order information since the model has no recurrence. \"\n",
    "            \"(4) Encoder-decoder architecture where the encoder processes input and the decoder generates output. \"\n",
    "            \"(5) Layer normalization and residual connections for stable training. \"\n",
    "            \"The Transformer has become the foundation for models like BERT, GPT, T5, and virtually all \"\n",
    "            \"modern large language models. It replaced RNNs and LSTMs as the dominant architecture for NLP tasks.\"\n",
    "        ),\n",
    "        \"attention\": (\n",
    "            \"Attention mechanisms allow neural networks to focus on relevant parts of the input when \"\n",
    "            \"producing output. Scaled dot-product attention computes: Attention(Q,K,V) = softmax(QK^T / sqrt(d_k))V. \"\n",
    "            \"Multi-head attention runs multiple attention functions in parallel.\"\n",
    "        ),\n",
    "        \"bert\": (\n",
    "            \"BERT (Bidirectional Encoder Representations from Transformers) was introduced by Google in 2018. \"\n",
    "            \"It uses masked language modeling and next sentence prediction for pre-training.\"\n",
    "        ),\n",
    "    }\n",
    "    query_lower = query.lower()\n",
    "    results = []\n",
    "    for keyword, content in search_db.items():\n",
    "        if keyword in query_lower:\n",
    "            results.append(content)\n",
    "    if results:\n",
    "        return \" \".join(results)\n",
    "    return f\"No results found for query: '{query}'. Try different search terms.\"\n",
    "\n",
    "\n",
    "# Step 2: Create a summarization tool\n",
    "@tool\n",
    "def summarize_text(text: str) -> str:\n",
    "    \"\"\"Summarize the given text into 3-5 concise key bullet points.\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following text into 3-5 concise bullet points. Each bullet should capture a key idea.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Step 3: Build the agent\n",
    "research_tools = [mock_web_search, summarize_text]\n",
    "\n",
    "research_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a research assistant. Use the search tool to find information, \"\n",
    "        \"then use the summarization tool to create concise summaries. \"\n",
    "        \"Always search first, then summarize the results.\"\n",
    "    )),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "research_agent = create_tool_calling_agent(llm, research_tools, research_prompt)\n",
    "research_executor = AgentExecutor(\n",
    "    agent=research_agent,\n",
    "    tools=research_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Step 4: Test the agent\n",
    "result = research_executor.invoke(\n",
    "    {\"input\": \"Search for information about the transformer architecture and summarize the key points\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Old vs. New API Comparison\n",
    "\n",
    "If you look at `langchain_agent.ipynb` in this repository, you'll see the **deprecated** LangChain patterns. Here's a side-by-side comparison showing how to migrate.\n",
    "\n",
    "### Deprecated Pattern (pre-LangChain 1.0)\n",
    "\n",
    "```python\n",
    "# OLD WAY - DEPRECATED\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 1. Create LLM\n",
    "llm = OpenAI(temperature=0)  # Deprecated class\n",
    "\n",
    "# 2. Define tools using Tool.from_function\n",
    "my_tool = Tool.from_function(\n",
    "    func=my_function,\n",
    "    name=\"My Tool\",\n",
    "    description=\"Does something useful\"\n",
    ")\n",
    "\n",
    "# 3. Create an LLMChain (deprecated)\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(\"...\")\n",
    ")\n",
    "\n",
    "# 4. Initialize agent (deprecated)\n",
    "agent = initialize_agent(\n",
    "    tools=[my_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5. Run\n",
    "agent.run(\"Do something\")  # .run() is deprecated\n",
    "```\n",
    "\n",
    "### Modern Pattern (LangChain 0.2+)\n",
    "\n",
    "```python\n",
    "# NEW WAY - RECOMMENDED\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "# 1. Create LLM (use langchain_openai package)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 2. Define tools using @tool decorator\n",
    "@tool\n",
    "def my_tool(input_param: str) -> str:\n",
    "    \"\"\"Does something useful.\"\"\"  # Docstring becomes the description\n",
    "    return do_something(input_param)\n",
    "\n",
    "# 3. No more LLMChain needed! Use prompt | llm or agents\n",
    "\n",
    "# 4. Create agent with explicit prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, [my_tool], prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=[my_tool], verbose=True)\n",
    "\n",
    "# 5. Invoke (not run)\n",
    "result = executor.invoke({\"input\": \"Do something\"})\n",
    "print(result[\"output\"])\n",
    "```\n",
    "\n",
    "### Migration Cheat Sheet\n",
    "\n",
    "| Deprecated | Modern Replacement |\n",
    "|---|---|\n",
    "| `from langchain.llms import OpenAI` | `from langchain_openai import ChatOpenAI` |\n",
    "| `LLMChain(llm=..., prompt=...)` | `prompt \\| llm` (LCEL) or agent |\n",
    "| `Tool.from_function(func=..., name=..., description=...)` | `@tool` decorator |\n",
    "| `initialize_agent(tools, llm)` | `create_tool_calling_agent(llm, tools, prompt)` + `AgentExecutor(...)` |\n",
    "| `agent.run(\"query\")` | `executor.invoke({\"input\": \"query\"})` |\n",
    "| `chain(\"input\")` | `chain.invoke(\"input\")` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live demo: Modern pattern in action\n",
    "# This is the clean, modern way to build an agent\n",
    "\n",
    "@tool\n",
    "def reverse_string(text: str) -> str:\n",
    "    \"\"\"Reverse the given string. Returns the string with characters in reverse order.\"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "@tool\n",
    "def uppercase_string(text: str) -> str:\n",
    "    \"\"\"Convert the given string to uppercase. Returns the string with all characters capitalized.\"\"\"\n",
    "    return text.upper()\n",
    "\n",
    "# Modern pattern: explicit prompt, create_tool_calling_agent, AgentExecutor\n",
    "modern_tools = [reverse_string, uppercase_string]\n",
    "\n",
    "modern_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a string manipulation assistant. Use your tools to transform text as requested.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "modern_agent = create_tool_calling_agent(llm, modern_tools, modern_prompt)\n",
    "modern_executor = AgentExecutor(\n",
    "    agent=modern_agent,\n",
    "    tools=modern_tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = modern_executor.invoke(\n",
    "    {\"input\": \"Reverse the word 'hello' and then convert the result to uppercase\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Error Handling and Retry Logic\n",
    "\n",
    "Build an agent with a tool that randomly fails, and implement proper error handling with retry logic.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create an `unreliable_api` tool that fails ~50% of the time\n",
    "2. Create a `safe_api_call` wrapper tool that adds retry logic (up to 3 attempts)\n",
    "3. Build an agent using both tools and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: YOUR CODE HERE\n",
    "\n",
    "# Step 1: Create an unreliable tool\n",
    "@tool\n",
    "def unreliable_api(query: str) -> str:\n",
    "    \"\"\"Query an unreliable API that sometimes fails. Returns information about\n",
    "    the query topic when successful.\"\"\"\n",
    "    # TODO: Use random.random() to simulate ~50% failure rate\n",
    "    # On failure: raise an exception or return an error message\n",
    "    # On success: return a mock response based on the query\n",
    "    pass\n",
    "\n",
    "\n",
    "# Step 2: Create a reliable wrapper with retry logic\n",
    "@tool\n",
    "def safe_api_call(query: str) -> str:\n",
    "    \"\"\"Safely call the unreliable API with automatic retry logic. Tries up to\n",
    "    3 times before returning an error.\"\"\"\n",
    "    # TODO: Implement retry logic that calls unreliable_api up to 3 times\n",
    "    # Return the result on success, or an error message after all retries fail\n",
    "    pass\n",
    "\n",
    "\n",
    "# Step 3: Build and test the agent\n",
    "retry_tools = None  # Replace with list of tools\n",
    "retry_agent = None   # Replace with agent\n",
    "retry_executor = None  # Replace with AgentExecutor\n",
    "\n",
    "# result = retry_executor.invoke({\"input\": \"Look up information about quantum computing\"})\n",
    "# print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: SOLUTION\n",
    "\n",
    "# Step 1: Create an unreliable tool (simulates ~50% failure rate)\n",
    "call_count = {\"total\": 0, \"failures\": 0}\n",
    "\n",
    "@tool\n",
    "def unreliable_api(query: str) -> str:\n",
    "    \"\"\"Query an unreliable API that sometimes fails. Returns information\n",
    "    about the query topic when successful.\"\"\"\n",
    "    call_count[\"total\"] += 1\n",
    "    # Simulate 50% failure rate\n",
    "    if random.random() < 0.5:\n",
    "        call_count[\"failures\"] += 1\n",
    "        raise ConnectionError(f\"API connection failed (attempt #{call_count['total']})\")\n",
    "\n",
    "    # Mock successful responses\n",
    "    responses = {\n",
    "        \"quantum\": \"Quantum computing uses qubits that can exist in superposition, enabling parallel computation. Key players include IBM, Google, and IonQ.\",\n",
    "        \"ai\": \"Artificial Intelligence has seen rapid advances with transformer models, achieving human-level performance on many benchmarks.\",\n",
    "        \"climate\": \"Global temperatures have risen 1.1C since pre-industrial times. Renewable energy adoption is accelerating worldwide.\",\n",
    "    }\n",
    "    query_lower = query.lower()\n",
    "    for keyword, response in responses.items():\n",
    "        if keyword in query_lower:\n",
    "            return response\n",
    "    return f\"Retrieved general information about: {query}\"\n",
    "\n",
    "\n",
    "# Step 2: Create a reliable wrapper with retry logic\n",
    "@tool\n",
    "def safe_api_call(query: str) -> str:\n",
    "    \"\"\"Safely query information with automatic retry logic. Retries up to 3\n",
    "    times if the underlying API fails. Use this instead of unreliable_api\n",
    "    for reliable results.\"\"\"\n",
    "    max_retries = 3\n",
    "    errors = []\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Directly call the underlying logic (not the tool object)\n",
    "            call_count[\"total\"] += 1\n",
    "            if random.random() < 0.5:\n",
    "                call_count[\"failures\"] += 1\n",
    "                raise ConnectionError(f\"Connection failed on attempt {attempt}\")\n",
    "\n",
    "            # Mock responses\n",
    "            responses = {\n",
    "                \"quantum\": \"Quantum computing uses qubits that can exist in superposition, enabling parallel computation. Key players include IBM, Google, and IonQ.\",\n",
    "                \"ai\": \"Artificial Intelligence has seen rapid advances with transformer models, achieving human-level performance on many benchmarks.\",\n",
    "                \"climate\": \"Global temperatures have risen 1.1C since pre-industrial times. Renewable energy adoption is accelerating worldwide.\",\n",
    "            }\n",
    "            query_lower = query.lower()\n",
    "            for keyword, response in responses.items():\n",
    "                if keyword in query_lower:\n",
    "                    return f\"[Success on attempt {attempt}] {response}\"\n",
    "            return f\"[Success on attempt {attempt}] General information about: {query}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Attempt {attempt}: {str(e)}\")\n",
    "            if attempt < max_retries:\n",
    "                continue\n",
    "\n",
    "    return f\"All {max_retries} attempts failed. Errors: {'; '.join(errors)}\"\n",
    "\n",
    "\n",
    "# Step 3: Build the agent\n",
    "retry_tools = [safe_api_call, calculator_tool]\n",
    "\n",
    "retry_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful research assistant. Use safe_api_call to look up information. \"\n",
    "        \"If a lookup fails after retries, inform the user that the information is temporarily unavailable.\"\n",
    "    )),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "retry_agent = create_tool_calling_agent(llm, retry_tools, retry_prompt)\n",
    "retry_executor = AgentExecutor(\n",
    "    agent=retry_agent,\n",
    "    tools=retry_tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Reset counters and test\n",
    "call_count = {\"total\": 0, \"failures\": 0}\n",
    "\n",
    "result = retry_executor.invoke(\n",
    "    {\"input\": \"Look up information about quantum computing\"}\n",
    ")\n",
    "print(\"\\nFinal Answer:\", result[\"output\"])\n",
    "print(f\"\\nAPI Stats: {call_count['total']} total calls, {call_count['failures']} failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **The Agent Loop**: Agents work in an Observe-Think-Act cycle, repeatedly calling tools until they have enough information to answer.\n",
    "\n",
    "2. **Building Tools**: The `@tool` decorator is the modern way to create tools in LangChain. It auto-generates schemas from type hints and docstrings.\n",
    "\n",
    "3. **Agent Types**:\n",
    "   - **Tool-Calling Agent**: Uses native LLM function calling. Best for production with modern models.\n",
    "   - **ReAct Agent**: Generates explicit Thought/Action/Observation traces. Best for debugging and transparency.\n",
    "\n",
    "4. **Error Handling**: Production agents need robust error handling at the tool level (try/except), retry logic, and graceful degradation.\n",
    "\n",
    "5. **API Migration**: Modern LangChain uses `@tool`, `create_tool_calling_agent`, and `AgentExecutor` instead of the deprecated `Tool.from_function`, `LLMChain`, and `initialize_agent`.\n",
    "\n",
    "### Coming Up Next\n",
    "\n",
    "**Module 13: Multi-Agent Systems** will cover:\n",
    "- Orchestrating multiple specialized agents\n",
    "- Agent-to-agent communication\n",
    "- Supervisor patterns and hierarchical agent architectures\n",
    "- Building on the sequential chain pattern from `ai_trading_agent.ipynb` with modern APIs\n",
    "\n",
    "### References\n",
    "\n",
    "- **Paper**: Yao, S., et al. (2023). [\"ReAct: Synergizing Reasoning and Acting in Language Models\"](https://arxiv.org/abs/2210.03629). ICLR 2023.\n",
    "- **Documentation**: [LangChain Agents](https://python.langchain.com/docs/how_to/#agents) (latest)\n",
    "- **Course**: DeepLearning.AI - [\"Functions, Tools and Agents with LangChain\"](https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/)\n",
    "- **Paper**: Schick, T., et al. (2023). [\"Toolformer: Language Models Can Teach Themselves to Use Tools\"](https://arxiv.org/abs/2302.04761).\n",
    "- **Paper**: Wei, J., et al. (2022). [\"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"](https://arxiv.org/abs/2201.11903). NeurIPS 2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}