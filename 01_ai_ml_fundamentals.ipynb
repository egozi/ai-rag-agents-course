{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: AI & Machine Learning Fundamentals\n",
    "\n",
    "## Overview\n",
    "\n",
    "The field of Artificial Intelligence has evolved dramatically over the past several decades:\n",
    "\n",
    "- **Rule-based systems** (1950s-1980s): Hand-coded if/else logic, expert systems\n",
    "- **Machine Learning** (1990s-2010s): Algorithms that learn patterns from data\n",
    "- **Deep Learning** (2012-present): Neural networks with many layers, fueled by GPUs and big data\n",
    "- **Generative AI** (2020-present): Models that generate text, images, code, and more (GPT, DALL-E, Stable Diffusion)\n",
    "\n",
    "This notebook covers the foundational ML concepts you need before diving into embeddings, RAG, and agents in later modules.\n",
    "\n",
    "### What you'll learn\n",
    "\n",
    "1. The AI landscape and when to use ML vs traditional programming\n",
    "2. Supervised, unsupervised, and reinforcement learning paradigms\n",
    "3. Linear regression from scratch with gradient descent\n",
    "4. How learning rate affects convergence\n",
    "5. Logistic regression for classification\n",
    "6. Evaluation metrics: accuracy, precision, recall, F1, confusion matrix\n",
    "7. How train/test split ratios affect model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    ")\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The AI Landscape\n",
    "\n",
    "### Evolution of AI\n",
    "\n",
    "```\n",
    "1950s          1980s          2000s          2012           2017           2020+\n",
    "  |              |              |              |              |              |\n",
    "  v              v              v              v              v              v\n",
    "\n",
    "Rule-Based --> Expert -----> Classical ----> Deep --------> Transformers -> Generative\n",
    "Systems        Systems        ML             Learning       (Attention)     AI\n",
    "(if/else)      (knowledge     (SVM, Trees,   (CNNs, RNNs)   (BERT, GPT)    (ChatGPT,\n",
    "               bases)         Regression)                                   DALL-E)\n",
    "```\n",
    "\n",
    "### When to use ML vs Traditional Programming\n",
    "\n",
    "| Criteria | Traditional Programming | Machine Learning |\n",
    "|----------|------------------------|------------------|\n",
    "| Rules are clear and finite | Best choice | Overkill |\n",
    "| Pattern is complex/unknown | Very difficult | Best choice |\n",
    "| Data is abundant | Not needed | Required |\n",
    "| Problem changes over time | Must rewrite rules | Model adapts with new data |\n",
    "| Interpretability needed | Easy to explain | Can be a black box |\n",
    "| Example | Tax calculator | Spam detection |\n",
    "\n",
    "**Key insight:** Use ML when you cannot easily write explicit rules, but you have data that contains the patterns you want to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised vs Unsupervised vs Reinforcement Learning\n",
    "\n",
    "### The Three Main Paradigms\n",
    "\n",
    "**Supervised Learning** learns from labeled examples (input-output pairs).\n",
    "- *\"Here are 10,000 emails labeled as spam or not spam. Learn the pattern.\"*\n",
    "- Tasks: Classification (discrete labels), Regression (continuous values)\n",
    "- Examples: Spam detection, house price prediction, image recognition\n",
    "\n",
    "**Unsupervised Learning** finds hidden patterns in unlabeled data.\n",
    "- *\"Here are 10,000 customer profiles. Find natural groupings.\"*\n",
    "- Tasks: Clustering, dimensionality reduction, anomaly detection\n",
    "- Examples: Customer segmentation, topic modeling, data compression\n",
    "\n",
    "**Reinforcement Learning** learns by trial and error with rewards/penalties.\n",
    "- *\"Play this game millions of times. Maximize your score.\"*\n",
    "- Tasks: Sequential decision making, control, game playing\n",
    "- Examples: AlphaGo, robotics, self-driving cars, RLHF for LLMs\n",
    "\n",
    "| Aspect | Supervised | Unsupervised | Reinforcement |\n",
    "|--------|-----------|-------------|---------------|\n",
    "| Data | Labeled | Unlabeled | Reward signal |\n",
    "| Goal | Predict output | Find structure | Maximize reward |\n",
    "| Feedback | Direct (correct answer) | None | Delayed (reward) |\n",
    "| Examples | Regression, Classification | Clustering, PCA | Game AI, Robotics |\n",
    "| Analogy | Studying with answer key | Exploring on your own | Learning by doing |\n",
    "\n",
    "In this module, we focus on **supervised learning** -- the most widely used paradigm and the foundation for understanding modern AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression from Scratch\n",
    "\n",
    "Linear regression models the relationship between input features and a continuous output:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ = input features (e.g., house size, number of bedrooms)\n",
    "- $w$ = weights (learned parameters)\n",
    "- $b$ = bias term\n",
    "- $\\hat{y}$ = predicted output (e.g., house price)\n",
    "\n",
    "We minimize the **Mean Squared Error (MSE)** loss:\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Generate Synthetic Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic housing data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Features: house size (sqft) and number of bedrooms\n",
    "size = np.random.uniform(600, 4000, n_samples)       # square feet\n",
    "bedrooms = np.random.randint(1, 6, n_samples)         # 1-5 bedrooms\n",
    "\n",
    "# True relationship: price = 150*size + 20000*bedrooms + 50000 + noise\n",
    "noise = np.random.normal(0, 30000, n_samples)\n",
    "price = 150 * size + 20000 * bedrooms + 50000 + noise\n",
    "\n",
    "# Combine features into matrix X\n",
    "X = np.column_stack([size, bedrooms])\n",
    "y = price\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"\\nFirst 5 samples:\")\n",
    "print(f\"{'Size (sqft)':<15} {'Bedrooms':<12} {'Price ($)':>12}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(5):\n",
    "    print(f\"{X[i, 0]:<15.0f} {X[i, 1]:<12.0f} {y[i]:>12,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Price vs Size\n",
    "axes[0].scatter(X[:, 0], y, alpha=0.5, c='steelblue', edgecolors='k', linewidth=0.5)\n",
    "axes[0].set_xlabel('House Size (sqft)', fontsize=12)\n",
    "axes[0].set_ylabel('Price ($)', fontsize=12)\n",
    "axes[0].set_title('House Price vs Size', fontsize=14)\n",
    "\n",
    "# Price vs Bedrooms\n",
    "axes[1].scatter(X[:, 1], y, alpha=0.5, c='coral', edgecolors='k', linewidth=0.5)\n",
    "axes[1].set_xlabel('Number of Bedrooms', fontsize=12)\n",
    "axes[1].set_ylabel('Price ($)', fontsize=12)\n",
    "axes[1].set_title('House Price vs Bedrooms', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize the loss function.\n",
    "\n",
    "The update rules are:\n",
    "\n",
    "$$w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "$$b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** -- a hyperparameter that controls the step size.\n",
    "\n",
    "The gradients for MSE loss are:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = -\\frac{2}{n} X^T (y - \\hat{y})$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{2}{n} \\sum (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Gradient Descent for Linear Regression\n",
    "\n",
    "Your task: implement the `gradient_descent` function that trains a linear regression model from scratch.\n",
    "\n",
    "**Hints:**\n",
    "- Forward pass: compute predictions using $\\hat{y} = X \\cdot w + b$\n",
    "- Compute the MSE loss\n",
    "- Compute gradients of the loss with respect to weights and bias\n",
    "- Update weights and bias using the gradients and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, lr=0.01, epochs=1000):\n",
    "    \"\"\"\n",
    "    Implement linear regression using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features) - input features\n",
    "        y: numpy array of shape (n_samples,) - target values\n",
    "        lr: float - learning rate\n",
    "        epochs: int - number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        weights: numpy array of shape (n_features,) - learned weights\n",
    "        bias: float - learned bias\n",
    "        loss_history: list of float - MSE loss at each epoch\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # TODO: Initialize weights to zeros and bias to 0\n",
    "    weights = None\n",
    "    bias = None\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Forward pass - compute predictions\n",
    "        y_pred = None\n",
    "        \n",
    "        # TODO: Compute MSE loss\n",
    "        loss = None\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # TODO: Compute gradients\n",
    "        dw = None  # gradient with respect to weights\n",
    "        db = None  # gradient with respect to bias\n",
    "        \n",
    "        # TODO: Update parameters\n",
    "        weights = None\n",
    "        bias = None\n",
    "    \n",
    "    return weights, bias, loss_history\n",
    "\n",
    "# Test your implementation (will fail until you complete the TODOs)\n",
    "# We normalize features first for stable gradient descent\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# weights, bias, loss_history = gradient_descent(X_scaled, y, lr=0.01, epochs=1000)\n",
    "# print(f\"Learned weights: {weights}\")\n",
    "# print(f\"Learned bias: {bias:.2f}\")\n",
    "# print(f\"Final loss: {loss_history[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, lr=0.01, epochs=1000):\n",
    "    \"\"\"\n",
    "    Implement linear regression using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features) - input features\n",
    "        y: numpy array of shape (n_samples,) - target values\n",
    "        lr: float - learning rate\n",
    "        epochs: int - number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        weights: numpy array of shape (n_features,) - learned weights\n",
    "        bias: float - learned bias\n",
    "        loss_history: list of float - MSE loss at each epoch\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize weights to zeros and bias to 0\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0.0\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass - compute predictions\n",
    "        y_pred = X.dot(weights) + bias\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        error = y - y_pred\n",
    "        loss = np.mean(error ** 2)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Compute gradients\n",
    "        dw = -(2 / n_samples) * X.T.dot(error)\n",
    "        db = -(2 / n_samples) * np.sum(error)\n",
    "        \n",
    "        # Update parameters\n",
    "        weights = weights - lr * dw\n",
    "        bias = bias - lr * db\n",
    "    \n",
    "    return weights, bias, loss_history\n",
    "\n",
    "# Normalize features for stable gradient descent\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train the model\n",
    "weights, bias, loss_history = gradient_descent(X_scaled, y, lr=0.01, epochs=1000)\n",
    "\n",
    "print(f\"Learned weights: {weights}\")\n",
    "print(f\"Learned bias: {bias:,.2f}\")\n",
    "print(f\"Final loss: {loss_history[-1]:,.2f}\")\n",
    "print(f\"Loss reduction: {loss_history[0]:,.2f} -> {loss_history[-1]:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curve over training iterations\n",
    "axes[0].plot(loss_history, color='steelblue', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Curve', fontsize=14)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predictions vs Actual (using size as x-axis for visualization)\n",
    "y_pred = X_scaled.dot(weights) + bias\n",
    "axes[1].scatter(y, y_pred, alpha=0.5, c='steelblue', edgecolors='k', linewidth=0.5)\n",
    "# Perfect prediction line\n",
    "min_val = min(y.min(), y_pred.min())\n",
    "max_val = max(y.max(), y_pred.max())\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual Price ($)', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Price ($)', fontsize=12)\n",
    "axes[1].set_title('Predictions vs Actual Values', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with sklearn's LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sklearn's linear regression for comparison\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X_scaled, y)\n",
    "\n",
    "print(\"Comparison of our implementation vs sklearn:\")\n",
    "print(f\"{'':>25} {'Ours':>15} {'sklearn':>15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Weight (size)':>25} {weights[0]:>15,.2f} {sklearn_model.coef_[0]:>15,.2f}\")\n",
    "print(f\"{'Weight (bedrooms)':>25} {weights[1]:>15,.2f} {sklearn_model.coef_[1]:>15,.2f}\")\n",
    "print(f\"{'Bias':>25} {bias:>15,.2f} {sklearn_model.intercept_:>15,.2f}\")\n",
    "\n",
    "# Compute R-squared for both\n",
    "y_pred_ours = X_scaled.dot(weights) + bias\n",
    "y_pred_sklearn = sklearn_model.predict(X_scaled)\n",
    "\n",
    "ss_res_ours = np.sum((y - y_pred_ours) ** 2)\n",
    "ss_res_sklearn = np.sum((y - y_pred_sklearn) ** 2)\n",
    "ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "r2_ours = 1 - (ss_res_ours / ss_tot)\n",
    "r2_sklearn = 1 - (ss_res_sklearn / ss_tot)\n",
    "\n",
    "print(f\"{'R-squared':>25} {r2_ours:>15.6f} {r2_sklearn:>15.6f}\")\n",
    "print(f\"\\nOur gradient descent achieves nearly identical results to sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Descent Visualization\n",
    "\n",
    "The **learning rate** is one of the most important hyperparameters in ML. Let's see how it affects training:\n",
    "\n",
    "- **Too small**: Very slow convergence, may not reach optimum in time\n",
    "- **Just right**: Smooth convergence to the optimum\n",
    "- **Too large**: Oscillation or divergence -- the loss may explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves for different learning rates\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    _, _, loss_hist = gradient_descent(X_scaled, y, lr=lr, epochs=500)\n",
    "    axes[0].plot(loss_hist, label=f'lr={lr}', color=color, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Effect of Learning Rate on Convergence', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Zoomed in on first 100 epochs\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    _, _, loss_hist = gradient_descent(X_scaled, y, lr=lr, epochs=100)\n",
    "    axes[1].plot(loss_hist, label=f'lr={lr}', color=color, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[1].set_title('First 100 Epochs (Zoomed In)', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final losses\n",
    "print(\"Final MSE loss after 500 epochs:\")\n",
    "for lr in learning_rates:\n",
    "    _, _, loss_hist = gradient_descent(X_scaled, y, lr=lr, epochs=500)\n",
    "    print(f\"  lr={lr:<8} -> loss = {loss_hist[-1]:>15,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent steps on a 2D loss surface (contour plot)\n",
    "# For visualization, we use a simple 1-feature linear regression\n",
    "np.random.seed(42)\n",
    "X_simple = np.random.uniform(0, 10, 50).reshape(-1, 1)\n",
    "y_simple = 3 * X_simple.squeeze() + 7 + np.random.normal(0, 2, 50)\n",
    "\n",
    "# Create a grid of weight and bias values to compute loss surface\n",
    "w_range = np.linspace(-2, 8, 100)\n",
    "b_range = np.linspace(-5, 20, 100)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# Compute loss for each (w, b) pair\n",
    "Loss_surface = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        y_pred = W[i, j] * X_simple.squeeze() + B[i, j]\n",
    "        Loss_surface[i, j] = np.mean((y_simple - y_pred) ** 2)\n",
    "\n",
    "# Run gradient descent and track the path\n",
    "def gradient_descent_1d_track(X, y, lr=0.01, epochs=50):\n",
    "    \"\"\"Track the path of gradient descent for 1D linear regression.\"\"\"\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    path = [(w, b)]\n",
    "    n = len(X)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        y_pred = w * X + b\n",
    "        error = y - y_pred\n",
    "        dw = -(2 / n) * np.sum(error * X)\n",
    "        db = -(2 / n) * np.sum(error)\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "        path.append((w, b))\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Track paths for different learning rates\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (lr, color) in enumerate(zip([0.001, 0.01, 0.05], colors)):\n",
    "    path = gradient_descent_1d_track(X_simple.squeeze(), y_simple, lr=lr, epochs=50)\n",
    "    \n",
    "    # Contour plot\n",
    "    cs = axes[idx].contour(W, B, Loss_surface, levels=30, cmap='viridis', alpha=0.7)\n",
    "    axes[idx].clabel(cs, inline=True, fontsize=7)\n",
    "    \n",
    "    # Gradient descent path\n",
    "    axes[idx].plot(path[:, 0], path[:, 1], 'o-', color=color, markersize=4, linewidth=2,\n",
    "                   label=f'GD path ({len(path)-1} steps)')\n",
    "    axes[idx].plot(path[0, 0], path[0, 1], 'ks', markersize=10, label='Start')\n",
    "    axes[idx].plot(path[-1, 0], path[-1, 1], 'r*', markersize=15, label='End')\n",
    "    \n",
    "    axes[idx].set_xlabel('Weight (w)', fontsize=12)\n",
    "    axes[idx].set_ylabel('Bias (b)', fontsize=12)\n",
    "    axes[idx].set_title(f'Learning Rate = {lr}', fontsize=14)\n",
    "    axes[idx].legend(fontsize=9, loc='upper right')\n",
    "\n",
    "plt.suptitle('Gradient Descent Steps on Loss Surface', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Logistic Regression for Classification\n",
    "\n",
    "While linear regression predicts continuous values, **logistic regression** predicts discrete class labels.\n",
    "\n",
    "### Classification vs Regression\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|--------|-----------|----------------|\n",
    "| Output | Continuous (e.g., price) | Discrete (e.g., spam/not spam) |\n",
    "| Loss function | MSE | Cross-entropy |\n",
    "| Output layer | Linear | Sigmoid/Softmax |\n",
    "| Example | Predict house price | Predict flower species |\n",
    "\n",
    "Logistic regression applies the **sigmoid function** to a linear model:\n",
    "\n",
    "$$P(y=1|X) = \\sigma(X \\cdot w + b) = \\frac{1}{1 + e^{-(X \\cdot w + b)}}$$\n",
    "\n",
    "### The Iris Dataset\n",
    "\n",
    "The Iris dataset is a classic ML benchmark with 150 samples of 3 flower species, each described by 4 features (sepal length/width, petal length/width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(f\"Dataset shape: {X_iris.shape}\")\n",
    "print(f\"Number of classes: {len(iris.target_names)}\")\n",
    "print(f\"Class names: {iris.target_names}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"\\nSamples per class:\")\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    print(f\"  {name}: {np.sum(y_iris == i)}\")\n",
    "\n",
    "# Visualize the data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors_iris = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "for i, (name, color) in enumerate(zip(iris.target_names, colors_iris)):\n",
    "    mask = y_iris == i\n",
    "    axes[0].scatter(X_iris[mask, 0], X_iris[mask, 1], label=name, c=color, alpha=0.7, edgecolors='k', linewidth=0.5)\n",
    "    axes[1].scatter(X_iris[mask, 2], X_iris[mask, 3], label=name, c=color, alpha=0.7, edgecolors='k', linewidth=0.5)\n",
    "\n",
    "axes[0].set_xlabel(iris.feature_names[0], fontsize=12)\n",
    "axes[0].set_ylabel(iris.feature_names[1], fontsize=12)\n",
    "axes[0].set_title('Sepal Features', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "axes[1].set_xlabel(iris.feature_names[2], fontsize=12)\n",
    "axes[1].set_ylabel(iris.feature_names[3], fontsize=12)\n",
    "axes[1].set_title('Petal Features', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Logistic Regression Classifier for Iris\n",
    "\n",
    "Your task: split the data, train a logistic regression model, and evaluate it.\n",
    "\n",
    "**Steps:**\n",
    "1. Split data into 80% train / 20% test using `train_test_split`\n",
    "2. Scale the features using `StandardScaler`\n",
    "3. Train a `LogisticRegression` model\n",
    "4. Make predictions on the test set\n",
    "5. Print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Build a logistic regression classifier\n",
    "\n",
    "# TODO: Split data into train/test (80/20), use random_state=42\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "# TODO: Scale features using StandardScaler (fit on train, transform both)\n",
    "scaler_iris = None\n",
    "X_train_scaled = None\n",
    "X_test_scaled = None\n",
    "\n",
    "# TODO: Create and train a LogisticRegression model (use max_iter=200)\n",
    "log_reg = None\n",
    "\n",
    "# TODO: Make predictions on the test set\n",
    "y_pred = None\n",
    "\n",
    "# TODO: Calculate and print accuracy\n",
    "accuracy = None\n",
    "# print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Build a logistic regression classifier\n",
    "\n",
    "# Split data into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale features\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_scaled = scaler_iris.fit_transform(X_train)\n",
    "X_test_scaled = scaler_iris.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg = LogisticRegression(max_iter=200, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Correctly classified: {np.sum(y_pred == y_test)} / {len(y_test)}\")\n",
    "\n",
    "# Show individual predictions\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(f\"{'Actual':<15} {'Predicted':<15} {'Correct':>8}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(min(10, len(y_test))):\n",
    "    actual = iris.target_names[y_test[i]]\n",
    "    predicted = iris.target_names[y_pred[i]]\n",
    "    correct = \"Yes\" if y_test[i] == y_pred[i] else \"No\"\n",
    "    print(f\"{actual:<15} {predicted:<15} {correct:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "\n",
    "Accuracy alone can be misleading, especially with imbalanced datasets. Here are the key metrics:\n",
    "\n",
    "### Metric Definitions\n",
    "\n",
    "- **Accuracy** = (correct predictions) / (total predictions) -- Overall correctness\n",
    "- **Precision** = TP / (TP + FP) -- \"Of all positive predictions, how many were actually positive?\"\n",
    "- **Recall** = TP / (TP + FN) -- \"Of all actual positives, how many did we find?\"\n",
    "- **F1 Score** = 2 * (Precision * Recall) / (Precision + Recall) -- Harmonic mean of precision and recall\n",
    "\n",
    "### When to Use Which Metric\n",
    "\n",
    "| Scenario | Priority Metric | Why |\n",
    "|----------|----------------|-----|\n",
    "| Spam detection | Precision | Don't want to lose important emails |\n",
    "| Cancer screening | Recall | Don't want to miss any cases |\n",
    "| Balanced classes | Accuracy or F1 | All metrics are informative |\n",
    "| Imbalanced classes | F1 or AUC | Accuracy can be misleading |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all evaluation metrics for our Iris classifier\n",
    "print(\"=\" * 50)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Individual metrics\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred, average='weighted'):.4f} (weighted)\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred, average='weighted'):.4f} (weighted)\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_pred, average='weighted'):.4f} (weighted)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp1.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14)\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=iris.target_names)\n",
    "disp2.plot(ax=axes[1], cmap='Blues', values_format='.2f')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"How to read the confusion matrix:\")\n",
    "print(\"  - Rows = actual class, Columns = predicted class\")\n",
    "print(\"  - Diagonal = correct predictions\")\n",
    "print(\"  - Off-diagonal = misclassifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Model Performance Across Different Train/Test Splits\n",
    "\n",
    "The train/test split ratio affects both model performance and our ability to evaluate it:\n",
    "- **More training data** = better model learning\n",
    "- **More test data** = more reliable evaluation\n",
    "\n",
    "Your task: train and evaluate a logistic regression model with different split ratios and compare the results.\n",
    "\n",
    "**Steps:**\n",
    "1. Loop over split ratios: 60/40, 70/30, 80/20, 90/10\n",
    "2. For each ratio, split, scale, train, predict, and compute accuracy, precision, recall, F1\n",
    "3. Store the results and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Compare performance across different train/test splits\n",
    "\n",
    "split_ratios = [0.4, 0.3, 0.2, 0.1]  # test set proportions\n",
    "results = {\"test_size\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "for test_size in split_ratios:\n",
    "    # TODO: Split the data using train_test_split with random_state=42\n",
    "    X_tr, X_te, y_tr, y_te = None, None, None, None\n",
    "    \n",
    "    # TODO: Scale features (fit on train, transform both)\n",
    "    scaler_ex3 = None\n",
    "    X_tr_scaled = None\n",
    "    X_te_scaled = None\n",
    "    \n",
    "    # TODO: Train LogisticRegression (max_iter=200, random_state=42)\n",
    "    model_ex3 = None\n",
    "    \n",
    "    # TODO: Make predictions\n",
    "    y_pred_ex3 = None\n",
    "    \n",
    "    # TODO: Compute metrics and append to results dict\n",
    "    # results[\"test_size\"].append(test_size)\n",
    "    # results[\"accuracy\"].append(...)\n",
    "    # results[\"precision\"].append(...)\n",
    "    # results[\"recall\"].append(...)\n",
    "    # results[\"f1\"].append(...)\n",
    "    pass\n",
    "\n",
    "# TODO: Print results table\n",
    "# TODO: Plot metrics vs split ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare performance across different train/test splits\n",
    "\n",
    "split_ratios = [0.4, 0.3, 0.2, 0.1]  # test set proportions\n",
    "results = {\"test_size\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "for test_size in split_ratios:\n",
    "    # Split the data\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_iris, y_iris, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_ex3 = StandardScaler()\n",
    "    X_tr_scaled = scaler_ex3.fit_transform(X_tr)\n",
    "    X_te_scaled = scaler_ex3.transform(X_te)\n",
    "    \n",
    "    # Train model\n",
    "    model_ex3 = LogisticRegression(max_iter=200, random_state=42)\n",
    "    model_ex3.fit(X_tr_scaled, y_tr)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_ex3 = model_ex3.predict(X_te_scaled)\n",
    "    \n",
    "    # Compute metrics\n",
    "    results[\"test_size\"].append(test_size)\n",
    "    results[\"accuracy\"].append(accuracy_score(y_te, y_pred_ex3))\n",
    "    results[\"precision\"].append(precision_score(y_te, y_pred_ex3, average='weighted'))\n",
    "    results[\"recall\"].append(recall_score(y_te, y_pred_ex3, average='weighted'))\n",
    "    results[\"f1\"].append(f1_score(y_te, y_pred_ex3, average='weighted'))\n",
    "\n",
    "# Print results table\n",
    "train_pcts = [f\"{int((1 - ts) * 100)}/{int(ts * 100)}\" for ts in split_ratios]\n",
    "print(f\"{'Split (Train/Test)':<20} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Test Samples':>13}\")\n",
    "print(\"-\" * 75)\n",
    "for i, ts in enumerate(split_ratios):\n",
    "    n_test = int(len(y_iris) * ts)\n",
    "    print(f\"{train_pcts[i]:<20} {results['accuracy'][i]:>10.4f} {results['precision'][i]:>10.4f} \"\n",
    "          f\"{results['recall'][i]:>10.4f} {results['f1'][i]:>10.4f} {n_test:>13}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_labels = [f\"{int((1-ts)*100)}/{int(ts*100)}\" for ts in split_ratios]\n",
    "x_pos = np.arange(len(split_ratios))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax.bar(x_pos - 1.5*width, results['accuracy'], width, label='Accuracy', color='#3498db', alpha=0.85)\n",
    "bars2 = ax.bar(x_pos - 0.5*width, results['precision'], width, label='Precision', color='#2ecc71', alpha=0.85)\n",
    "bars3 = ax.bar(x_pos + 0.5*width, results['recall'], width, label='Recall', color='#e74c3c', alpha=0.85)\n",
    "bars4 = ax.bar(x_pos + 1.5*width, results['f1'], width, label='F1 Score', color='#f39c12', alpha=0.85)\n",
    "\n",
    "ax.set_xlabel('Train/Test Split Ratio', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Across Different Train/Test Splits', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0.85, 1.02)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  - More training data generally improves model performance\")\n",
    "print(\"  - Very small test sets (e.g., 10%) give less reliable estimates\")\n",
    "print(\"  - The 80/20 split is a commonly used default and a good balance\")\n",
    "print(\"  - For small datasets, consider cross-validation instead of a single split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Key Takeaways\n",
    "\n",
    "### What we covered\n",
    "\n",
    "1. **AI Landscape**: The evolution from rule-based systems to generative AI, and when ML is the right approach.\n",
    "\n",
    "2. **Learning Paradigms**: Supervised learning (labeled data, prediction), unsupervised learning (unlabeled data, structure discovery), and reinforcement learning (reward-driven).\n",
    "\n",
    "3. **Linear Regression**: Predicting continuous values by minimizing MSE with gradient descent. We implemented it from scratch and verified against sklearn.\n",
    "\n",
    "4. **Gradient Descent**: The core optimization algorithm in ML. Learning rate is critical -- too small means slow convergence, too large means divergence.\n",
    "\n",
    "5. **Logistic Regression**: Extending linear models to classification using the sigmoid function.\n",
    "\n",
    "6. **Evaluation Metrics**: Accuracy is not enough. Precision, recall, and F1 give a more complete picture, especially for imbalanced datasets.\n",
    "\n",
    "7. **Train/Test Split**: The ratio matters. The 80/20 split is a sensible default, but cross-validation is preferred for small datasets.\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "These fundamentals form the foundation for understanding:\n",
    "- **Embeddings** (Module 2): How models represent text and images as vectors\n",
    "- **Transformers** (Module 3): The architecture behind GPT, BERT, and modern LLMs\n",
    "- **RAG** (Module 4): Combining retrieval with generation for grounded AI responses\n",
    "- **Agents** (Module 5): Autonomous AI systems that use tools and make decisions\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- **Book**: Aurelien Geron, *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*, 3rd Edition (Chapters 1-4)\n",
    "- **Course**: Andrew Ng, *Machine Learning Specialization* (Coursera, Course 1: Supervised Machine Learning)\n",
    "- **Video**: 3Blue1Brown, *\"But what is a neural network?\"* (YouTube) -- excellent visual intuition for neural networks\n",
    "- **Documentation**: [scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html) -- comprehensive reference for all algorithms used in this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
