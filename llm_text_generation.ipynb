{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# LLM Text Generation: Understanding the Process\n",
    "\n",
    "This notebook demonstrates the step-by-step process of how Large Language Models (LLMs) generate text. We'll use GPT-2, a small transformer model, to understand:\n",
    "\n",
    "1. How models tokenize input text\n",
    "2. What the raw model output looks like\n",
    "3. How to extract and select the next token\n",
    "4. How to generate text token by token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load GPT-2 Model and Tokenizer\n",
    "\n",
    "We'll use the smallest GPT-2 model (124M parameters) from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/envs/llm/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: gpt2\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # This is the smallest GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 1: Tokenization\n",
    "\n",
    "Let's start with a story prompt and see how it gets tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Once upon a time in a distant'\n",
      "\n",
      "Token IDs: [7454, 2402, 257, 640, 287, 257, 12899]\n",
      "Number of tokens: 7\n",
      "\n",
      "Token breakdown:\n",
      "  Token 0: ID= 7454 -> 'Once'\n",
      "  Token 1: ID= 2402 -> ' upon'\n",
      "  Token 2: ID=  257 -> ' a'\n",
      "  Token 3: ID=  640 -> ' time'\n",
      "  Token 4: ID=  287 -> ' in'\n",
      "  Token 5: ID=  257 -> ' a'\n",
      "  Token 6: ID=12899 -> ' distant'\n"
     ]
    }
   ],
   "source": [
    "# Our starting prompt\n",
    "prompt = \"Once upon a time in a distant\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nToken IDs: {input_ids[0].tolist()}\")\n",
    "print(f\"Number of tokens: {len(input_ids[0])}\")\n",
    "\n",
    "# Show each token\n",
    "print(\"\\nToken breakdown:\")\n",
    "for i, token_id in enumerate(input_ids[0]):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"  Token {i}: ID={token_id:5d} -> '{token_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 2: Get Raw Model Output\n",
    "\n",
    "Now let's pass the tokens through the model and examine the raw output.\n",
    "\n",
    "**Key Concept:** The model outputs a tensor of shape `[batch_size, sequence_length, vocab_size]` containing logits (unnormalized scores) for each possible next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 7, 50257])\n",
      "  - Batch size: 1\n",
      "  - Sequence length: 7\n",
      "  - Vocabulary size: 50257\n",
      "\n",
      "Next token logits shape: torch.Size([50257])\n",
      "\n",
      "First 10 raw logit values: [-100.52684020996094, -98.48580169677734, -105.69715118408203, -103.9027328491211, -103.19031524658203, -103.01127624511719, -98.2580337524414, -100.1230697631836, -98.6717758178711, -101.82699584960938]\n"
     ]
    }
   ],
   "source": [
    "# Get model output\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    \n",
    "# The logits are the raw scores for each token in the vocabulary\n",
    "logits = outputs.logits\n",
    "\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"  - Batch size: {logits.shape[0]}\")\n",
    "print(f\"  - Sequence length: {logits.shape[1]}\")\n",
    "print(f\"  - Vocabulary size: {logits.shape[2]}\")\n",
    "\n",
    "# We only care about the logits for the last token (predicting the next word)\n",
    "next_token_logits = logits[0, -1, :]\n",
    "print(f\"\\nNext token logits shape: {next_token_logits.shape}\")\n",
    "print(f\"\\nFirst 10 raw logit values: {next_token_logits[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 3: Understanding Logits\n",
    "\n",
    "Logits are unnormalized scores. Let's examine them more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits statistics:\n",
      "  Min logit: -120.79\n",
      "  Max logit: -88.59\n",
      "  Mean logit: -104.88\n",
      "  Std logit: 4.16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logits statistics:\")\n",
    "print(f\"  Min logit: {next_token_logits.min().item():.2f}\")\n",
    "print(f\"  Max logit: {next_token_logits.max().item():.2f}\")\n",
    "print(f\"  Mean logit: {next_token_logits.mean().item():.2f}\")\n",
    "print(f\"  Std logit: {next_token_logits.std().item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 4: Convert Logits to Probabilities\n",
    "\n",
    "To interpret these scores as probabilities, we apply the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities shape: torch.Size([50257])\n",
      "Sum of probabilities: 1.000010\n",
      "\n",
      "First 10 probability values: [1.0761151543192682e-06, 8.284580871986691e-06, 6.115349115987101e-09, 3.6789781887591744e-08, 7.50112221226118e-08, 8.971847620387052e-08, 1.0403725354990456e-05, 1.6114396430566558e-06, 6.878647127450677e-06, 2.9322995942493435e-07]\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "print(f\"Sum of probabilities: {probabilities.sum().item():.6f}\")\n",
    "print(f\"\\nFirst 10 probability values: {probabilities[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 5: Extract the Top Tokens\n",
    "\n",
    "Let's see which tokens have the highest probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most likely next tokens:\n",
      "\n",
      "  1. ' future' (ID: 2003) - Probability: 0.1639 (16.39%)\n",
      "  2. ' land' (ID: 1956) - Probability: 0.1185 (11.85%)\n",
      "  3. ' galaxy' (ID: 16161) - Probability: 0.1085 (10.85%)\n",
      "  4. ' past' (ID: 1613) - Probability: 0.0494 (4.94%)\n",
      "  5. ' place' (ID: 1295) - Probability: 0.0396 (3.96%)\n",
      "  6. ' world' (ID: 995) - Probability: 0.0385 (3.85%)\n",
      "  7. ' time' (ID: 640) - Probability: 0.0342 (3.42%)\n",
      "  8. ' corner' (ID: 5228) - Probability: 0.0200 (2.00%)\n",
      "  9. ' space' (ID: 2272) - Probability: 0.0192 (1.92%)\n",
      "  10. ',' (ID: 11) - Probability: 0.0180 (1.80%)\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 tokens\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "print(f\"Top {top_k} most likely next tokens:\\n\")\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    token_text = tokenizer.decode([idx])\n",
    "    print(f\"  {i+1}. '{token_text}' (ID: {idx.item()}) - Probability: {prob.item():.4f} ({prob.item()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Exercise 1: Extract the Best Token\n",
    "\n",
    "**Your Task:** Write code to extract the single most likely next token from the model output.\n",
    "\n",
    "Hints:\n",
    "- You can use `torch.argmax()` to find the index of the maximum value\n",
    "- Use `tokenizer.decode()` to convert the token ID back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to extract the best (most likely) next token\n",
    "# Your code here:\n",
    "\n",
    "best_token_id = None  # Replace with your code\n",
    "best_token_text = None  # Replace with your code\n",
    "\n",
    "print(f\"Best next token: '{best_token_text}' (ID: {best_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Solution for Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best next token: ' future' (ID: 2003)\n",
      "Probability: 0.1639\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "best_token_id = torch.argmax(probabilities).item()\n",
    "best_token_text = tokenizer.decode([best_token_id])\n",
    "\n",
    "print(f\"Best next token: '{best_token_text}' (ID: {best_token_id})\")\n",
    "print(f\"Probability: {probabilities[best_token_id].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 6: Greedy Decoding - Generate Multiple Tokens\n",
    "\n",
    "Now let's generate a full sentence by repeatedly selecting the best token (greedy decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: 'Once upon a time in a distant'\n",
      "Generating 15 tokens...\n",
      "\n",
      "Step 1: ' future' (prob: 0.1639)\n",
      "Step 2: ',' (prob: 0.6390)\n",
      "Step 3: ' the' (prob: 0.1520)\n",
      "Step 4: ' world' (prob: 0.0421)\n",
      "Step 5: ' was' (prob: 0.1780)\n",
      "Step 6: ' a' (prob: 0.0659)\n",
      "Step 7: ' place' (prob: 0.0594)\n",
      "Step 8: ' of' (prob: 0.5240)\n",
      "Step 9: ' great' (prob: 0.0372)\n",
      "Step 10: ' beauty' (prob: 0.0609)\n",
      "Step 11: ' and' (prob: 0.4223)\n",
      "Step 12: ' great' (prob: 0.0739)\n",
      "Step 13: ' danger' (prob: 0.1127)\n",
      "Step 14: '.' (prob: 0.5765)\n",
      "Step 15: ' The' (prob: 0.1227)\n",
      "\n",
      "============================================================\n",
      "Final generated text:\n",
      "Once upon a time in a distant future, the world was a place of great beauty and great danger. The\n"
     ]
    }
   ],
   "source": [
    "def generate_text_greedy(prompt, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate text using greedy decoding (always select the most likely token)\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print(f\"Starting prompt: '{prompt}'\")\n",
    "    print(f\"Generating {max_length} tokens...\\n\")\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # Select the most likely token\n",
    "        next_token_id = torch.argmax(probabilities).item()\n",
    "        next_token_text = tokenizer.decode([next_token_id])\n",
    "        \n",
    "        # Print step-by-step generation\n",
    "        print(f\"Step {step+1}: '{next_token_text}' (prob: {probabilities[next_token_id].item():.4f})\")\n",
    "        \n",
    "        generated_tokens.append(next_token_id)\n",
    "        \n",
    "        # Add the generated token to input for next iteration\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]])], dim=-1)\n",
    "    \n",
    "    # Decode the full generated sequence\n",
    "    full_text = tokenizer.decode(input_ids[0])\n",
    "    return full_text\n",
    "\n",
    "# Generate text\n",
    "result = generate_text_greedy(\"Once upon a time in a distant\", max_length=15)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final generated text:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Step 7: Different Sampling Strategies\n",
    "\n",
    "Greedy decoding always picks the most likely token, which can lead to repetitive text. Let's explore other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 7.1 Temperature Sampling\n",
    "\n",
    "Temperature controls the randomness:\n",
    "- Low temperature (< 1.0): More confident, deterministic\n",
    "- High temperature (> 1.0): More random, creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of temperature on token probabilities:\n",
      "\n",
      "Temperature = 0.5\n",
      "  ' future': 0.4339\n",
      "  ' land': 0.2270\n",
      "  ' galaxy': 0.1901\n",
      "  ' past': 0.0394\n",
      "  ' place': 0.0254\n",
      "\n",
      "Temperature = 1.0\n",
      "  ' future': 0.1639\n",
      "  ' land': 0.1185\n",
      "  ' galaxy': 0.1085\n",
      "  ' past': 0.0494\n",
      "  ' place': 0.0396\n",
      "\n",
      "Temperature = 2.0\n",
      "  ' future': 0.0105\n",
      "  ' land': 0.0089\n",
      "  ' galaxy': 0.0085\n",
      "  ' past': 0.0057\n",
      "  ' place': 0.0051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def apply_temperature(logits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Apply temperature scaling to logits\n",
    "    \"\"\"\n",
    "    return logits / temperature\n",
    "\n",
    "# Compare different temperatures\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "\n",
    "print(\"Effect of temperature on token probabilities:\\n\")\n",
    "for temp in temperatures:\n",
    "    scaled_logits = apply_temperature(next_token_logits, temp)\n",
    "    probs = torch.softmax(scaled_logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    print(f\"Temperature = {temp}\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"  '{token}': {prob.item():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement Temperature Sampling\n",
    "\n",
    "**Your Task:** Modify the generation function to use temperature sampling instead of greedy decoding.\n",
    "\n",
    "Steps:\n",
    "1. Apply temperature scaling to the logits\n",
    "2. Convert to probabilities with softmax\n",
    "3. Sample from the probability distribution using `torch.multinomial()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_temperature(prompt, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using temperature sampling\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # TODO: Apply temperature and sample\n",
    "        # Your code here:\n",
    "        \n",
    "        # 1. Apply temperature\n",
    "        scaled_logits = None  # Replace with your code\n",
    "        \n",
    "        # 2. Get probabilities\n",
    "        probabilities = None  # Replace with your code\n",
    "        \n",
    "        # 3. Sample from distribution\n",
    "        next_token_id = None  # Replace with your code (use torch.multinomial)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Test with different temperatures\n",
    "print(\"Greedy (temp=0.1):\")\n",
    "print(generate_text_with_temperature(\"Once upon a time\", max_length=15, temperature=0.1))\n",
    "print(\"\\nBalanced (temp=1.0):\")\n",
    "print(generate_text_with_temperature(\"Once upon a time\", max_length=15, temperature=1.0))\n",
    "print(\"\\nCreative (temp=1.5):\")\n",
    "print(generate_text_with_temperature(\"Once upon a time\", max_length=15, temperature=1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Solution for Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy (temp=0.1):\n",
      "Once upon a time, the world was a place of great beauty, and the world was a\n",
      "\n",
      "Balanced (temp=1.0):\n",
      "Once upon a time, the best rewards you can get for being Candywinners are gained for\n",
      "\n",
      "Creative (temp=1.5):\n",
      "Once upon a time desired Freeman, explained damn Henri.\"...\"Claude was getting comple School built\n"
     ]
    }
   ],
   "source": [
    "def generate_text_with_temperature(prompt, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using temperature sampling - SOLUTION\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = torch.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # Sample from distribution\n",
    "        next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Test with different temperatures\n",
    "print(\"Greedy (temp=0.1):\")\n",
    "print(generate_text_with_temperature(\"Once upon a time\", max_length=15, temperature=0.1))\n",
    "print(\"\\nBalanced (temp=1.0):\")\n",
    "print(generate_text_with_temperature(\"Once upon a time\", max_length=15, temperature=1.0))\n",
    "print(\"\\nCreative (temp=1.5):\")\n",
    "print(generate_text_with_temperature(\"Once upon a time\", max_length=15, temperature=1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Step 8: Top-k Sampling\n",
    "\n",
    "Top-k sampling only considers the k most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k sampling (k=10):\n",
      "Once upon a time, there were many great men, such as Alexander the Great and Alexander the Great, who were the\n",
      "\n",
      "Top-k sampling (k=50):\n",
      "Once upon a time, all of the children of the world were born, and the man who had the most children was\n"
     ]
    }
   ],
   "source": [
    "def generate_text_top_k(prompt, max_length=20, temperature=1.0, top_k=50):\n",
    "    \"\"\"\n",
    "    Generate text using top-k sampling\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Get top-k logits\n",
    "        top_k_logits, top_k_indices = torch.topk(scaled_logits, top_k)\n",
    "        \n",
    "        # Get probabilities only for top-k\n",
    "        top_k_probs = torch.softmax(top_k_logits, dim=-1)\n",
    "        \n",
    "        # Sample from top-k\n",
    "        sampled_index = torch.multinomial(top_k_probs, num_samples=1)\n",
    "        next_token_id = top_k_indices[sampled_index]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "print(\"Top-k sampling (k=10):\")\n",
    "print(generate_text_top_k(\"Once upon a time\", max_length=20, temperature=0.8, top_k=10))\n",
    "print(\"\\nTop-k sampling (k=50):\")\n",
    "print(generate_text_top_k(\"Once upon a time\", max_length=20, temperature=0.8, top_k=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Tokenization**: How text is converted to token IDs\n",
    "2. **Model Output**: The model produces logits (unnormalized scores) for every token in the vocabulary\n",
    "3. **Probabilities**: Logits are converted to probabilities using softmax\n",
    "4. **Token Selection**: Different strategies for selecting the next token:\n",
    "   - **Greedy**: Always pick the most likely token\n",
    "   - **Temperature Sampling**: Control randomness with temperature\n",
    "   - **Top-k Sampling**: Sample from the k most likely tokens\n",
    "\n",
    "### Key Takeaways:\n",
    "- LLMs generate text one token at a time\n",
    "- The model outputs a probability distribution over all possible next tokens\n",
    "- Different sampling strategies trade off between quality and diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Compare All Methods\n",
    "\n",
    "Generate the same story beginning with all three methods and compare the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Greedy Decoding:\n",
      "============================================================\n",
      "Starting prompt: 'In a world where dragons still existed,'\n",
      "Generating 25 tokens...\n",
      "\n",
      "Step 1: ' the' (prob: 0.1028)\n",
      "Step 2: ' dragon' (prob: 0.0205)\n",
      "Step 3: ' was' (prob: 0.0638)\n",
      "Step 4: ' a' (prob: 0.1217)\n",
      "Step 5: ' symbol' (prob: 0.0363)\n",
      "Step 6: ' of' (prob: 0.8414)\n",
      "Step 7: ' power' (prob: 0.1195)\n",
      "Step 8: ' and' (prob: 0.3137)\n",
      "Step 9: ' power' (prob: 0.0732)\n",
      "Step 10: 'lessness' (prob: 0.4622)\n",
      "Step 11: '.' (prob: 0.6250)\n",
      "Step 12: '\n",
      "' (prob: 0.1486)\n",
      "Step 13: '\n",
      "' (prob: 0.9621)\n",
      "Step 14: 'The' (prob: 0.0956)\n",
      "Step 15: ' dragon' (prob: 0.1934)\n",
      "Step 16: ' was' (prob: 0.2617)\n",
      "Step 17: ' a' (prob: 0.1859)\n",
      "Step 18: ' symbol' (prob: 0.2447)\n",
      "Step 19: ' of' (prob: 0.9380)\n",
      "Step 20: ' power' (prob: 0.1057)\n",
      "Step 21: ' and' (prob: 0.4528)\n",
      "Step 22: ' power' (prob: 0.5776)\n",
      "Step 23: 'lessness' (prob: 0.9973)\n",
      "Step 24: '.' (prob: 0.7717)\n",
      "Step 25: '\n",
      "' (prob: 0.5058)\n",
      "In a world where dragons still existed, the dragon was a symbol of power and powerlessness.\n",
      "\n",
      "The dragon was a symbol of power and powerlessness.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Temperature Sampling (temp=0.8):\n",
      "============================================================\n",
      "In a world where dragons still existed, I'm not sure I can recall so many of the iconic creatures. These are too many to list, but a few of\n",
      "\n",
      "============================================================\n",
      "Top-k Sampling (k=50, temp=0.8):\n",
      "============================================================\n",
      "In a world where dragons still existed, I figured it would be an honor to talk to you about the Dragon Slayer. What are your thoughts of the recent history of\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In a world where dragons still existed,\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Greedy Decoding:\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_text_greedy(prompt, max_length=25))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Temperature Sampling (temp=0.8):\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_text_with_temperature(prompt, max_length=25, temperature=0.8))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Top-k Sampling (k=50, temp=0.8):\")\n",
    "print(\"=\" * 60)\n",
    "print(generate_text_top_k(prompt, max_length=25, temperature=0.8, top_k=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea49d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Follow-Up Exercises: Beginner Level\n",
    "\n",
    "Now that you understand the basics of text generation, try these exercises to deepen your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Exercise 3: Top-p (Nucleus) Sampling\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Objective:** Implement top-p (nucleus) sampling, a more dynamic alternative to top-k sampling.\n",
    "\n",
    "### What is Top-p Sampling?\n",
    "\n",
    "Instead of selecting a fixed number of tokens (like top-k), top-p sampling selects the smallest set of tokens whose cumulative probability exceeds a threshold `p`.\n",
    "\n",
    "**Example:** If p=0.9, select the minimum number of tokens that together have 90% probability.\n",
    "\n",
    "### Why is this useful?\n",
    "- **Dynamic selection**: Sometimes the model is very confident (few tokens needed), sometimes uncertain (many tokens needed)\n",
    "- **Better quality**: Avoids sampling very unlikely tokens while maintaining diversity\n",
    "- **Widely used**: This is the default in many modern LLM APIs (GPT-3, GPT-4, etc.)\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "Complete the function below to implement top-p sampling.\n",
    "\n",
    "**Steps:**\n",
    "1. Apply temperature scaling to logits\n",
    "2. Convert to probabilities using softmax\n",
    "3. Sort probabilities in descending order\n",
    "4. Calculate cumulative sum of sorted probabilities\n",
    "5. Find where cumulative sum exceeds p\n",
    "6. Keep only those tokens and sample from them\n",
    "\n",
    "**Hints:**\n",
    "- Use `torch.sort()` to sort probabilities (set `descending=True`)\n",
    "- Use `torch.cumsum()` to calculate cumulative sum\n",
    "- Use boolean indexing to filter tokens: `probs[cumsum <= p]`\n",
    "- Don't forget to renormalize probabilities after filtering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_top_p(prompt, max_length=20, temperature=1.0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text using top-p (nucleus) sampling\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        max_length: Number of tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Cumulative probability threshold (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # TODO: Implement top-p sampling\n",
    "        # Your code here:\n",
    "        \n",
    "        # 1. Apply temperature\n",
    "        scaled_logits = None  # Your code\n",
    "        \n",
    "        # 2. Get probabilities\n",
    "        probabilities = None  # Your code\n",
    "        \n",
    "        # 3. Sort probabilities in descending order\n",
    "        sorted_probs, sorted_indices = None, None  # Your code (use torch.sort)\n",
    "        \n",
    "        # 4. Calculate cumulative sum\n",
    "        cumsum_probs = None  # Your code (use torch.cumsum)\n",
    "        \n",
    "        # 5. Find tokens to keep (where cumsum <= top_p)\n",
    "        # Add at least one token to handle edge case\n",
    "        keep_mask = None  # Your code\n",
    "        \n",
    "        # 6. Filter and renormalize probabilities\n",
    "        filtered_probs = None  # Your code\n",
    "        filtered_indices = None  # Your code\n",
    "        \n",
    "        # Renormalize\n",
    "        filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "        \n",
    "        # 7. Sample from filtered distribution\n",
    "        sampled_index = torch.multinomial(filtered_probs, num_samples=1)\n",
    "        next_token_id = filtered_indices[sampled_index]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Top-p sampling (p=0.5 - conservative):\")\n",
    "print(generate_text_top_p(\"Once upon a time\", max_length=20, temperature=0.8, top_p=0.5))\n",
    "print(\"\\nTop-p sampling (p=0.9 - balanced):\")\n",
    "print(generate_text_top_p(\"Once upon a time\", max_length=20, temperature=0.8, top_p=0.9))\n",
    "print(\"\\nTop-p sampling (p=0.95 - creative):\")\n",
    "print(generate_text_top_p(\"Once upon a time\", max_length=20, temperature=0.8, top_p=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Solution for Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_top_p(prompt, max_length=20, temperature=1.0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text using top-p (nucleus) sampling - SOLUTION\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = torch.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        # Sort probabilities in descending order\n",
    "        sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
    "        \n",
    "        # Calculate cumulative sum\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        # Find tokens to keep (where cumsum <= top_p)\n",
    "        # We keep at least one token (the highest probability one)\n",
    "        keep_mask = cumsum_probs <= top_p\n",
    "        # Ensure at least one token is kept\n",
    "        keep_mask[0] = True\n",
    "        \n",
    "        # Filter probabilities and indices\n",
    "        filtered_probs = sorted_probs[keep_mask]\n",
    "        filtered_indices = sorted_indices[keep_mask]\n",
    "        \n",
    "        # Renormalize\n",
    "        filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "        \n",
    "        # Sample from filtered distribution\n",
    "        sampled_index = torch.multinomial(filtered_probs, num_samples=1)\n",
    "        next_token_id = filtered_indices[sampled_index]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Test\n",
    "print(\"Top-p sampling (p=0.5):\")\n",
    "print(generate_text_top_p(\"Once upon a time\", max_length=20, temperature=0.8, top_p=0.5))\n",
    "print(\"\\nTop-p sampling (p=0.9):\")\n",
    "print(generate_text_top_p(\"Once upon a time\", max_length=20, temperature=0.8, top_p=0.9))\n",
    "print(\"\\nTop-p sampling (p=0.95):\")\n",
    "print(generate_text_top_p(\"Once upon a time\", max_length=20, temperature=0.8, top_p=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Exercise 4: Implementing Stopping Criteria\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Objective:** Add intelligent stopping conditions to text generation instead of always generating a fixed number of tokens.\n",
    "\n",
    "### Why Stopping Criteria?\n",
    "\n",
    "In real applications, you don't want to generate exactly N tokens. You want to stop when:\n",
    "- An end-of-sequence (EOS) token is generated\n",
    "- A complete sentence is finished (ending with `.`, `!`, or `?`)\n",
    "- A maximum length is reached (safety limit)\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "Modify the generation function to support multiple stopping conditions.\n",
    "\n",
    "**Requirements:**\n",
    "1. Stop if EOS token is generated (check `tokenizer.eos_token_id`)\n",
    "2. Stop if a sentence-ending punctuation is generated (`.`, `!`, `?`)\n",
    "3. Stop if max_length is reached\n",
    "4. Return the generated text and the reason for stopping\n",
    "\n",
    "**Hints:**\n",
    "- Use `tokenizer.eos_token_id` to get the EOS token ID\n",
    "- Use `tokenizer.decode([token_id])` to check if a token is punctuation\n",
    "- Keep track of which stopping condition was met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_stopping(prompt, max_length=50, temperature=0.8, stop_on_sentence=True):\n",
    "    \"\"\"\n",
    "    Generate text with intelligent stopping criteria\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        max_length: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        stop_on_sentence: Whether to stop at sentence endings\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (generated_text, stop_reason)\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # TODO: Implement stopping criteria\n",
    "    # Your code here:\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Sample next token (using temperature sampling)\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        probabilities = torch.softmax(scaled_logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "        \n",
    "        # Add to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        # TODO: Check stopping conditions\n",
    "        \n",
    "        # 1. Check if EOS token\n",
    "        if None:  # Your code\n",
    "            return tokenizer.decode(input_ids[0]), \"EOS token\"\n",
    "        \n",
    "        # 2. Check if sentence-ending punctuation\n",
    "        if stop_on_sentence:\n",
    "            token_text = None  # Your code: decode the token\n",
    "            if None:  # Your code: check if token is '.', '!', or '?'\n",
    "                return tokenizer.decode(input_ids[0]), \"Sentence ending\"\n",
    "    \n",
    "    # 3. Reached max length\n",
    "    return tokenizer.decode(input_ids[0]), \"Max length\"\n",
    "\n",
    "# Test your implementation\n",
    "prompts = [\n",
    "    \"The secret to happiness is\",\n",
    "    \"In the beginning\",\n",
    "    \"Scientists discovered that\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    text, reason = generate_with_stopping(prompt, max_length=30, temperature=0.8)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: {text}\")\n",
    "    print(f\"Stopped because: {reason}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Solution for Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_stopping(prompt, max_length=50, temperature=0.8, stop_on_sentence=True):\n",
    "    \"\"\"\n",
    "    Generate text with intelligent stopping criteria - SOLUTION\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Sample next token\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        probabilities = torch.softmax(scaled_logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "        \n",
    "        # Add to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "        # Check stopping conditions\n",
    "        \n",
    "        # 1. Check if EOS token\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            return tokenizer.decode(input_ids[0]), \"EOS token\"\n",
    "        \n",
    "        # 2. Check if sentence-ending punctuation\n",
    "        if stop_on_sentence:\n",
    "            token_text = tokenizer.decode([next_token_id.item()])\n",
    "            if token_text.strip() in ['.', '!', '?']:\n",
    "                return tokenizer.decode(input_ids[0]), \"Sentence ending\"\n",
    "    \n",
    "    # 3. Reached max length\n",
    "    return tokenizer.decode(input_ids[0]), \"Max length\"\n",
    "\n",
    "# Test\n",
    "prompts = [\n",
    "    \"The secret to happiness is\",\n",
    "    \"In the beginning\",\n",
    "    \"Scientists discovered that\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    text, reason = generate_with_stopping(prompt, max_length=30, temperature=0.8)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: {text}\")\n",
    "    print(f\"Stopped because: {reason}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Exercise 5: Batch Text Generation\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Objective:** Generate multiple different continuations from the same prompt in parallel.\n",
    "\n",
    "### Why Batch Generation?\n",
    "\n",
    "In many applications, you want to:\n",
    "- Generate multiple story variations\n",
    "- Compare different outputs and select the best one\n",
    "- Show users multiple options to choose from\n",
    "- Improve efficiency by processing in parallel\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "Create a function that generates N different continuations from the same prompt.\n",
    "\n",
    "**Requirements:**\n",
    "1. Generate `num_samples` different continuations\n",
    "2. Each continuation should use temperature sampling for diversity\n",
    "3. Set different random seeds for each sample (or just rely on sampling randomness)\n",
    "4. Return a list of all generated texts\n",
    "\n",
    "**Challenge:** Can you make this more efficient by using batch processing?\n",
    "\n",
    "**Hints:**\n",
    "- Simple approach: Loop N times and generate each sample independently\n",
    "- Advanced approach: Use batch dimension in tensors (replicate input_ids N times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_samples(prompt, num_samples=5, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate multiple different continuations from the same prompt\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        num_samples: Number of different continuations to generate\n",
    "        max_length: Tokens to generate per sample\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        list: List of generated texts\n",
    "    \"\"\"\n",
    "    # TODO: Generate num_samples different continuations\n",
    "    # Your code here:\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Generate one sample\n",
    "        generated_text = None  # Your code: use generate_text_with_temperature or similar\n",
    "        samples.append(generated_text)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Test your implementation\n",
    "prompt = \"The mysterious door opened to reveal\"\n",
    "samples = generate_multiple_samples(prompt, num_samples=5, max_length=15, temperature=1.2)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"Generated continuations:\\n\")\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    print(f\"{i}. {sample}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Solution for Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_samples(prompt, num_samples=5, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate multiple different continuations - SOLUTION\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Generate one sample using temperature sampling\n",
    "        generated_text = generate_text_with_temperature(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            temperature=temperature\n",
    "        )\n",
    "        samples.append(generated_text)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Test\n",
    "prompt = \"The mysterious door opened to reveal\"\n",
    "samples = generate_multiple_samples(prompt, num_samples=5, max_length=15, temperature=1.2)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"Generated continuations:\\n\")\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    print(f\"{i}. {sample}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Compare Sampling Methods\n",
    "\n",
    "Now that you've implemented all the sampling methods, create a comprehensive comparison:\n",
    "\n",
    "1. Generate 10 continuations using each method (greedy, temperature, top-k, top-p)\n",
    "2. For each method, calculate:\n",
    "   - Average text length\n",
    "   - Number of unique continuations\n",
    "   - Diversity score (ratio of unique words to total words)\n",
    "3. Visualize the results (optional: create bar charts)\n",
    "4. Write a brief analysis of which method works best for different scenarios\n",
    "\n",
    "**Think about:**\n",
    "- Which method produces the most diverse outputs?\n",
    "- Which method produces the most coherent outputs?\n",
    "- Which method would you use for creative writing vs. factual completion?\n",
    "- How do temperature and top-p interact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis code here\n",
    "# Feel free to be creative with your comparison!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations on completing the beginner exercises! You now have a solid understanding of:\n",
    "- Different sampling strategies (greedy, temperature, top-k, top-p)\n",
    "- Stopping criteria for text generation\n",
    "- Generating multiple samples for diversity\n",
    "\n",
    "**Ready for more?** Try the intermediate exercises:\n",
    "- Implement beam search\n",
    "- Add repetition penalties\n",
    "- Build an interactive storytelling application\n",
    "- Experiment with different models and compare their outputs\n",
    "\n",
    "**Remember:** Understanding how LLMs generate text token-by-token is fundamental to working with modern AI systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}