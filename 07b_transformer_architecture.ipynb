{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "# Module 7b: The Transformer Architecture\n",
    "\n",
    "## Prerequisites\n",
    "- **Module 7a: Self-Attention** -- you should be comfortable with scaled dot-product attention and multi-head attention before starting this module.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Module 7a we built the core **self-attention** mechanism from scratch. In this notebook we assemble the remaining pieces needed to go from a single attention operation to a **complete Transformer architecture**:\n",
    "\n",
    "1. **Positional Encoding** -- injecting sequence-order information\n",
    "2. **Layer Normalization** -- stabilizing training\n",
    "3. **Residual (Skip) Connections** -- enabling gradient flow\n",
    "4. **Position-wise Feed-Forward Network** -- adding non-linear capacity\n",
    "5. **Encoder Block** -- combining all of the above\n",
    "6. **Decoder Block** -- adding causal masking and cross-attention\n",
    "7. **Architecture Variants** -- BERT, GPT, T5 and how they differ\n",
    "8. **Pre-training Objectives** -- MLM, CLM, span corruption\n",
    "\n",
    "By the end you will have a working, from-scratch Transformer encoder that you can trace tensor shapes through from input to output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0002-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0003-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Positional Encoding\n",
    "\n",
    "### Why do we need positional information?\n",
    "\n",
    "Self-attention is **permutation-invariant**: if you shuffle the input tokens, each token's attention output changes (because it now attends to different positions), but the *set* of operations is the same -- there is nothing in the attention equations themselves that distinguishes \"position 0\" from \"position 5\". In contrast, RNNs process tokens sequentially and naturally encode order.\n",
    "\n",
    "To give the Transformer a sense of **where** each token sits in the sequence, the original paper (Vaswani et al., 2017) adds a **positional encoding** vector to each token embedding before it enters the first layer.\n",
    "\n",
    "### Sinusoidal Positional Encoding\n",
    "\n",
    "The original Transformer uses fixed sinusoidal functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "where:\n",
    "- `pos` is the position in the sequence (0, 1, 2, ...)\n",
    "- `i` is the dimension index (0, 1, 2, ..., d_model/2 - 1)\n",
    "- `d_model` is the embedding dimension\n",
    "\n",
    "**Intuition:** Each dimension oscillates at a different frequency. Low-index dimensions change slowly (long wavelengths), high-index dimensions change rapidly. This creates a unique \"fingerprint\" for every position, and relative positions can be represented as linear functions of the encodings.\n",
    "\n",
    "### Learned Positional Embeddings\n",
    "\n",
    "An alternative (used in BERT, GPT-2, etc.) is to **learn** a positional embedding matrix of shape `(max_seq_len, d_model)`. This is simpler to implement (`nn.Embedding`) but cannot extrapolate to sequence lengths longer than those seen during training. The sinusoidal approach generalizes to arbitrary lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding from 'Attention Is All You Need'.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        # Compute the div_term: 10000^(2i/d_model) -- use log-space for numerical stability\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
    "        )  # (d_model/2,)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model) -- batch dimension\n",
    "        self.register_buffer('pe', pe)  # not a parameter, but should move with .to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor of same shape with positional encoding added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Quick test\n",
    "d_model = 64\n",
    "seq_len = 20\n",
    "pe_module = PositionalEncoding(d_model=d_model, dropout=0.0)\n",
    "\n",
    "# Feed in zeros so we can see the raw positional encodings\n",
    "dummy = torch.zeros(1, seq_len, d_model)\n",
    "encoded = pe_module(dummy)\n",
    "print(f\"Input shape:  {dummy.shape}\")\n",
    "print(f\"Output shape: {encoded.shape}\")\n",
    "print(f\"PE values at position 0: {encoded[0, 0, :8]}\")\n",
    "print(f\"PE values at position 1: {encoded[0, 1, :8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0003-0003-0001-000000000001",
   "metadata": {},
   "source": [
    "### Visualizing Positional Encodings\n",
    "\n",
    "Let's create a heatmap showing how the encoding values vary across positions and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings\n",
    "pe_vis = PositionalEncoding(d_model=128, max_len=100, dropout=0.0)\n",
    "pe_values = pe_vis(torch.zeros(1, 100, 128))[0].detach().numpy()  # (100, 128)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].imshow(pe_values, aspect='auto', cmap='RdBu', interpolation='nearest')\n",
    "axes[0].set_xlabel('Embedding Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Sinusoidal Positional Encoding Heatmap')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# A few individual dimensions over position\n",
    "for dim_idx in [0, 1, 4, 5, 20, 21]:\n",
    "    axes[1].plot(pe_values[:, dim_idx], label=f'dim {dim_idx}')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Encoding Value')\n",
    "axes[1].set_title('Positional Encoding by Dimension')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: lower dimensions oscillate slowly, higher dimensions oscillate rapidly.\")\n",
    "print(\"Each position gets a unique encoding vector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0004-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Implement Sinusoidal Positional Encoding + Visualize\n",
    "\n",
    "Implement the `PositionalEncodingExercise` class below. Fill in the `TODO` placeholders.\n",
    "\n",
    "**Requirements:**\n",
    "1. Compute the sinusoidal positional encoding matrix of shape `(max_len, d_model)`\n",
    "2. Apply `sin` to even indices and `cos` to odd indices\n",
    "3. In `forward`, add the positional encoding to the input\n",
    "4. Create a heatmap visualization of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingExercise(nn.Module):\n",
    "    \"\"\"Exercise: Implement sinusoidal positional encoding from scratch.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "\n",
    "        # TODO: Compute div_term using the formula: exp(arange(0, d_model, 2) * (-log(10000) / d_model))\n",
    "        div_term = None\n",
    "\n",
    "        # TODO: Fill even indices (0, 2, 4, ...) with sin(position * div_term)\n",
    "        # pe[:, 0::2] = ...\n",
    "\n",
    "        # TODO: Fill odd indices (1, 3, 5, ...) with cos(position * div_term)\n",
    "        # pe[:, 1::2] = ...\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Add positional encoding to x (only up to x's sequence length)\n",
    "        return None\n",
    "\n",
    "\n",
    "# TODO: Create an instance with d_model=64, max_len=50\n",
    "# Pass zeros through it and visualize as a heatmap\n",
    "# pe_ex = ...\n",
    "# result = ...\n",
    "\n",
    "# TODO: Create heatmap using plt.imshow\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.imshow(...)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0004-0003-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodingExercise(nn.Module):\n",
    "    \"\"\"Solution: Sinusoidal positional encoding from scratch.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "\n",
    "        # Compute div_term in log-space for numerical stability\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Even indices get sin, odd indices get cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create and visualize\n",
    "pe_ex = PositionalEncodingExercise(d_model=64, max_len=50)\n",
    "result = pe_ex(torch.zeros(1, 50, 64))\n",
    "pe_data = result[0].detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(pe_data, aspect='auto', cmap='RdBu', interpolation='nearest')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Token Position')\n",
    "plt.title('Exercise 1 Solution: Sinusoidal Positional Encoding')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Output shape: {result.shape}\")\n",
    "print(f\"Each position has a unique encoding. Position 0 != Position 1:\")\n",
    "print(f\"  Position 0: {pe_data[0, :6].round(3)}\")\n",
    "print(f\"  Position 1: {pe_data[1, :6].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0005-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Layer Normalization\n",
    "\n",
    "### Why Normalization Helps Training\n",
    "\n",
    "Deep networks suffer from **internal covariate shift**: the distribution of each layer's inputs changes as the parameters of preceding layers change during training. Normalization techniques address this by re-centering and re-scaling activations, which:\n",
    "\n",
    "- Stabilizes and accelerates training\n",
    "- Reduces sensitivity to initialization and learning rate\n",
    "- Acts as a mild regularizer\n",
    "\n",
    "### Layer Norm vs Batch Norm\n",
    "\n",
    "| Property | Batch Norm | Layer Norm |\n",
    "|----------|-----------|------------|\n",
    "| Normalizes across | Batch dimension (for each feature) | Feature dimension (for each sample) |\n",
    "| Statistics | Mean/var over batch | Mean/var over features |\n",
    "| Depends on batch size | Yes | No |\n",
    "| Works for variable-length sequences | Poorly | Well |\n",
    "| Used in Transformers | Rarely | Always |\n",
    "\n",
    "**Layer Norm** computes statistics across the **feature dimension** for each individual sample independently. This makes it ideal for Transformers where:\n",
    "- Batch sizes may be small\n",
    "- Sequence lengths vary\n",
    "- We need inference with batch_size=1\n",
    "\n",
    "### Formula\n",
    "\n",
    "For a vector $\\mathbf{x}$ of dimension $d$:\n",
    "\n",
    "$$\\text{LayerNorm}(\\mathbf{x}) = \\gamma \\odot \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "where $\\mu = \\frac{1}{d}\\sum_i x_i$, $\\sigma^2 = \\frac{1}{d}\\sum_i (x_i - \\mu)^2$, and $\\gamma$, $\\beta$ are learnable scale/shift parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer Normalization implemented from scratch.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))   # learnable scale\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))   # learnable shift\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        mean = x.mean(dim=-1, keepdim=True)          # mean over d_model\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)  # variance over d_model\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "# Compare with PyTorch's built-in LayerNorm\n",
    "d_model = 64\n",
    "x = torch.randn(2, 10, d_model)  # (batch=2, seq_len=10, d_model=64)\n",
    "\n",
    "our_ln = LayerNorm(d_model)\n",
    "pytorch_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "# Copy weights so we can compare outputs\n",
    "pytorch_ln.weight = nn.Parameter(our_ln.gamma.data.clone())\n",
    "pytorch_ln.bias = nn.Parameter(our_ln.beta.data.clone())\n",
    "\n",
    "out_ours = our_ln(x)\n",
    "out_pytorch = pytorch_ln(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Our LayerNorm output shape: {out_ours.shape}\")\n",
    "print(f\"Max absolute difference: {(out_ours - out_pytorch).abs().max().item():.2e}\")\n",
    "print(f\"Outputs match: {torch.allclose(out_ours, out_pytorch, atol=1e-5)}\")\n",
    "\n",
    "# Verify normalization: mean should be ~0, std should be ~1\n",
    "print(f\"\\nAfter LayerNorm (sample 0, position 0):\")\n",
    "print(f\"  Mean: {out_ours[0, 0].mean().item():.6f}\")\n",
    "print(f\"  Std:  {out_ours[0, 0].std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Residual Connections\n",
    "\n",
    "### Skip Connections\n",
    "\n",
    "In the Transformer each sub-layer (attention, feed-forward) is wrapped with a **residual connection** followed by layer normalization:\n",
    "\n",
    "$$\\text{output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "### Why Residual Connections Help\n",
    "\n",
    "1. **Gradient flow**: Without skip connections, gradients must flow through every layer sequentially. With skip connections, gradients have a \"highway\" that bypasses layers, reducing vanishing gradients.\n",
    "2. **Training stability**: The identity path ensures that, at worst, a layer can learn to do nothing (the identity function), rather than being forced to learn a complex transformation from scratch.\n",
    "3. **Deeper networks**: Residual connections are what make it practical to train Transformers with 12, 24, or even 100+ layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0006-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"Residual connection followed by layer normalization.\n",
    "    \n",
    "    Implements: LayerNorm(x + Sublayer(x))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sublayer_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply residual connection: LayerNorm(x + Dropout(sublayer_output))\"\"\"\n",
    "        return self.norm(x + self.dropout(sublayer_output))\n",
    "\n",
    "\n",
    "# Demonstrate gradient flow with and without residual connections\n",
    "print(\"=== Gradient Flow: With vs Without Residual Connections ===\")\n",
    "print()\n",
    "\n",
    "def simulate_gradient_flow(n_layers, use_residual=True):\n",
    "    \"\"\"Simulate forward/backward pass and measure gradient magnitude at each layer.\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    d = 64\n",
    "    x = torch.randn(1, d, requires_grad=True)\n",
    "\n",
    "    layers = [nn.Linear(d, d) for _ in range(n_layers)]\n",
    "    # Initialize with small weights to simulate deep network issues\n",
    "    for layer in layers:\n",
    "        nn.init.normal_(layer.weight, std=0.5)\n",
    "\n",
    "    # Forward pass\n",
    "    activations = [x]\n",
    "    h = x\n",
    "    for layer in layers:\n",
    "        out = torch.tanh(layer(h))\n",
    "        if use_residual:\n",
    "            h = h + out  # residual connection\n",
    "        else:\n",
    "            h = out\n",
    "        activations.append(h)\n",
    "\n",
    "    # Backward pass\n",
    "    loss = h.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    return x.grad.norm().item()\n",
    "\n",
    "\n",
    "layer_counts = [1, 5, 10, 20, 50]\n",
    "grads_no_res = [simulate_gradient_flow(n, use_residual=False) for n in layer_counts]\n",
    "grads_with_res = [simulate_gradient_flow(n, use_residual=True) for n in layer_counts]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(layer_counts, grads_no_res, 'ro-', label='Without Residual', linewidth=2)\n",
    "ax.plot(layer_counts, grads_with_res, 'bs-', label='With Residual', linewidth=2)\n",
    "ax.set_xlabel('Number of Layers')\n",
    "ax.set_ylabel('Gradient Norm at Input')\n",
    "ax.set_title('Gradient Flow: Residual vs Non-Residual Networks')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Without residuals, gradients vanish as depth increases.\")\n",
    "print(\"With residuals, gradients remain healthy even in deep networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0007-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Feed-Forward Network\n",
    "\n",
    "Each Transformer layer contains a **position-wise feed-forward network** (FFN) applied independently to each position:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0,\\, xW_1 + b_1)\\, W_2 + b_2$$\n",
    "\n",
    "Key properties:\n",
    "- **Two linear transformations** with a ReLU activation in between\n",
    "- **Expansion factor**: The inner dimension `d_ff` is typically **4x** the model dimension. For example, if `d_model=512`, then `d_ff=2048`.\n",
    "- **Position-wise**: The same FFN is applied to every position independently (like a 1x1 convolution).\n",
    "- **Purpose**: Adds non-linear transformation capacity. Attention alone is essentially a weighted average (linear), so the FFN provides the network's non-linear processing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n",
    "# Test the feed-forward network\n",
    "d_model = 64\n",
    "d_ff = 256  # 4x expansion\n",
    "ffn = FeedForward(d_model, d_ff, dropout=0.0)\n",
    "\n",
    "x = torch.randn(2, 10, d_model)  # (batch=2, seq_len=10, d_model=64)\n",
    "out = ffn(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"\\nFFN parameters:\")\n",
    "print(f\"  W1: {d_model} -> {d_ff} = {d_model * d_ff:,} weights + {d_ff} bias\")\n",
    "print(f\"  W2: {d_ff} -> {d_model} = {d_ff * d_model:,} weights + {d_model} bias\")\n",
    "total_params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"  Total: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0008-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Multi-Head Attention (Recap from 7a)\n",
    "\n",
    "Before building the encoder block, let's define a clean multi-head attention module. This is the same mechanism from Module 7a, packaged as an `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism.\n",
    "    \n",
    "    Splits d_model into n_heads parallel attention heads,\n",
    "    each with dimension d_k = d_model / n_heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # Linear projections for Q, K, V, and output\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Compute scaled dot-product attention.\n",
    "        \n",
    "        Args:\n",
    "            Q: (batch, n_heads, seq_len, d_k)\n",
    "            K: (batch, n_heads, seq_len, d_k)\n",
    "            V: (batch, n_heads, seq_len, d_k)\n",
    "            mask: optional mask tensor\n",
    "        Returns:\n",
    "            output: (batch, n_heads, seq_len, d_k)\n",
    "            attn_weights: (batch, n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            query: (batch, seq_len_q, d_model)\n",
    "            key:   (batch, seq_len_k, d_model)\n",
    "            value: (batch, seq_len_v, d_model)\n",
    "            mask:  optional mask\n",
    "        Returns:\n",
    "            output: (batch, seq_len_q, d_model)\n",
    "            attn_weights: (batch, n_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1. Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        # 2. Reshape to (batch, n_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Scaled dot-product attention\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 4. Concatenate heads: (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 5. Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# Quick test\n",
    "mha = MultiHeadAttention(d_model=64, n_heads=4, dropout=0.0)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out, weights = mha(x, x, x)\n",
    "print(f\"Multi-Head Attention:\")\n",
    "print(f\"  Input:   {x.shape}\")\n",
    "print(f\"  Output:  {out.shape}\")\n",
    "print(f\"  Weights: {weights.shape}  (batch, heads, seq_q, seq_k)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0009-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Complete Encoder Block\n",
    "\n",
    "Now we assemble all components into a single **Transformer Encoder Block**:\n",
    "\n",
    "```\n",
    "Input (batch, seq_len, d_model)\n",
    "  |\n",
    "  |---> Multi-Head Self-Attention\n",
    "  |         |\n",
    "  +-----> Add (residual)\n",
    "            |\n",
    "          LayerNorm\n",
    "            |\n",
    "            |---> Feed-Forward Network\n",
    "            |         |\n",
    "            +-----> Add (residual)\n",
    "                      |\n",
    "                    LayerNorm\n",
    "                      |\n",
    "                    Output (batch, seq_len, d_model)\n",
    "```\n",
    "\n",
    "Each encoder block preserves the input shape, so blocks can be stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"A single Transformer Encoder block.\n",
    "    \n",
    "    Components:\n",
    "        1. Multi-Head Self-Attention + Add & Norm\n",
    "        2. Feed-Forward Network + Add & Norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sub-layer 1: Multi-Head Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Sub-layer 2: Feed-Forward Network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "            mask: optional attention mask\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Self-Attention + Add & Norm\n",
    "        attn_output, attn_weights = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))  # residual + norm\n",
    "\n",
    "        # Sub-layer 2: FFN + Add & Norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))   # residual + norm\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the encoder block\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "d_ff = 256\n",
    "\n",
    "encoder_block = TransformerEncoderBlock(d_model, n_heads, d_ff, dropout=0.0)\n",
    "\n",
    "x = torch.randn(2, 10, d_model)  # (batch=2, seq_len=10, d_model=64)\n",
    "out = encoder_block(x)\n",
    "\n",
    "print(f\"Encoder Block Test:\")\n",
    "print(f\"  Input shape:  {x.shape}\")\n",
    "print(f\"  Output shape: {out.shape}\")\n",
    "print(f\"  Shape preserved: {x.shape == out.shape}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in encoder_block.parameters())\n",
    "print(f\"\\n  Total parameters: {total_params:,}\")\n",
    "print(f\"\\n  Parameter breakdown:\")\n",
    "for name, param in encoder_block.named_parameters():\n",
    "    print(f\"    {name}: {param.shape} ({param.numel():,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Build a Transformer Encoder Block from Scratch\n",
    "\n",
    "Implement a complete Transformer encoder block as an `nn.Module`. You should use the `MultiHeadAttention`, `FeedForward`, and `nn.LayerNorm` classes.\n",
    "\n",
    "**Requirements:**\n",
    "1. Self-attention with residual connection and layer norm\n",
    "2. Feed-forward with residual connection and layer norm\n",
    "3. The output shape must match the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlockExercise(nn.Module):\n",
    "    \"\"\"Exercise: Implement a Transformer Encoder Block.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Create multi-head attention layer\n",
    "        self.self_attn = None\n",
    "\n",
    "        # TODO: Create first layer norm\n",
    "        self.norm1 = None\n",
    "\n",
    "        # TODO: Create feed-forward network\n",
    "        self.ffn = None\n",
    "\n",
    "        # TODO: Create second layer norm\n",
    "        self.norm2 = None\n",
    "\n",
    "        # TODO: Create dropout layers\n",
    "        self.dropout1 = None\n",
    "        self.dropout2 = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        # TODO: Sub-layer 1 -- Self-Attention + Add & Norm\n",
    "        # attn_output, _ = self.self_attn(...)\n",
    "        # x = self.norm1(...)\n",
    "\n",
    "        # TODO: Sub-layer 2 -- Feed-Forward + Add & Norm\n",
    "        # ffn_output = self.ffn(...)\n",
    "        # x = self.norm2(...)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# TODO: Test your implementation\n",
    "# enc = TransformerEncoderBlockExercise(d_model=64, n_heads=4, d_ff=256, dropout=0.0)\n",
    "# test_input = torch.randn(2, 10, 64)\n",
    "# test_output = enc(test_input)\n",
    "# print(f\"Input: {test_input.shape}, Output: {test_output.shape}\")\n",
    "# assert test_input.shape == test_output.shape, \"Shape mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-0003-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0010-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlockExercise(nn.Module):\n",
    "    \"\"\"Solution: Complete Transformer Encoder Block.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "\n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        # Sub-layer 1: Self-Attention + Add & Norm\n",
    "        attn_output, attn_weights = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # Sub-layer 2: Feed-Forward + Add & Norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the solution\n",
    "enc = TransformerEncoderBlockExercise(d_model=64, n_heads=4, d_ff=256, dropout=0.0)\n",
    "test_input = torch.randn(2, 10, 64)\n",
    "test_output = enc(test_input)\n",
    "\n",
    "print(f\"Input shape:  {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "assert test_input.shape == test_output.shape, \"Shape mismatch!\"\n",
    "print(\"Shape check passed!\")\n",
    "\n",
    "# Stack multiple blocks to form a deeper encoder\n",
    "n_layers = 3\n",
    "encoder_blocks = nn.ModuleList([\n",
    "    TransformerEncoderBlockExercise(d_model=64, n_heads=4, d_ff=256, dropout=0.0)\n",
    "    for _ in range(n_layers)\n",
    "])\n",
    "\n",
    "h = test_input\n",
    "for i, block in enumerate(encoder_blocks):\n",
    "    h = block(h)\n",
    "    print(f\"After block {i}: {h.shape}\")\n",
    "\n",
    "print(f\"\\nFinal output shape: {h.shape}\")\n",
    "print(f\"Total parameters in {n_layers}-layer encoder: {sum(p.numel() for p in encoder_blocks.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0011-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Decoder Block\n",
    "\n",
    "The Transformer **decoder** block is similar to the encoder block but with two key additions:\n",
    "\n",
    "1. **Masked Self-Attention (Causal Masking):** The decoder must not look at future tokens during generation. We apply a causal mask so that position $i$ can only attend to positions $\\leq i$.\n",
    "\n",
    "2. **Cross-Attention:** After masked self-attention, the decoder attends to the **encoder output**. Here, the queries come from the decoder but the keys and values come from the encoder.\n",
    "\n",
    "```\n",
    "Decoder Input (batch, tgt_len, d_model)\n",
    "  |\n",
    "  |---> Masked Multi-Head Self-Attention    <-- causal mask\n",
    "  |         |\n",
    "  +-----> Add & Norm\n",
    "            |\n",
    "            |---> Multi-Head Cross-Attention <-- Q from decoder, K,V from encoder\n",
    "            |         |\n",
    "            +-----> Add & Norm\n",
    "                      |\n",
    "                      |---> Feed-Forward\n",
    "                      |         |\n",
    "                      +-----> Add & Norm\n",
    "                                |\n",
    "                              Output (batch, tgt_len, d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"Create a causal (lower-triangular) mask.\n",
    "    \n",
    "    Returns a mask of shape (1, 1, seq_len, seq_len) where:\n",
    "        mask[..., i, j] = 1 if j <= i (allowed)\n",
    "        mask[..., i, j] = 0 if j > i  (blocked / future)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "    return mask  # (1, 1, seq_len, seq_len)\n",
    "\n",
    "\n",
    "# Visualize the causal mask\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "print(f\"Causal mask shape: {causal_mask.shape}\")\n",
    "print(f\"Causal mask:\\n{causal_mask[0, 0].int()}\")\n",
    "print()\n",
    "print(\"1 = can attend, 0 = cannot attend (future token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"A single Transformer Decoder block.\n",
    "    \n",
    "    Components:\n",
    "        1. Masked Multi-Head Self-Attention + Add & Norm\n",
    "        2. Multi-Head Cross-Attention + Add & Norm\n",
    "        3. Feed-Forward Network + Add & Norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sub-layer 1: Masked Self-Attention\n",
    "        self.masked_self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Sub-layer 2: Cross-Attention (decoder queries, encoder keys/values)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Sub-layer 3: Feed-Forward\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x:              (batch, tgt_len, d_model) -- decoder input\n",
    "            encoder_output: (batch, src_len, d_model) -- encoder output\n",
    "            src_mask:       mask for cross-attention (optional)\n",
    "            tgt_mask:       causal mask for self-attention (optional)\n",
    "        Returns:\n",
    "            (batch, tgt_len, d_model)\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Masked Self-Attention\n",
    "        self_attn_out, _ = self.masked_self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(self_attn_out))\n",
    "\n",
    "        # Sub-layer 2: Cross-Attention\n",
    "        # Query from decoder, Key and Value from encoder\n",
    "        cross_attn_out, _ = self.cross_attn(x, encoder_output, encoder_output, mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_out))\n",
    "\n",
    "        # Sub-layer 3: Feed-Forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout3(ffn_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the decoder block\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "d_ff = 256\n",
    "\n",
    "decoder_block = TransformerDecoderBlock(d_model, n_heads, d_ff, dropout=0.0)\n",
    "\n",
    "# Encoder output (e.g., from processing a source sentence)\n",
    "src_len = 12\n",
    "encoder_output = torch.randn(2, src_len, d_model)\n",
    "\n",
    "# Decoder input (e.g., target sentence generated so far)\n",
    "tgt_len = 8\n",
    "decoder_input = torch.randn(2, tgt_len, d_model)\n",
    "\n",
    "# Create causal mask for decoder self-attention\n",
    "tgt_mask = create_causal_mask(tgt_len)\n",
    "\n",
    "decoder_output = decoder_block(decoder_input, encoder_output, tgt_mask=tgt_mask)\n",
    "\n",
    "print(f\"Decoder Block Test:\")\n",
    "print(f\"  Encoder output shape: {encoder_output.shape}  (source)\")\n",
    "print(f\"  Decoder input shape:  {decoder_input.shape}   (target)\")\n",
    "print(f\"  Decoder output shape: {decoder_output.shape}  (same as decoder input)\")\n",
    "print(f\"  Causal mask shape:    {tgt_mask.shape}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in decoder_block.parameters())\n",
    "print(f\"\\n  Total parameters: {total_params:,}\")\n",
    "print(f\"  (More than encoder block because of the extra cross-attention sub-layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0012-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Architecture Variants\n",
    "\n",
    "The original Transformer is an **encoder-decoder** model. In practice, three major variants have emerged, each using a different subset of the architecture:\n",
    "\n",
    "### BERT: Encoder-Only (Bidirectional)\n",
    "\n",
    "```\n",
    "  Input: [CLS] The cat sat on the mat [SEP]\n",
    "           |    |   |   |   |   |   |    |\n",
    "         +------------------------------------+\n",
    "         |     Transformer Encoder Block      |\n",
    "         |     (bidirectional attention)       |\n",
    "         |            x N layers              |\n",
    "         +------------------------------------+\n",
    "           |    |   |   |   |   |   |    |\n",
    "          h0   h1  h2  h3  h4  h5  h6   h7\n",
    "           |                              |\n",
    "       [CLS] for                      Token-level\n",
    "     classification               representations\n",
    "```\n",
    "\n",
    "- **Attention**: Each token attends to ALL other tokens (bidirectional)\n",
    "- **Pre-training**: Masked Language Modeling (MLM) + Next Sentence Prediction\n",
    "- **Use cases**: Text classification, NER, question answering, semantic similarity\n",
    "- **Models**: BERT, RoBERTa, ALBERT, DistilBERT, DeBERTa\n",
    "\n",
    "### GPT: Decoder-Only (Causal/Autoregressive)\n",
    "\n",
    "```\n",
    "  Input: The  cat  sat  on   the  mat\n",
    "          |    |    |    |    |    |\n",
    "        +-------------------------------+\n",
    "        |  Transformer Decoder Block    |\n",
    "        |  (causal/masked attention)    |\n",
    "        |       x N layers              |\n",
    "        +-------------------------------+\n",
    "          |    |    |    |    |    |\n",
    "         cat  sat  on   the  mat  [END]\n",
    "         (predict next token at each step)\n",
    "```\n",
    "\n",
    "- **Attention**: Each token attends only to previous tokens (causal mask)\n",
    "- **Pre-training**: Causal Language Modeling (predict next token)\n",
    "- **Use cases**: Text generation, code generation, chatbots, reasoning\n",
    "- **Models**: GPT-2, GPT-3, GPT-4, LLaMA, Claude, Mistral\n",
    "\n",
    "### T5: Encoder-Decoder (Sequence-to-Sequence)\n",
    "\n",
    "```\n",
    "  Input: translate English to French: The cat sat on the mat\n",
    "          |   |   |   |   |   |   |   |   |   |    |   |\n",
    "        +----------------------------------------------+\n",
    "        |           Transformer Encoder                |\n",
    "        |       (bidirectional attention)               |\n",
    "        +----------------------------------------------+\n",
    "                          |\n",
    "                    encoder output\n",
    "                          |\n",
    "                          v\n",
    "        +----------------------------------------------+\n",
    "        |           Transformer Decoder                |\n",
    "        |  (causal self-attn + cross-attn to encoder)  |\n",
    "        +----------------------------------------------+\n",
    "          |      |      |      |      |       |\n",
    "         Le    chat   s'est  assis   sur      le tapis\n",
    "```\n",
    "\n",
    "- **Attention**: Encoder = bidirectional; Decoder = causal self-attention + cross-attention to encoder\n",
    "- **Pre-training**: Span corruption (mask random spans, reconstruct them)\n",
    "- **Use cases**: Translation, summarization, question answering\n",
    "- **Models**: T5, BART, mBART, FLAN-T5\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | BERT (Encoder) | GPT (Decoder) | T5 (Enc-Dec) |\n",
    "|---------|---------------|---------------|---------------|\n",
    "| Attention Direction | Bidirectional | Causal (left-to-right) | Encoder: bi, Decoder: causal |\n",
    "| Pre-training | MLM + NSP | Next token prediction | Span corruption |\n",
    "| Generation | Not naturally | Excellent | Excellent |\n",
    "| Understanding | Excellent | Good | Excellent |\n",
    "| Cross-Attention | No | No | Yes |\n",
    "| Parameters (base) | 110M | 117M (GPT-2) | 220M |\n",
    "| Typical Use | Classification, NER | Generation, chat | Translation, summarization |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0013-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Compare BERT and GPT Attention Patterns\n",
    "\n",
    "Create the attention masks for:\n",
    "1. **BERT-style** (bidirectional): every token can attend to every other token\n",
    "2. **GPT-style** (causal): each token can only attend to itself and previous tokens\n",
    "\n",
    "Visualize them side-by-side as heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0013-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"[END]\"]\n",
    "\n",
    "# TODO: Create BERT-style attention mask (all ones -- every token attends to every token)\n",
    "# bert_mask = ...\n",
    "bert_mask = None\n",
    "\n",
    "# TODO: Create GPT-style causal attention mask (lower triangular -- no future tokens)\n",
    "# gpt_mask = ...\n",
    "gpt_mask = None\n",
    "\n",
    "# TODO: Create side-by-side heatmap visualization\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Plot bert_mask on axes[0] with title \"BERT (Bidirectional)\"\n",
    "# Plot gpt_mask on axes[1] with title \"GPT (Causal)\"\n",
    "# Set tick labels to the tokens list\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0013-0003-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0013-0004-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\", \"[END]\"]\n",
    "\n",
    "# BERT: bidirectional -- all tokens attend to all tokens\n",
    "bert_mask = torch.ones(seq_len, seq_len)\n",
    "\n",
    "# GPT: causal -- lower triangular (each token attends only to itself and past)\n",
    "gpt_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "# Visualize side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# BERT mask\n",
    "im0 = axes[0].imshow(bert_mask.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title('BERT (Bidirectional) Attention Mask', fontsize=13)\n",
    "axes[0].set_xlabel('Key Position (attending to)')\n",
    "axes[0].set_ylabel('Query Position (attending from)')\n",
    "axes[0].set_xticks(range(seq_len))\n",
    "axes[0].set_yticks(range(seq_len))\n",
    "axes[0].set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].set_yticklabels(tokens, fontsize=9)\n",
    "\n",
    "# GPT mask\n",
    "im1 = axes[1].imshow(gpt_mask.numpy(), cmap='Oranges', vmin=0, vmax=1)\n",
    "axes[1].set_title('GPT (Causal) Attention Mask', fontsize=13)\n",
    "axes[1].set_xlabel('Key Position (attending to)')\n",
    "axes[1].set_ylabel('Query Position (attending from)')\n",
    "axes[1].set_xticks(range(seq_len))\n",
    "axes[1].set_yticks(range(seq_len))\n",
    "axes[1].set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "axes[1].set_yticklabels(tokens, fontsize=9)\n",
    "\n",
    "# Add cell values\n",
    "for ax, mask in [(axes[0], bert_mask), (axes[1], gpt_mask)]:\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            val = int(mask[i, j].item())\n",
    "            color = 'white' if val == 1 else 'black'\n",
    "            ax.text(j, i, str(val), ha='center', va='center', fontsize=8, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"BERT: All positions are 1 -- every token sees the full context.\")\n",
    "print(\"GPT:  Lower triangle is 1 -- each token only sees past and present.\")\n",
    "print(f\"\\nBERT total connections: {int(bert_mask.sum().item())} (all-to-all)\")\n",
    "print(f\"GPT  total connections: {int(gpt_mask.sum().item())} (causal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0014-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Pre-training Objectives\n",
    "\n",
    "Each architecture variant uses a different **pre-training objective** -- the task the model is trained on before fine-tuning:\n",
    "\n",
    "### Masked Language Modeling (MLM) -- BERT\n",
    "\n",
    "Randomly mask 15% of input tokens and train the model to predict them:\n",
    "\n",
    "```\n",
    "Input:  The [MASK] sat on the [MASK]\n",
    "Target:      cat              mat\n",
    "```\n",
    "\n",
    "Of the 15% selected tokens:\n",
    "- 80% are replaced with `[MASK]`\n",
    "- 10% are replaced with a random token\n",
    "- 10% are left unchanged\n",
    "\n",
    "This forces bidirectional understanding: to predict a masked word, the model must use both left and right context.\n",
    "\n",
    "### Causal Language Modeling (CLM) -- GPT\n",
    "\n",
    "Predict the next token given all previous tokens:\n",
    "\n",
    "```\n",
    "Input:  The  cat  sat  on   the\n",
    "Target: cat  sat  on   the  mat\n",
    "```\n",
    "\n",
    "The loss is computed at every position, making training very efficient. This naturally produces a model that can **generate** text by sampling one token at a time.\n",
    "\n",
    "### Span Corruption -- T5\n",
    "\n",
    "Replace random contiguous spans with sentinel tokens, and train the model to output the missing spans:\n",
    "\n",
    "```\n",
    "Input:  The <X> on the <Y>\n",
    "Target: <X> cat sat <Y> mat <END>\n",
    "```\n",
    "\n",
    "This is a seq2seq objective: the encoder processes the corrupted input and the decoder generates the missing pieces. It combines benefits of both MLM (bidirectional context in encoder) and CLM (autoregressive generation in decoder).\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Objective | Model | Masks | Predicts | Direction |\n",
    "|-----------|-------|-------|----------|-----------|\n",
    "| MLM | BERT | Random 15% of tokens | Masked tokens | Bidirectional |\n",
    "| CLM | GPT | Causal (future) | Next token | Left-to-right |\n",
    "| Span Corruption | T5 | Random spans | Missing spans | Encoder: bi; Decoder: L-to-R |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0015-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Putting It All Together\n",
    "\n",
    "Let's build a minimal **complete Transformer Encoder** that processes a sequence through all stages, and trace the tensor shapes at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0015-0002-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Complete Transformer Encoder.\n",
    "    \n",
    "    Pipeline:\n",
    "        Token IDs -> Embedding -> Positional Encoding -> N x EncoderBlock -> Output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int,\n",
    "        n_layers: int,\n",
    "        max_seq_len: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "\n",
    "        # Stack of encoder blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len) -- token IDs\n",
    "            mask: optional attention mask\n",
    "            verbose: if True, print shapes at each step\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"  Input token IDs:      {x.shape}\")\n",
    "\n",
    "        # Step 1: Token embedding (scale by sqrt(d_model) as in original paper)\n",
    "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
    "        if verbose:\n",
    "            print(f\"  After embedding:      {x.shape}\")\n",
    "\n",
    "        # Step 2: Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        if verbose:\n",
    "            print(f\"  After pos encoding:   {x.shape}\")\n",
    "\n",
    "        # Step 3: Pass through encoder blocks\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, mask)\n",
    "            if verbose:\n",
    "                print(f\"  After encoder block {i}: {x.shape}\")\n",
    "\n",
    "        # Step 4: Final layer norm\n",
    "        x = self.final_norm(x)\n",
    "        if verbose:\n",
    "            print(f\"  After final norm:     {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# === Build and trace a complete encoder ===\n",
    "\n",
    "# Hyperparameters (small for demonstration)\n",
    "vocab_size = 1000\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "d_ff = 256\n",
    "n_layers = 3\n",
    "max_seq_len = 128\n",
    "\n",
    "# Create model\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    n_layers=n_layers,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "# Create dummy input: batch of 2 sequences, each 10 tokens\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Tracing data flow through the Transformer Encoder\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nConfig: vocab={vocab_size}, d_model={d_model}, heads={n_heads}, \"\n",
    "      f\"d_ff={d_ff}, layers={n_layers}\")\n",
    "print()\n",
    "\n",
    "output = encoder(input_ids, verbose=True)\n",
    "\n",
    "print(f\"\\nFinal output shape: {output.shape}\")\n",
    "print(f\"  - Batch size: {output.shape[0]}\")\n",
    "print(f\"  - Sequence length: {output.shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {output.shape[2]}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"\\nTotal model parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0015-0003-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the architecture as a shape diagram\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Complete Transformer Encoder Architecture\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "  Token IDs: ({batch_size}, {seq_len})\n",
    "      |\n",
    "      v\n",
    "  [Token Embedding]  (vocab_size={vocab_size}, d_model={d_model})\n",
    "      |  -> ({batch_size}, {seq_len}, {d_model})\n",
    "      v\n",
    "  [* sqrt(d_model)]  (scaling factor = {math.sqrt(d_model):.2f})\n",
    "      |\n",
    "      v\n",
    "  [+ Positional Encoding]  (sinusoidal, max_len={max_seq_len})\n",
    "      |  -> ({batch_size}, {seq_len}, {d_model})\n",
    "      v\"\"\")\n",
    "\n",
    "for i in range(n_layers):\n",
    "    print(f\"  [Encoder Block {i}]\")\n",
    "    print(f\"      |-- Multi-Head Attn ({n_heads} heads, d_k={d_model // n_heads})\")\n",
    "    print(f\"      |-- Add & LayerNorm\")\n",
    "    print(f\"      |-- FFN ({d_model} -> {d_ff} -> {d_model})\")\n",
    "    print(f\"      |-- Add & LayerNorm\")\n",
    "    print(f\"      |  -> ({batch_size}, {seq_len}, {d_model})\")\n",
    "    print(f\"      v\")\n",
    "\n",
    "print(f\"  [Final LayerNorm]\")\n",
    "print(f\"      |  -> ({batch_size}, {seq_len}, {d_model})\")\n",
    "print(f\"      v\")\n",
    "print(f\"  Output: ({batch_size}, {seq_len}, {d_model})\")\n",
    "print(f\"\\n  Each position now contains a context-aware representation\")\n",
    "print(f\"  that can be used for downstream tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0016-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Summary & References\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Positional Encoding** injects sequence-order information that attention alone cannot capture. The sinusoidal approach uses fixed sin/cos functions at different frequencies; the learned approach uses a trainable embedding table.\n",
    "\n",
    "2. **Layer Normalization** normalizes across the feature dimension for each token independently, stabilizing training without depending on batch statistics.\n",
    "\n",
    "3. **Residual Connections** (`x + Sublayer(x)`) provide gradient highways that enable training of very deep networks.\n",
    "\n",
    "4. **The Feed-Forward Network** is a two-layer MLP (with ReLU and a 4x expansion) applied position-wise, providing non-linear transformation capacity.\n",
    "\n",
    "5. **The Encoder Block** combines self-attention and FFN, each wrapped with residual connections and layer norm. Multiple blocks are stacked to form deeper models.\n",
    "\n",
    "6. **The Decoder Block** adds masked (causal) self-attention and cross-attention to the encoder block, enabling autoregressive generation conditioned on encoder output.\n",
    "\n",
    "7. **Architecture Variants** choose different subsets:\n",
    "   - **BERT** (encoder-only): bidirectional, great for understanding tasks\n",
    "   - **GPT** (decoder-only): causal, great for generation tasks\n",
    "   - **T5** (encoder-decoder): full model, great for seq2seq tasks\n",
    "\n",
    "8. **Pre-training Objectives** define what the model learns:\n",
    "   - MLM (BERT): predict masked tokens\n",
    "   - CLM (GPT): predict next token\n",
    "   - Span corruption (T5): reconstruct masked spans\n",
    "\n",
    "### References\n",
    "\n",
    "- **Paper:** Vaswani et al., [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (2017) -- the original Transformer paper\n",
    "- **Blog:** Jay Alammar, [\"The Illustrated Transformer\"](https://jalammar.github.io/illustrated-transformer/) -- excellent visual walkthrough\n",
    "- **Blog:** Lilian Weng, [\"Attention? Attention!\"](https://lilianweng.github.io/posts/2018-06-24-attention/) -- comprehensive attention survey\n",
    "- **Book:** Sebastian Raschka, [\"Build a Large Language Model (From Scratch)\"](https://www.manning.com/books/build-a-large-language-model-from-scratch) (Ch. 3-4) -- hands-on implementation\n",
    "- **Code:** Harvard NLP, [\"The Annotated Transformer\"](https://nlp.seas.harvard.edu/annotated-transformer/) -- line-by-line code walkthrough of the paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}