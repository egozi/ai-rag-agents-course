{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13: Multi-Agent Systems & Advanced Patterns\n",
    "\n",
    "## Overview\n",
    "\n",
    "Modern AI applications increasingly rely on **multiple agents working together** rather than a single monolithic agent. Just as software engineering evolved from single-process programs to microservices, AI agents are evolving from single-agent systems to coordinated multi-agent architectures.\n",
    "\n",
    "In this module, we go beyond single agents to explore:\n",
    "\n",
    "1. **Multi-Agent Patterns**: Sequential, parallel, hierarchical, and debate architectures\n",
    "2. **ReAct in Depth**: Full Thought-Action-Observation loop implemented from scratch\n",
    "3. **Agent Memory**: Buffer, summary, and entity memory for persistent context\n",
    "4. **Agent Planning**: Decomposing complex goals into executable sub-tasks\n",
    "5. **Self-Reflection**: Agents that critique and improve their own output\n",
    "6. **LangGraph**: Building agent workflows as state machines\n",
    "7. **Debate Systems**: Multiple agents arguing opposing positions\n",
    "\n",
    "**Note:** The `ai_trading_agent.ipynb` notebook in this repository demonstrates a 5-agent sequential chain using LangChain's `SequentialChain`. This module covers more modern and flexible patterns, including how to refactor that approach using LangGraph.\n",
    "\n",
    "### What you'll learn\n",
    "\n",
    "- How to architect multi-agent systems for different use cases\n",
    "- How to implement ReAct reasoning from scratch (no framework)\n",
    "- How agent memory works and why it matters for multi-turn interactions\n",
    "- How planning and self-reflection make agents more capable\n",
    "- How to use LangGraph to build robust agent workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai python-dotenv langchain langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"/home/amir/source/.env\")\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Any, TypedDict, Annotated\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"OpenAI client initialized with model: {MODEL}\")\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function used throughout the notebook\n",
    "def chat(messages, model=MODEL, temperature=0.7, max_tokens=1024):\n",
    "    \"\"\"Simple wrapper around OpenAI chat completion.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Quick test\n",
    "test_response = chat([{\"role\": \"user\", \"content\": \"Say 'Module 13 is ready!' in exactly those words.\"}])\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Agent Patterns\n",
    "\n",
    "There are four fundamental patterns for organizing multiple agents. Each suits different problem types.\n",
    "\n",
    "### Pattern 1: Sequential\n",
    "\n",
    "Agents run in order. The output of one agent becomes the input to the next.\n",
    "\n",
    "```\n",
    "  Input\n",
    "    |\n",
    "    v\n",
    "+----------+     +----------+     +----------+\n",
    "| Agent A  | --> | Agent B  | --> | Agent C  |\n",
    "| Research |     | Writing  |     | Editing  |\n",
    "+----------+     +----------+     +----------+\n",
    "                                       |\n",
    "                                       v\n",
    "                                    Output\n",
    "```\n",
    "\n",
    "**When to use:** Pipeline tasks where each stage refines or transforms the previous output.  \n",
    "**Example:** The trading agent in `ai_trading_agent.ipynb` -- data analyst -> sentiment analyst -> macro analyst -> strategist -> risk manager.\n",
    "\n",
    "### Pattern 2: Parallel\n",
    "\n",
    "Independent agents run simultaneously on the same input. Results are combined.\n",
    "\n",
    "```\n",
    "              Input\n",
    "            /   |   \\\n",
    "           v    v    v\n",
    "       +------+------+------+\n",
    "       |  A   |  B   |  C   |\n",
    "       | Fact | Style| Tone |\n",
    "       |Check |Check |Check |\n",
    "       +------+------+------+\n",
    "           \\    |    /\n",
    "            v   v   v\n",
    "         +-----------+\n",
    "         | Combiner  |\n",
    "         +-----------+\n",
    "              |\n",
    "              v\n",
    "           Output\n",
    "```\n",
    "\n",
    "**When to use:** Tasks that can be broken into independent sub-problems (e.g., multi-criteria evaluation).  \n",
    "**Example:** Code review where separate agents check style, security, and performance.\n",
    "\n",
    "### Pattern 3: Hierarchical\n",
    "\n",
    "A manager agent delegates tasks to worker agents and synthesizes their results.\n",
    "\n",
    "```\n",
    "              +---------+\n",
    "              | Manager |\n",
    "              +---------+\n",
    "             /     |     \\\n",
    "            v      v      v\n",
    "       +------+ +------+ +------+\n",
    "       |Worker| |Worker| |Worker|\n",
    "       |  A   | |  B   | |  C   |\n",
    "       +------+ +------+ +------+\n",
    "            \\      |      /\n",
    "             v     v     v\n",
    "              +---------+\n",
    "              | Manager |\n",
    "              |(synth.) |\n",
    "              +---------+\n",
    "```\n",
    "\n",
    "**When to use:** Complex tasks requiring dynamic delegation and coordination.  \n",
    "**Example:** A project manager agent that assigns research, writing, and review tasks to specialist agents.\n",
    "\n",
    "### Pattern 4: Debate\n",
    "\n",
    "Agents argue opposing positions. A judge agent evaluates and decides.\n",
    "\n",
    "```\n",
    "       +----------+     +----------+\n",
    "       | Pro Agent | <-> | Con Agent|\n",
    "       +----------+     +----------+\n",
    "            |                 |\n",
    "            v                 v\n",
    "       +--------------------------+\n",
    "       |       Judge Agent        |\n",
    "       | (evaluates both sides)   |\n",
    "       +--------------------------+\n",
    "                   |\n",
    "                   v\n",
    "              Final Verdict\n",
    "```\n",
    "\n",
    "**When to use:** Decisions requiring balanced analysis, reducing bias, or adversarial testing.  \n",
    "**Example:** Evaluating whether a business proposal is viable by having pro and con agents debate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Sequential Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequential_agents(topic):\n",
    "    \"\"\"Sequential pattern: researcher -> writer -> editor.\"\"\"\n",
    "    print(f\"=== Sequential Pipeline for: '{topic}' ===\\n\")\n",
    "    \n",
    "    # Agent 1: Researcher\n",
    "    print(\"[Agent 1: Researcher]\")\n",
    "    research = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a research analyst. Provide 3-4 key facts about the given topic. Be concise and factual.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Research this topic: {topic}\"}\n",
    "    ])\n",
    "    print(research)\n",
    "    print()\n",
    "    \n",
    "    # Agent 2: Writer (uses research output)\n",
    "    print(\"[Agent 2: Writer]\")\n",
    "    draft = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a skilled writer. Given research notes, write a short 2-paragraph article. Make it engaging and informative.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write an article based on these research notes:\\n\\n{research}\"}\n",
    "    ])\n",
    "    print(draft)\n",
    "    print()\n",
    "    \n",
    "    # Agent 3: Editor (uses writer output)\n",
    "    print(\"[Agent 3: Editor]\")\n",
    "    final = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a meticulous editor. Review the article for clarity, grammar, and engagement. Provide the improved version.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Edit and improve this article:\\n\\n{draft}\"}\n",
    "    ])\n",
    "    print(final)\n",
    "    \n",
    "    return {\"research\": research, \"draft\": draft, \"final\": final}\n",
    "\n",
    "result = run_sequential_agents(\"The impact of transformers on natural language processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Parallel Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_agents(text):\n",
    "    \"\"\"Parallel pattern: multiple reviewers analyze text independently, then combine.\"\"\"\n",
    "    print(f\"=== Parallel Review Pipeline ===\\n\")\n",
    "    print(f\"Text to review: '{text[:80]}...'\\n\")\n",
    "    \n",
    "    reviewers = {\n",
    "        \"Fact Checker\": \"You are a fact checker. Evaluate the factual accuracy of the given text. List any claims that need verification or correction. Be concise.\",\n",
    "        \"Style Reviewer\": \"You are a writing style expert. Evaluate the clarity, readability, and engagement of the text. Suggest specific improvements. Be concise.\",\n",
    "        \"Tone Analyst\": \"You are a tone and audience analyst. Evaluate whether the tone is appropriate for a general audience. Note any issues. Be concise.\"\n",
    "    }\n",
    "    \n",
    "    # Run all reviewers (in practice these could be concurrent; here sequential for simplicity)\n",
    "    reviews = {}\n",
    "    for name, system_prompt in reviewers.items():\n",
    "        print(f\"[{name}]\")\n",
    "        review = chat([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Review this text:\\n\\n{text}\"}\n",
    "        ], max_tokens=300)\n",
    "        reviews[name] = review\n",
    "        print(review)\n",
    "        print()\n",
    "    \n",
    "    # Combiner agent synthesizes all reviews\n",
    "    print(\"[Combiner Agent]\")\n",
    "    combined_reviews = \"\\n\\n\".join([f\"### {name}:\\n{review}\" for name, review in reviews.items()])\n",
    "    synthesis = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are an editorial coordinator. Given multiple reviewer assessments, synthesize them into a single prioritized list of 3-5 action items for the author.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Here are the reviews:\\n\\n{combined_reviews}\"}\n",
    "    ], max_tokens=400)\n",
    "    print(synthesis)\n",
    "    \n",
    "    return {\"reviews\": reviews, \"synthesis\": synthesis}\n",
    "\n",
    "sample_text = \"\"\"Quantum computing leverages quantum mechanical phenomena like superposition \n",
    "and entanglement to process information in fundamentally new ways. Unlike classical \n",
    "computers that use bits (0 or 1), quantum computers use qubits that can exist in \n",
    "multiple states simultaneously. This allows them to solve certain problems exponentially \n",
    "faster than any classical computer. Google achieved quantum supremacy in 2019 when their \n",
    "Sycamore processor performed a calculation in 200 seconds that would take the world's \n",
    "fastest supercomputer 10,000 years.\"\"\"\n",
    "\n",
    "result = run_parallel_agents(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReAct in Depth\n",
    "\n",
    "ReAct (Reasoning + Acting) is a foundational agent pattern where the LLM alternates between:\n",
    "\n",
    "1. **Thought**: The agent reasons about what to do next\n",
    "2. **Action**: The agent selects a tool and provides input\n",
    "3. **Observation**: The tool returns a result\n",
    "4. **Repeat** until the agent has a final answer\n",
    "\n",
    "```\n",
    "User Question\n",
    "     |\n",
    "     v\n",
    "+--------------------+\n",
    "| Thought: I need to |<---+\n",
    "| search for X...    |    |\n",
    "+--------------------+    |\n",
    "     |                    |\n",
    "     v                    |\n",
    "+--------------------+    |\n",
    "| Action: search     |    |\n",
    "| Action Input: X    |    |\n",
    "+--------------------+    |\n",
    "     |                    |\n",
    "     v                    |\n",
    "+--------------------+    |\n",
    "| Observation: ...   |----+\n",
    "| (tool result)      |    |\n",
    "+--------------------+    |\n",
    "     |                    |\n",
    "     v                    |\n",
    "  Has enough    No -------+\n",
    "  info?\n",
    "     | Yes\n",
    "     v\n",
    "Final Answer\n",
    "```\n",
    "\n",
    "Let's implement a full ReAct agent **from scratch** -- no framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple tools for our ReAct agent\n",
    "\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        # Only allow safe math operations\n",
    "        allowed_names = {\"abs\": abs, \"round\": round, \"min\": min, \"max\": max, \"pow\": pow}\n",
    "        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def lookup(term: str) -> str:\n",
    "    \"\"\"Look up a fact from a small knowledge base.\"\"\"\n",
    "    knowledge = {\n",
    "        \"python\": \"Python is a programming language created by Guido van Rossum, first released in 1991. It emphasizes readability and simplicity.\",\n",
    "        \"transformer\": \"The Transformer architecture was introduced in the 2017 paper 'Attention Is All You Need' by Vaswani et al. It uses self-attention mechanisms.\",\n",
    "        \"gpt\": \"GPT (Generative Pre-trained Transformer) is a family of language models by OpenAI. GPT-3 has 175 billion parameters.\",\n",
    "        \"bert\": \"BERT (Bidirectional Encoder Representations from Transformers) was released by Google in 2018. It uses masked language modeling for pre-training.\",\n",
    "        \"attention\": \"Attention mechanism allows models to focus on relevant parts of the input. Self-attention computes relationships between all positions in a sequence.\",\n",
    "        \"rag\": \"RAG (Retrieval-Augmented Generation) combines a retriever that finds relevant documents with a generator that produces answers based on those documents.\",\n",
    "        \"langchain\": \"LangChain is a framework for developing applications powered by language models. It provides tools for chains, agents, and memory.\",\n",
    "        \"embedding\": \"An embedding is a dense vector representation of data (text, images) in a continuous vector space where similar items are close together.\"\n",
    "    }\n",
    "    term_lower = term.lower().strip()\n",
    "    for key, value in knowledge.items():\n",
    "        if key in term_lower or term_lower in key:\n",
    "            return value\n",
    "    return f\"No information found for '{term}'. Available topics: {', '.join(knowledge.keys())}\"\n",
    "\n",
    "def string_length(text: str) -> str:\n",
    "    \"\"\"Count the number of characters in a string.\"\"\"\n",
    "    return str(len(text))\n",
    "\n",
    "# Tool registry\n",
    "TOOLS = {\n",
    "    \"calculator\": {\"fn\": calculator, \"desc\": \"Evaluate a math expression. Input: a mathematical expression like '2 + 3 * 4'\"},\n",
    "    \"lookup\": {\"fn\": lookup, \"desc\": \"Look up information about a topic. Input: a topic name like 'python' or 'transformer'\"},\n",
    "    \"string_length\": {\"fn\": string_length, \"desc\": \"Count characters in a string. Input: any text string\"}\n",
    "}\n",
    "\n",
    "print(\"Tools registered:\")\n",
    "for name, tool in TOOLS.items():\n",
    "    print(f\"  - {name}: {tool['desc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct agent implementation from scratch\n",
    "\n",
    "REACT_SYSTEM_PROMPT = \"\"\"You are a helpful assistant that answers questions by reasoning step-by-step and using tools.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tool_descriptions}\n",
    "\n",
    "To use a tool, respond with EXACTLY this format:\n",
    "Thought: <your reasoning about what to do next>\n",
    "Action: <tool_name>\n",
    "Action Input: <input to the tool>\n",
    "\n",
    "After receiving an observation (tool result), continue reasoning.\n",
    "\n",
    "When you have enough information to answer, respond with:\n",
    "Thought: I now have enough information to answer.\n",
    "Final Answer: <your complete answer>\n",
    "\n",
    "IMPORTANT:\n",
    "- Always start with a Thought\n",
    "- Use exactly one tool per step\n",
    "- Do not make up information; use tools to find facts\n",
    "- Always end with 'Final Answer:' when done\n",
    "\"\"\"\n",
    "\n",
    "def parse_react_output(text):\n",
    "    \"\"\"Parse the LLM output for Thought, Action, Action Input, or Final Answer.\"\"\"\n",
    "    result = {\"thought\": None, \"action\": None, \"action_input\": None, \"final_answer\": None}\n",
    "    \n",
    "    # Extract thought\n",
    "    thought_match = re.search(r\"Thought:\\s*(.+?)(?=\\nAction:|\\nFinal Answer:|$)\", text, re.DOTALL)\n",
    "    if thought_match:\n",
    "        result[\"thought\"] = thought_match.group(1).strip()\n",
    "    \n",
    "    # Check for final answer\n",
    "    final_match = re.search(r\"Final Answer:\\s*(.+)\", text, re.DOTALL)\n",
    "    if final_match:\n",
    "        result[\"final_answer\"] = final_match.group(1).strip()\n",
    "        return result\n",
    "    \n",
    "    # Extract action and action input\n",
    "    action_match = re.search(r\"Action:\\s*(.+)\", text)\n",
    "    if action_match:\n",
    "        result[\"action\"] = action_match.group(1).strip()\n",
    "    \n",
    "    input_match = re.search(r\"Action Input:\\s*(.+)\", text)\n",
    "    if input_match:\n",
    "        result[\"action_input\"] = input_match.group(1).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def react_agent(question, max_steps=5, verbose=True):\n",
    "    \"\"\"Run a ReAct agent loop from scratch.\"\"\"\n",
    "    # Build tool descriptions for the system prompt\n",
    "    tool_desc = \"\\n\".join([f\"- {name}: {info['desc']}\" for name, info in TOOLS.items()])\n",
    "    system_prompt = REACT_SYSTEM_PROMPT.format(tool_descriptions=tool_desc)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Get LLM response\n",
    "        response = chat(messages, temperature=0.0, max_tokens=500)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Step {step + 1} ---\")\n",
    "            print(response)\n",
    "        \n",
    "        # Parse the response\n",
    "        parsed = parse_react_output(response)\n",
    "        \n",
    "        # Check if we have a final answer\n",
    "        if parsed[\"final_answer\"]:\n",
    "            if verbose:\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(f\"FINAL ANSWER: {parsed['final_answer']}\")\n",
    "            return parsed[\"final_answer\"]\n",
    "        \n",
    "        # Execute the action\n",
    "        if parsed[\"action\"] and parsed[\"action\"] in TOOLS:\n",
    "            tool_fn = TOOLS[parsed[\"action\"]][\"fn\"]\n",
    "            observation = tool_fn(parsed[\"action_input\"] or \"\")\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nObservation: {observation}\")\n",
    "            \n",
    "            # Add the assistant response and observation to messages\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Observation: {observation}\"})\n",
    "        elif parsed[\"action\"]:\n",
    "            # Unknown tool\n",
    "            error_msg = f\"Unknown tool '{parsed['action']}'. Available tools: {', '.join(TOOLS.keys())}\"\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Observation: {error_msg}\"})\n",
    "            if verbose:\n",
    "                print(f\"\\nObservation: {error_msg}\")\n",
    "        else:\n",
    "            # Could not parse action -- ask the model to try again\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Please respond using the exact format: Thought/Action/Action Input or Thought/Final Answer.\"})\n",
    "    \n",
    "    return \"Max steps reached without a final answer.\"\n",
    "\n",
    "print(\"ReAct agent ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ReAct agent with a multi-step question\n",
    "answer = react_agent(\n",
    "    \"What year was the Transformer architecture introduced, and what is 2024 minus that year?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example: multi-tool usage\n",
    "answer = react_agent(\n",
    "    \"Look up what GPT is, then calculate how many characters are in the description you find.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Memory\n",
    "\n",
    "Without memory, every agent interaction starts from scratch. Memory gives agents the ability to:\n",
    "\n",
    "- **Recall** previous conversation context\n",
    "- **Track** entities and facts mentioned over time\n",
    "- **Maintain** coherent multi-turn conversations\n",
    "\n",
    "### Types of Agent Memory\n",
    "\n",
    "| Memory Type | How it Works | Pros | Cons |\n",
    "|-------------|-------------|------|------|\n",
    "| **Buffer** | Store all messages verbatim | Full context preserved | Token limit grows linearly |\n",
    "| **Summary** | Periodically summarize old messages | Bounded token usage | Lossy -- details may be lost |\n",
    "| **Entity** | Extract and track entities | Structured knowledge | Requires entity extraction |\n",
    "\n",
    "```\n",
    "Buffer Memory:       [msg1, msg2, msg3, msg4, msg5, ...]\n",
    "                     (keeps everything -- tokens grow fast)\n",
    "\n",
    "Summary Memory:      [summary_of_msg1-3, msg4, msg5, ...]\n",
    "                     (compresses old messages -- bounded tokens)\n",
    "\n",
    "Entity Memory:       {\"Alice\": \"software engineer, lives in SF\",\n",
    "                      \"Project X\": \"ML pipeline, deadline March\"}\n",
    "                     (structured entity tracking)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: Buffer Memory\n",
    "\n",
    "class BufferMemory:\n",
    "    \"\"\"Simple buffer memory that stores all conversation messages.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant.\"):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    def add_user_message(self, content):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    def add_assistant_message(self, content):\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "    \n",
    "    def get_messages(self):\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def token_estimate(self):\n",
    "        \"\"\"Rough estimate of token count (4 chars per token heuristic).\"\"\"\n",
    "        total_chars = sum(len(m[\"content\"]) for m in self.messages)\n",
    "        return total_chars // 4\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BufferMemory(messages={len(self.messages)}, ~{self.token_estimate()} tokens)\"\n",
    "\n",
    "\n",
    "# Implementation: Summary Memory\n",
    "\n",
    "class SummaryMemory:\n",
    "    \"\"\"Memory that summarizes old messages to stay within token limits.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant.\", max_messages=6):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.summary = \"\"\n",
    "        self.recent_messages = []\n",
    "        self.max_messages = max_messages  # keep this many recent messages before summarizing\n",
    "    \n",
    "    def add_user_message(self, content):\n",
    "        self.recent_messages.append({\"role\": \"user\", \"content\": content})\n",
    "        self._maybe_summarize()\n",
    "    \n",
    "    def add_assistant_message(self, content):\n",
    "        self.recent_messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        self._maybe_summarize()\n",
    "    \n",
    "    def _maybe_summarize(self):\n",
    "        \"\"\"If we have too many messages, summarize the older ones.\"\"\"\n",
    "        if len(self.recent_messages) > self.max_messages:\n",
    "            # Take the older half of messages and summarize\n",
    "            to_summarize = self.recent_messages[:self.max_messages // 2]\n",
    "            self.recent_messages = self.recent_messages[self.max_messages // 2:]\n",
    "            \n",
    "            # Build the text to summarize\n",
    "            convo_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in to_summarize])\n",
    "            prev_summary = f\"Previous summary: {self.summary}\\n\\n\" if self.summary else \"\"\n",
    "            \n",
    "            self.summary = chat([\n",
    "                {\"role\": \"system\", \"content\": \"Summarize this conversation concisely, preserving key facts, names, and decisions. Output only the summary.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prev_summary}New messages to incorporate:\\n{convo_text}\"}\n",
    "            ], max_tokens=200)\n",
    "    \n",
    "    def get_messages(self):\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        if self.summary:\n",
    "            messages.append({\"role\": \"system\", \"content\": f\"Summary of earlier conversation: {self.summary}\"})\n",
    "        messages.extend(self.recent_messages)\n",
    "        return messages\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SummaryMemory(recent={len(self.recent_messages)}, has_summary={bool(self.summary)})\"\n",
    "\n",
    "\n",
    "print(\"Memory classes defined!\")\n",
    "print(f\"BufferMemory: stores all messages verbatim\")\n",
    "print(f\"SummaryMemory: compresses old messages into summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate buffer memory across multiple turns\n",
    "\n",
    "def chat_with_memory(memory, user_message):\n",
    "    \"\"\"Send a message using the given memory, store the exchange.\"\"\"\n",
    "    memory.add_user_message(user_message)\n",
    "    response = chat(memory.get_messages(), max_tokens=300)\n",
    "    memory.add_assistant_message(response)\n",
    "    return response\n",
    "\n",
    "# Demo: multi-turn conversation with buffer memory\n",
    "print(\"=== Buffer Memory Demo ===\")\n",
    "print()\n",
    "\n",
    "buf_mem = BufferMemory(system_prompt=\"You are a helpful AI tutor. Remember everything the student tells you.\")\n",
    "\n",
    "turns = [\n",
    "    \"Hi! My name is Alex and I'm learning about machine learning.\",\n",
    "    \"I'm particularly interested in NLP. What should I learn first?\",\n",
    "    \"I already know Python and basic linear algebra. Does that help?\",\n",
    "    \"What's my name and what am I interested in?\"  # Tests memory!\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(turns, 1):\n",
    "    print(f\"--- Turn {i} ---\")\n",
    "    print(f\"User: {msg}\")\n",
    "    response = chat_with_memory(buf_mem, msg)\n",
    "    print(f\"Assistant: {response}\")\n",
    "    print(f\"[Memory state: {buf_mem}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Agent with Conversation Memory\n",
    "\n",
    "Build a `MemoryAgent` class that:\n",
    "1. Uses buffer memory to remember all previous exchanges\n",
    "2. Has a `chat(message)` method that returns the assistant's response\n",
    "3. Has a `memory_stats()` method that returns conversation statistics\n",
    "4. Can answer questions about earlier parts of the conversation\n",
    "\n",
    "**Requirements:**\n",
    "- The agent should have a configurable system prompt\n",
    "- It should track the number of turns\n",
    "- The `memory_stats()` method should return a dict with `turns`, `total_messages`, and `estimated_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a MemoryAgent\n",
    "\n",
    "class MemoryAgent:\n",
    "    \"\"\"An agent with conversation memory across interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant with perfect memory.\"):\n",
    "        # TODO: Initialize the agent with a system prompt and buffer memory\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = None  # TODO: initialize message list with system prompt\n",
    "        self.turns = None  # TODO: initialize turn counter\n",
    "    \n",
    "    def chat(self, message):\n",
    "        \"\"\"\n",
    "        Send a message to the agent and get a response.\n",
    "        The agent remembers all previous exchanges.\n",
    "        \n",
    "        Args:\n",
    "            message: The user's message\n",
    "        Returns:\n",
    "            The assistant's response string\n",
    "        \"\"\"\n",
    "        # TODO: Add the user message to memory\n",
    "        # TODO: Call the LLM with the full message history\n",
    "        # TODO: Add the assistant response to memory\n",
    "        # TODO: Increment turn counter\n",
    "        # TODO: Return the response\n",
    "        return None\n",
    "    \n",
    "    def memory_stats(self):\n",
    "        \"\"\"\n",
    "        Return statistics about the conversation memory.\n",
    "        \n",
    "        Returns:\n",
    "            dict with keys: 'turns', 'total_messages', 'estimated_tokens'\n",
    "        \"\"\"\n",
    "        # TODO: Calculate and return memory statistics\n",
    "        return None\n",
    "\n",
    "# Test (will fail until you complete the TODOs)\n",
    "# agent = MemoryAgent(system_prompt=\"You are a friendly AI tutor.\")\n",
    "# print(agent.chat(\"My name is Jordan. I'm studying transformers.\"))\n",
    "# print(agent.chat(\"What's my name?\"))\n",
    "# print(agent.memory_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: MemoryAgent with full conversation memory\n",
    "\n",
    "class MemoryAgent:\n",
    "    \"\"\"An agent with conversation memory across interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant with perfect memory.\"):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        self.turns = 0\n",
    "    \n",
    "    def chat(self, message):\n",
    "        \"\"\"Send a message and get a response. Memory persists across calls.\"\"\"\n",
    "        # Add user message to memory\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Call the LLM with full history\n",
    "        response = chat(self.messages, max_tokens=400)\n",
    "        \n",
    "        # Store response in memory\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # Increment turn counter\n",
    "        self.turns += 1\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def memory_stats(self):\n",
    "        \"\"\"Return conversation memory statistics.\"\"\"\n",
    "        total_chars = sum(len(m[\"content\"]) for m in self.messages)\n",
    "        return {\n",
    "            \"turns\": self.turns,\n",
    "            \"total_messages\": len(self.messages),\n",
    "            \"estimated_tokens\": total_chars // 4\n",
    "        }\n",
    "\n",
    "\n",
    "# 5-turn demo showing memory retention\n",
    "print(\"=== MemoryAgent 5-Turn Demo ===\")\n",
    "print()\n",
    "\n",
    "agent = MemoryAgent(system_prompt=\"You are a friendly AI tutor who remembers everything about the student.\")\n",
    "\n",
    "conversation = [\n",
    "    \"Hi! My name is Jordan and I'm a data scientist at TechCorp.\",\n",
    "    \"I want to learn about multi-agent systems. I already know LangChain basics.\",\n",
    "    \"My favorite part of AI is the reasoning capabilities of LLMs.\",\n",
    "    \"Can you suggest a project that combines what I know with multi-agent systems?\",\n",
    "    \"Quick quiz: what's my name, where do I work, and what's my favorite part of AI?\"\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(conversation, 1):\n",
    "    print(f\"--- Turn {i} ---\")\n",
    "    print(f\"User: {msg}\")\n",
    "    response = agent.chat(msg)\n",
    "    print(f\"Agent: {response}\")\n",
    "    stats = agent.memory_stats()\n",
    "    print(f\"[Stats: {stats}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agent Planning\n",
    "\n",
    "Complex tasks require **planning** -- breaking a big goal into smaller, manageable sub-tasks.\n",
    "\n",
    "The planning pattern:\n",
    "\n",
    "```\n",
    "Complex Goal\n",
    "     |\n",
    "     v\n",
    "+-------------------+\n",
    "| Planner Agent     |\n",
    "| (decompose goal)  |\n",
    "+-------------------+\n",
    "     |\n",
    "     v\n",
    "Step 1 -> Execute -> Result 1\n",
    "Step 2 -> Execute -> Result 2\n",
    "Step 3 -> Execute -> Result 3\n",
    "     |\n",
    "     v\n",
    "+-------------------+\n",
    "| Synthesizer Agent |\n",
    "| (combine results) |\n",
    "+-------------------+\n",
    "     |\n",
    "     v\n",
    "Final Output\n",
    "```\n",
    "\n",
    "This is powerful because:\n",
    "- Each sub-task is simpler and more likely to succeed\n",
    "- You can retry individual steps without redoing everything\n",
    "- The plan itself can be inspected and modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning agent implementation\n",
    "\n",
    "def planning_agent(goal, verbose=True):\n",
    "    \"\"\"\n",
    "    Agent that decomposes a complex goal into sub-tasks, executes each, \n",
    "    and synthesizes the results.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"=== Planning Agent ===\")\n",
    "        print(f\"Goal: {goal}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Plan -- decompose the goal into sub-tasks\n",
    "    if verbose:\n",
    "        print(\"\\n[Phase 1: PLANNING]\")\n",
    "    \n",
    "    plan_response = chat([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a planning agent. Given a complex goal, break it down into 3-5 concrete sub-tasks.\n",
    "Output your plan as a JSON array of strings, each being one sub-task.\n",
    "Example: [\"Research topic X\", \"Outline key points\", \"Write first draft\", \"Review and edit\"]\n",
    "Output ONLY the JSON array, nothing else.\"\"\" },\n",
    "        {\"role\": \"user\", \"content\": f\"Break this goal into sub-tasks: {goal}\"}\n",
    "    ], temperature=0.3, max_tokens=300)\n",
    "    \n",
    "    # Parse the plan\n",
    "    try:\n",
    "        # Find JSON array in response\n",
    "        json_match = re.search(r'\\[.*\\]', plan_response, re.DOTALL)\n",
    "        if json_match:\n",
    "            steps = json.loads(json_match.group())\n",
    "        else:\n",
    "            steps = json.loads(plan_response)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: split by newlines\n",
    "        steps = [line.strip().lstrip('0123456789.-) ') for line in plan_response.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Plan ({len(steps)} steps):\")\n",
    "        for i, step in enumerate(steps, 1):\n",
    "            print(f\"  {i}. {step}\")\n",
    "    \n",
    "    # Step 2: Execute each sub-task\n",
    "    if verbose:\n",
    "        print(\"\\n[Phase 2: EXECUTION]\")\n",
    "    \n",
    "    results = []\n",
    "    for i, step in enumerate(steps, 1):\n",
    "        if verbose:\n",
    "            print(f\"\\n  Executing step {i}: {step}\")\n",
    "        \n",
    "        # Context from previous steps\n",
    "        context = \"\"\n",
    "        if results:\n",
    "            context = \"Previous step results:\\n\" + \"\\n\".join(\n",
    "                [f\"Step {j+1}: {r[:200]}...\" if len(r) > 200 else f\"Step {j+1}: {r}\" \n",
    "                 for j, r in enumerate(results)]\n",
    "            ) + \"\\n\\n\"\n",
    "        \n",
    "        step_result = chat([\n",
    "            {\"role\": \"system\", \"content\": f\"You are executing one step of a larger plan. The overall goal is: {goal}. Complete only the current step thoroughly and concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{context}Current step to execute: {step}\"}\n",
    "        ], max_tokens=500)\n",
    "        \n",
    "        results.append(step_result)\n",
    "        if verbose:\n",
    "            print(f\"  Result: {step_result[:200]}...\" if len(step_result) > 200 else f\"  Result: {step_result}\")\n",
    "    \n",
    "    # Step 3: Synthesize all results\n",
    "    if verbose:\n",
    "        print(\"\\n[Phase 3: SYNTHESIS]\")\n",
    "    \n",
    "    all_results = \"\\n\\n\".join([f\"### Step {i+1}: {steps[i]}\\n{result}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    final = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a synthesis agent. Combine the results from multiple completed sub-tasks into a coherent, well-organized final output. Maintain quality and flow.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Goal: {goal}\\n\\nCompleted sub-task results:\\n\\n{all_results}\\n\\nSynthesize these into a final, polished output.\"}\n",
    "    ], max_tokens=800)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFinal Output:\\n{final}\")\n",
    "    \n",
    "    return {\"plan\": steps, \"step_results\": results, \"final\": final}\n",
    "\n",
    "print(\"Planning agent ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: planning agent in action\n",
    "result = planning_agent(\"Explain the differences between CNNs and Transformers for a beginner audience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Planning Agent for Research Reports\n",
    "\n",
    "Build a planning agent that decomposes the task of \"writing a research report\" into sub-tasks, executes each sub-task, and combines the results into a final report.\n",
    "\n",
    "**Requirements:**\n",
    "- `research_report_agent(topic)` function\n",
    "- Must generate a plan with at least 4 steps\n",
    "- Each step should be executed with context from previous steps\n",
    "- Final output should be a coherent report\n",
    "- Print each phase (planning, execution, synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Planning agent for research reports\n",
    "\n",
    "def research_report_agent(topic):\n",
    "    \"\"\"\n",
    "    Agent that plans, researches, and writes a short report on the given topic.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic to write about\n",
    "    Returns:\n",
    "        dict with 'plan', 'step_results', and 'report' keys\n",
    "    \"\"\"\n",
    "    print(f\"=== Research Report Agent ===\")\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Phase 1: Create a research plan\n",
    "    # TODO: Use the LLM to generate a plan with at least 4 steps\n",
    "    # The plan should cover: background research, key concepts,\n",
    "    # analysis/comparison, and writing/structuring\n",
    "    plan = None  # Should be a list of step strings\n",
    "    \n",
    "    # Phase 2: Execute each step\n",
    "    # TODO: Loop through the plan, execute each step with context\n",
    "    # from previous steps, collect results\n",
    "    step_results = None  # Should be a list of result strings\n",
    "    \n",
    "    # Phase 3: Synthesize into final report\n",
    "    # TODO: Combine all step results into a coherent report\n",
    "    report = None\n",
    "    \n",
    "    return {\"plan\": plan, \"step_results\": step_results, \"report\": report}\n",
    "\n",
    "# Test (will fail until you complete the TODOs)\n",
    "# result = research_report_agent(\"The evolution of attention mechanisms in deep learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Planning agent for research reports\n",
    "\n",
    "def research_report_agent(topic):\n",
    "    \"\"\"\n",
    "    Agent that plans, researches, and writes a short report on the given topic.\n",
    "    \"\"\"\n",
    "    print(f\"=== Research Report Agent ===\")\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Phase 1: Create a research plan\n",
    "    print(\"\\n[Phase 1: PLANNING]\")\n",
    "    plan_response = chat([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a research planner. Given a topic, create a structured plan for writing a short research report.\n",
    "The plan must have exactly 5 steps covering:\n",
    "1. Background and historical context\n",
    "2. Key concepts and terminology\n",
    "3. Current state of the art\n",
    "4. Challenges and open problems\n",
    "5. Future directions and conclusion\n",
    "\n",
    "Output ONLY a JSON array of 5 strings, each describing one research step.\n",
    "Example: [\"Research the historical background of X\", \"Define key concepts\", ...]\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Create a research plan for: {topic}\"}\n",
    "    ], temperature=0.3, max_tokens=400)\n",
    "    \n",
    "    try:\n",
    "        json_match = re.search(r'\\[.*\\]', plan_response, re.DOTALL)\n",
    "        if json_match:\n",
    "            plan = json.loads(json_match.group())\n",
    "        else:\n",
    "            plan = json.loads(plan_response)\n",
    "    except json.JSONDecodeError:\n",
    "        plan = [line.strip().lstrip('0123456789.-) ') for line in plan_response.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    print(f\"Plan ({len(plan)} steps):\")\n",
    "    for i, step in enumerate(plan, 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "    \n",
    "    # Phase 2: Execute each step with cumulative context\n",
    "    print(\"\\n[Phase 2: EXECUTION]\")\n",
    "    step_results = []\n",
    "    \n",
    "    for i, step in enumerate(plan):\n",
    "        print(f\"\\n  Executing step {i+1}/{len(plan)}: {step}\")\n",
    "        \n",
    "        # Build context from previous results\n",
    "        context = \"\"\n",
    "        if step_results:\n",
    "            context = \"Context from completed research steps:\\n\"\n",
    "            for j, prev_result in enumerate(step_results):\n",
    "                truncated = prev_result[:300] + \"...\" if len(prev_result) > 300 else prev_result\n",
    "                context += f\"\\nStep {j+1} result: {truncated}\\n\"\n",
    "            context += \"\\n\"\n",
    "        \n",
    "        result = chat([\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"You are a research writer working on a report about: {topic}.\n",
    "Complete only the current step. Write 2-3 detailed paragraphs.\n",
    "Use an academic but accessible tone.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{context}Current step: {step}\"}\n",
    "        ], max_tokens=600)\n",
    "        \n",
    "        step_results.append(result)\n",
    "        preview = result[:150] + \"...\" if len(result) > 150 else result\n",
    "        print(f\"  Done. Preview: {preview}\")\n",
    "    \n",
    "    # Phase 3: Synthesize into a cohesive report\n",
    "    print(\"\\n[Phase 3: SYNTHESIS]\")\n",
    "    \n",
    "    all_sections = \"\\n\\n\".join([\n",
    "        f\"### Section {i+1}: {plan[i]}\\n{result}\" \n",
    "        for i, result in enumerate(step_results)\n",
    "    ])\n",
    "    \n",
    "    report = chat([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a senior editor. Combine the following research sections into a polished, \n",
    "cohesive report. Add smooth transitions between sections, a brief introduction, and a conclusion.\n",
    "Use markdown formatting with headers. Keep the total length reasonable (800-1200 words).\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Topic: {topic}\\n\\nResearch sections to combine:\\n\\n{all_sections}\"}\n",
    "    ], max_tokens=2000)\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FINAL REPORT:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(report)\n",
    "    \n",
    "    return {\"plan\": plan, \"step_results\": step_results, \"report\": report}\n",
    "\n",
    "# Run the research report agent\n",
    "result = research_report_agent(\"The evolution of attention mechanisms in deep learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Self-Reflection\n",
    "\n",
    "One of the most powerful patterns in agentic AI is **self-reflection** -- agents that critique and improve their own output.\n",
    "\n",
    "The pattern is simple but effective:\n",
    "\n",
    "```\n",
    "Prompt\n",
    "  |\n",
    "  v\n",
    "+----------+     +----------+     +----------+\n",
    "| Generate | --> | Critique | --> | Revise   |\n",
    "| (draft)  |     | (review) |     | (improve)|\n",
    "+----------+     +----------+     +----------+\n",
    "                      |                 |\n",
    "                      +---- repeat -----+\n",
    "```\n",
    "\n",
    "This mimics how humans write: draft, review, revise. Each iteration typically improves quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_reflection_agent(task, iterations=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Agent that generates output, critiques it, and iteratively improves it.\n",
    "    \n",
    "    Args:\n",
    "        task: The task to complete\n",
    "        iterations: Number of generate-critique-revise cycles\n",
    "        verbose: Whether to print intermediate steps\n",
    "    Returns:\n",
    "        dict with 'drafts', 'critiques', and 'final' keys\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"=== Self-Reflection Agent ({iterations} iterations) ===\")\n",
    "        print(f\"Task: {task}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    drafts = []\n",
    "    critiques = []\n",
    "    \n",
    "    # Initial generation\n",
    "    if verbose:\n",
    "        print(\"\\n[Iteration 1: Initial Draft]\")\n",
    "    \n",
    "    current_draft = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a skilled writer. Complete the task to the best of your ability.\"},\n",
    "        {\"role\": \"user\", \"content\": task}\n",
    "    ], max_tokens=600)\n",
    "    \n",
    "    drafts.append(current_draft)\n",
    "    if verbose:\n",
    "        print(f\"Draft 1:\\n{current_draft}\\n\")\n",
    "    \n",
    "    # Iterative critique and revision\n",
    "    for i in range(1, iterations):\n",
    "        # Critique\n",
    "        if verbose:\n",
    "            print(f\"[Iteration {i+1}: Critique]\")\n",
    "        \n",
    "        critique = chat([\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You are a critical reviewer. Analyze the given text and provide:\n",
    "1. Three specific strengths\n",
    "2. Three specific weaknesses or areas for improvement\n",
    "3. A concrete suggestion for each weakness\n",
    "Be constructive and specific.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Original task: {task}\\n\\nText to critique:\\n{current_draft}\"}\n",
    "        ], max_tokens=500)\n",
    "        \n",
    "        critiques.append(critique)\n",
    "        if verbose:\n",
    "            print(f\"Critique:\\n{critique}\\n\")\n",
    "        \n",
    "        # Revise\n",
    "        if verbose:\n",
    "            print(f\"[Iteration {i+1}: Revision]\")\n",
    "        \n",
    "        current_draft = chat([\n",
    "            {\"role\": \"system\", \"content\": \"You are a skilled writer revising your work based on feedback. Address all the critique points while maintaining what works well.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Original task: {task}\\n\\nYour previous draft:\\n{current_draft}\\n\\nCritique to address:\\n{critique}\\n\\nPlease write an improved version.\"}\n",
    "        ], max_tokens=700)\n",
    "        \n",
    "        drafts.append(current_draft)\n",
    "        if verbose:\n",
    "            print(f\"Revised Draft {i+1}:\\n{current_draft}\\n\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Completed {iterations} iterations of self-reflection.\")\n",
    "        print(f\"Drafts: {len(drafts)}, Critiques: {len(critiques)}\")\n",
    "    \n",
    "    return {\"drafts\": drafts, \"critiques\": critiques, \"final\": current_draft}\n",
    "\n",
    "print(\"Self-reflection agent ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: watch the agent improve its output over iterations\n",
    "result = self_reflection_agent(\n",
    "    \"Write a concise explanation of how attention mechanisms work in transformers, suitable for someone who knows basic neural networks but not transformers.\",\n",
    "    iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare first draft vs final draft\n",
    "print(\"=== FIRST DRAFT ===\")\n",
    "print(result[\"drafts\"][0])\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"=== FINAL DRAFT (after self-reflection) ===\")\n",
    "print(result[\"final\"])\n",
    "print()\n",
    "print(f\"First draft length: {len(result['drafts'][0])} chars\")\n",
    "print(f\"Final draft length: {len(result['final'])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LangGraph Introduction\n",
    "\n",
    "LangGraph represents agent workflows as **state machines** -- directed graphs where:\n",
    "\n",
    "- **Nodes** are functions (agents, tools, or processing steps)\n",
    "- **Edges** define transitions between nodes\n",
    "- **State** is a shared data structure passed between nodes\n",
    "- **Conditional edges** allow dynamic routing based on state\n",
    "\n",
    "```\n",
    "Traditional Chain:    A -> B -> C  (linear, rigid)\n",
    "\n",
    "LangGraph:            A -> B -> C  (if state.ok)\n",
    "                           |-> D  (if state.needs_review)\n",
    "                           |-> A  (if state.retry)\n",
    "```\n",
    "\n",
    "This is much more flexible than LangChain's `SequentialChain` because:\n",
    "1. Nodes can loop (retry failed steps)\n",
    "2. Routing can depend on runtime state\n",
    "3. The workflow structure is explicit and inspectable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Define the state that will be passed between nodes\n",
    "class ArticleState(TypedDict):\n",
    "    topic: str\n",
    "    research: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    final_article: str\n",
    "    revision_count: int\n",
    "    needs_revision: bool\n",
    "\n",
    "# Define node functions -- each takes state and returns updated state\n",
    "\n",
    "def researcher_node(state: ArticleState) -> dict:\n",
    "    \"\"\"Research the topic and gather key information.\"\"\"\n",
    "    print(\"[Node: Researcher]\")\n",
    "    research = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a research analyst. Provide 4-5 key facts and insights about the topic. Be thorough but concise.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Research this topic: {state['topic']}\"}\n",
    "    ], max_tokens=500)\n",
    "    print(f\"  Research complete ({len(research)} chars)\")\n",
    "    return {\"research\": research}\n",
    "\n",
    "def writer_node(state: ArticleState) -> dict:\n",
    "    \"\"\"Write or revise the article based on research and any critique.\"\"\"\n",
    "    print(\"[Node: Writer]\")\n",
    "    \n",
    "    if state.get(\"critique\") and state.get(\"draft\"):\n",
    "        # Revision mode\n",
    "        prompt = f\"\"\"Revise this article based on the critique.\n",
    "\n",
    "Original article:\n",
    "{state['draft']}\n",
    "\n",
    "Critique:\n",
    "{state['critique']}\n",
    "\n",
    "Write an improved version addressing all feedback.\"\"\"\n",
    "    else:\n",
    "        # Initial draft mode\n",
    "        prompt = f\"\"\"Write a short, engaging article (3-4 paragraphs) based on this research:\n",
    "\n",
    "{state['research']}\"\"\"\n",
    "    \n",
    "    draft = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a skilled writer. Write clear, engaging content.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], max_tokens=600)\n",
    "    \n",
    "    revision_count = state.get(\"revision_count\", 0) + 1\n",
    "    print(f\"  Draft written (revision {revision_count}, {len(draft)} chars)\")\n",
    "    return {\"draft\": draft, \"revision_count\": revision_count}\n",
    "\n",
    "def critic_node(state: ArticleState) -> dict:\n",
    "    \"\"\"Critique the article and decide if revision is needed.\"\"\"\n",
    "    print(\"[Node: Critic]\")\n",
    "    critique = chat([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a critical editor. Review the article and:\n",
    "1. Rate it 1-10 for quality\n",
    "2. List specific improvements needed (if any)\n",
    "3. Start your response with 'SCORE: X/10'\n",
    "\n",
    "If the score is 8 or above, the article is ready for publication.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Article to review:\\n\\n{state['draft']}\"}\n",
    "    ], max_tokens=400)\n",
    "    \n",
    "    # Parse score to determine if revision is needed\n",
    "    score_match = re.search(r'SCORE:\\s*(\\d+)', critique)\n",
    "    score = int(score_match.group(1)) if score_match else 5\n",
    "    needs_revision = score < 8 and state.get(\"revision_count\", 0) < 3\n",
    "    \n",
    "    print(f\"  Score: {score}/10, Needs revision: {needs_revision}\")\n",
    "    return {\"critique\": critique, \"needs_revision\": needs_revision}\n",
    "\n",
    "def finalizer_node(state: ArticleState) -> dict:\n",
    "    \"\"\"Prepare the final article.\"\"\"\n",
    "    print(\"[Node: Finalizer]\")\n",
    "    print(f\"  Article finalized after {state.get('revision_count', 0)} revision(s)\")\n",
    "    return {\"final_article\": state[\"draft\"]}\n",
    "\n",
    "\n",
    "# Conditional routing function\n",
    "def should_revise(state: ArticleState) -> str:\n",
    "    \"\"\"Determine next node based on whether revision is needed.\"\"\"\n",
    "    if state.get(\"needs_revision\", False):\n",
    "        return \"writer\"  # Go back to writer for revision\n",
    "    return \"finalizer\"   # Move to finalization\n",
    "\n",
    "\n",
    "print(\"LangGraph nodes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LangGraph workflow\n",
    "\n",
    "workflow = StateGraph(ArticleState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"researcher\", researcher_node)\n",
    "workflow.add_node(\"writer\", writer_node)\n",
    "workflow.add_node(\"critic\", critic_node)\n",
    "workflow.add_node(\"finalizer\", finalizer_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"researcher\")       # Start with research\n",
    "workflow.add_edge(\"researcher\", \"writer\")     # Research -> Write\n",
    "workflow.add_edge(\"writer\", \"critic\")          # Write -> Critique\n",
    "\n",
    "# Conditional edge: critic decides whether to revise or finalize\n",
    "workflow.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_revise,\n",
    "    {\n",
    "        \"writer\": \"writer\",       # Loop back for revision\n",
    "        \"finalizer\": \"finalizer\"  # Move to finalization\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"finalizer\", END)  # Finalizer -> End\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled!\")\n",
    "print(\"\")\n",
    "print(\"Graph structure:\")\n",
    "print(\"  START -> researcher -> writer -> critic\")\n",
    "print(\"                           ^         |\")\n",
    "print(\"                           |    (conditional)\")\n",
    "print(\"                           +-- needs_revision --+\")\n",
    "print(\"                                     |\")\n",
    "print(\"                               finalizer -> END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LangGraph workflow\n",
    "print(\"=== Running LangGraph Article Pipeline ===\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "initial_state = {\n",
    "    \"topic\": \"How retrieval-augmented generation (RAG) is changing enterprise AI\",\n",
    "    \"research\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"critique\": \"\",\n",
    "    \"final_article\": \"\",\n",
    "    \"revision_count\": 0,\n",
    "    \"needs_revision\": False\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL ARTICLE:\")\n",
    "print(\"=\" * 60)\n",
    "print(final_state[\"final_article\"])\n",
    "print(f\"\\nRevisions: {final_state['revision_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Debate System\n",
    "\n",
    "The debate pattern is particularly useful for:\n",
    "- Reducing bias in LLM outputs\n",
    "- Exploring both sides of complex decisions\n",
    "- Stress-testing ideas before committing to them\n",
    "\n",
    "### Exercise 3: Build a Debate System\n",
    "\n",
    "Build a multi-agent debate system with:\n",
    "- **Pro Agent**: argues in favor of the proposition\n",
    "- **Con Agent**: argues against the proposition\n",
    "- **Judge Agent**: evaluates both sides and renders a verdict\n",
    "\n",
    "**Requirements:**\n",
    "- `debate(topic, rounds=3)` function\n",
    "- Each round: pro argues, con responds, both see previous arguments\n",
    "- After all rounds, the judge evaluates and decides\n",
    "- Return the full debate transcript and verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Multi-agent debate system\n",
    "\n",
    "def debate(topic, rounds=3):\n",
    "    \"\"\"\n",
    "    Run a multi-round debate between a pro agent and a con agent,\n",
    "    with a judge agent rendering a final verdict.\n",
    "    \n",
    "    Args:\n",
    "        topic: The proposition to debate\n",
    "        rounds: Number of debate rounds\n",
    "    Returns:\n",
    "        dict with 'transcript' (list of dicts) and 'verdict' (string)\n",
    "    \"\"\"\n",
    "    print(f\"=== DEBATE ===\")\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"Rounds: {rounds}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    transcript = []\n",
    "    \n",
    "    for round_num in range(1, rounds + 1):\n",
    "        # TODO: Build context from previous rounds for both agents\n",
    "        context = None\n",
    "        \n",
    "        # TODO: Pro agent makes their argument\n",
    "        # Should see previous arguments to build upon and counter\n",
    "        pro_argument = None\n",
    "        \n",
    "        # TODO: Con agent responds\n",
    "        # Should see pro's argument and previous context\n",
    "        con_argument = None\n",
    "        \n",
    "        # TODO: Append to transcript\n",
    "        pass\n",
    "    \n",
    "    # TODO: Judge evaluates the full debate and renders verdict\n",
    "    verdict = None\n",
    "    \n",
    "    return {\"transcript\": transcript, \"verdict\": verdict}\n",
    "\n",
    "# Test (will fail until you complete the TODOs)\n",
    "# result = debate(\"AI-generated code will replace most human programmers within 10 years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Multi-agent debate system\n",
    "\n",
    "def debate(topic, rounds=3):\n",
    "    \"\"\"\n",
    "    Run a multi-round debate between a pro agent and a con agent,\n",
    "    with a judge agent rendering a final verdict.\n",
    "    \"\"\"\n",
    "    print(f\"=== DEBATE ===\")\n",
    "    print(f\"Proposition: {topic}\")\n",
    "    print(f\"Rounds: {rounds}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    transcript = []\n",
    "    \n",
    "    for round_num in range(1, rounds + 1):\n",
    "        print(f\"\\n--- Round {round_num} ---\")\n",
    "        \n",
    "        # Build context from previous rounds\n",
    "        context = \"\"\n",
    "        if transcript:\n",
    "            context = \"Previous rounds:\\n\"\n",
    "            for entry in transcript:\n",
    "                context += f\"\\nRound {entry['round']}:\\n\"\n",
    "                context += f\"PRO: {entry['pro']}\\n\"\n",
    "                context += f\"CON: {entry['con']}\\n\"\n",
    "            context += \"\\n\"\n",
    "        \n",
    "        # Pro agent argues\n",
    "        print(\"\\n[PRO Agent]\")\n",
    "        pro_prompt = f\"{context}\" if context else \"\"\n",
    "        pro_prompt += f\"This is round {round_num} of {rounds}. Make your strongest argument FOR the proposition.\"\n",
    "        if round_num > 1:\n",
    "            pro_prompt += \" Address the opponent's previous points and strengthen your position.\"\n",
    "        \n",
    "        pro_argument = chat([\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"You are a skilled debater arguing IN FAVOR of this proposition: '{topic}'.\n",
    "Make compelling, well-reasoned arguments. Use evidence and logic.\n",
    "Keep your argument to 2-3 paragraphs.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": pro_prompt}\n",
    "        ], max_tokens=400)\n",
    "        print(pro_argument)\n",
    "        \n",
    "        # Con agent responds\n",
    "        print(\"\\n[CON Agent]\")\n",
    "        con_prompt = f\"{context}\" if context else \"\"\n",
    "        con_prompt += f\"\\nPro's argument this round:\\n{pro_argument}\\n\\n\"\n",
    "        con_prompt += f\"This is round {round_num} of {rounds}. Counter the pro's arguments and make your strongest case AGAINST the proposition.\"\n",
    "        \n",
    "        con_argument = chat([\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"You are a skilled debater arguing AGAINST this proposition: '{topic}'.\n",
    "Make compelling, well-reasoned counter-arguments. Use evidence and logic.\n",
    "Keep your argument to 2-3 paragraphs.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": con_prompt}\n",
    "        ], max_tokens=400)\n",
    "        print(con_argument)\n",
    "        \n",
    "        transcript.append({\n",
    "            \"round\": round_num,\n",
    "            \"pro\": pro_argument,\n",
    "            \"con\": con_argument\n",
    "        })\n",
    "    \n",
    "    # Judge evaluates\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"[JUDGE]\")\n",
    "    \n",
    "    full_debate = \"\\n\\n\".join([\n",
    "        f\"--- Round {entry['round']} ---\\nPRO: {entry['pro']}\\nCON: {entry['con']}\"\n",
    "        for entry in transcript\n",
    "    ])\n",
    "    \n",
    "    verdict = chat([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are an impartial debate judge. Evaluate the debate objectively.\n",
    "Your verdict must include:\n",
    "1. Summary of the strongest argument from each side\n",
    "2. Which side argued more effectively and why\n",
    "3. Your final verdict: PRO wins, CON wins, or DRAW\n",
    "4. A brief explanation of your reasoning\n",
    "Be fair and thorough.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Proposition: {topic}\\n\\nFull debate transcript:\\n\\n{full_debate}\"}\n",
    "    ], max_tokens=600)\n",
    "    \n",
    "    print(verdict)\n",
    "    \n",
    "    return {\"transcript\": transcript, \"verdict\": verdict}\n",
    "\n",
    "# Run the debate\n",
    "debate_result = debate(\"AI-generated code will replace most human programmers within 10 years\", rounds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Refactoring the Trading Agent with LangGraph\n",
    "\n",
    "The existing `ai_trading_agent.ipynb` uses LangChain's `SequentialChain` to connect 5 specialized agents:\n",
    "\n",
    "```\n",
    "Current Architecture (SequentialChain):\n",
    "\n",
    "  ticker -> [Market Data] -> [Sentiment] -> [Macro] -> [Strategy] -> [Risk]\n",
    "                                                            |\n",
    "                                                         (linear,\n",
    "                                                          no loops,\n",
    "                                                          no conditionals)\n",
    "```\n",
    "\n",
    "With LangGraph, we could add much more sophisticated behavior:\n",
    "\n",
    "```\n",
    "Modernized Architecture (LangGraph StateGraph):\n",
    "\n",
    "  ticker -> [Market Data] -+-> [Strategy]\n",
    "            [Sentiment]  --+       |\n",
    "            [Macro]      --+   (conditional)\n",
    "            (parallel)         /         \\\n",
    "                         [Risk OK]    [Risk Too High]\n",
    "                            |              |\n",
    "                          Output     [Revise Strategy]\n",
    "                                          |\n",
    "                                     (loop back to Risk)\n",
    "```\n",
    "\n",
    "### Key improvements with LangGraph:\n",
    "\n",
    "| Feature | SequentialChain | LangGraph |\n",
    "|---------|----------------|----------|\n",
    "| Execution order | Strictly linear | Flexible (parallel, conditional) |\n",
    "| Error handling | Fails the entire chain | Can retry individual nodes |\n",
    "| Conditional logic | Not supported | Built-in conditional edges |\n",
    "| Looping | Not supported | Nodes can loop back |\n",
    "| State management | Output variables only | Rich typed state |\n",
    "| Observability | `verbose=True` | Full state inspection at each node |\n",
    "\n",
    "### Conceptual Mapping\n",
    "\n",
    "```python\n",
    "# OLD: SequentialChain\n",
    "sequential_agent = SequentialChain(\n",
    "    chains=[market_data_chain, sentiment_chain, ...],\n",
    "    input_variables=[\"ticker\"],\n",
    "    output_variables=[\"market_data\", \"sentiment_analysis\", ...]\n",
    ")\n",
    "\n",
    "# NEW: LangGraph StateGraph\n",
    "class TradingState(TypedDict):\n",
    "    ticker: str\n",
    "    market_data: str\n",
    "    sentiment_analysis: str\n",
    "    macro_analysis: str\n",
    "    strategy: str\n",
    "    risk_assessment: str\n",
    "    risk_acceptable: bool\n",
    "\n",
    "workflow = StateGraph(TradingState)\n",
    "workflow.add_node(\"market_data\", market_data_node)\n",
    "workflow.add_node(\"sentiment\", sentiment_node)\n",
    "workflow.add_node(\"macro\", macro_node)\n",
    "workflow.add_node(\"strategy\", strategy_node)\n",
    "workflow.add_node(\"risk\", risk_node)\n",
    "\n",
    "# Can now add conditional edges, parallel execution, loops\n",
    "workflow.add_conditional_edges(\"risk\", check_risk, {...})\n",
    "```\n",
    "\n",
    "The key insight: LangGraph turns a rigid pipeline into a **flexible, observable state machine** that can handle real-world complexity like retries, conditionals, and feedback loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Multi-Agent Pipeline with LangGraph\n",
    "\n",
    "Build a 3-agent content pipeline using LangGraph:\n",
    "\n",
    "1. **Researcher**: gathers key facts about the topic\n",
    "2. **Writer**: produces a draft article from the research\n",
    "3. **Editor**: reviews and polishes the final output\n",
    "\n",
    "**Requirements:**\n",
    "- Define a `PipelineState` TypedDict with appropriate fields\n",
    "- Create node functions for each agent\n",
    "- Build a StateGraph with proper edges\n",
    "- Add a conditional edge: if the editor's quality score is below 7, loop back to the writer\n",
    "- Limit revisions to 2 maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Multi-agent pipeline with LangGraph\n",
    "\n",
    "# TODO: Define the state\n",
    "class PipelineState(TypedDict):\n",
    "    topic: str\n",
    "    research: str\n",
    "    draft: str\n",
    "    editor_feedback: str\n",
    "    final_output: str\n",
    "    quality_score: int\n",
    "    revision_count: int\n",
    "\n",
    "# TODO: Implement researcher node\n",
    "def research_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Gather key facts about the topic.\"\"\"\n",
    "    return {\"research\": None}  # TODO\n",
    "\n",
    "# TODO: Implement writer node\n",
    "def write_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Write or revise the article.\"\"\"\n",
    "    return {\"draft\": None, \"revision_count\": None}  # TODO\n",
    "\n",
    "# TODO: Implement editor node\n",
    "def edit_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Review the article and provide a quality score.\"\"\"\n",
    "    return {\"editor_feedback\": None, \"quality_score\": None}  # TODO\n",
    "\n",
    "# TODO: Implement output node\n",
    "def output_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Finalize the output.\"\"\"\n",
    "    return {\"final_output\": None}  # TODO\n",
    "\n",
    "# TODO: Implement routing function\n",
    "def route_after_edit(state: PipelineState) -> str:\n",
    "    \"\"\"Route to writer for revision or to output.\"\"\"\n",
    "    return None  # TODO: return \"writer\" or \"output\"\n",
    "\n",
    "# TODO: Build and compile the StateGraph\n",
    "# pipeline = StateGraph(PipelineState)\n",
    "# ... add nodes, edges, conditional edges ...\n",
    "# app = pipeline.compile()\n",
    "\n",
    "# TODO: Run the pipeline\n",
    "# result = app.invoke({\"topic\": \"...\", ...})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Multi-agent pipeline with LangGraph\n",
    "\n",
    "class PipelineState(TypedDict):\n",
    "    topic: str\n",
    "    research: str\n",
    "    draft: str\n",
    "    editor_feedback: str\n",
    "    final_output: str\n",
    "    quality_score: int\n",
    "    revision_count: int\n",
    "\n",
    "\n",
    "def research_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Gather key facts about the topic.\"\"\"\n",
    "    print(\"[Researcher Node]\")\n",
    "    research = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are a thorough researcher. Provide 5 key facts and insights about the given topic. Include specific details, numbers, and examples where possible.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Research this topic thoroughly: {state['topic']}\"}\n",
    "    ], max_tokens=500)\n",
    "    print(f\"  Research gathered ({len(research)} chars)\")\n",
    "    return {\"research\": research}\n",
    "\n",
    "\n",
    "def write_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Write or revise the article.\"\"\"\n",
    "    print(\"[Writer Node]\")\n",
    "    revision_count = state.get(\"revision_count\", 0) + 1\n",
    "    \n",
    "    if state.get(\"editor_feedback\") and state.get(\"draft\"):\n",
    "        # Revision mode\n",
    "        prompt = f\"\"\"Revise the article based on editor feedback.\n",
    "\n",
    "Current draft:\n",
    "{state['draft']}\n",
    "\n",
    "Editor feedback:\n",
    "{state['editor_feedback']}\n",
    "\n",
    "Write an improved version that addresses all the feedback.\"\"\"\n",
    "    else:\n",
    "        # Initial draft\n",
    "        prompt = f\"\"\"Write a well-structured article (3-4 paragraphs) on the topic '{state['topic']}' \n",
    "using this research:\n",
    "\n",
    "{state['research']}\n",
    "\n",
    "Make it informative, engaging, and well-organized.\"\"\"\n",
    "    \n",
    "    draft = chat([\n",
    "        {\"role\": \"system\", \"content\": \"You are an excellent writer. Write clear, engaging, well-structured content.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], max_tokens=700)\n",
    "    \n",
    "    print(f\"  Draft written (revision {revision_count}, {len(draft)} chars)\")\n",
    "    return {\"draft\": draft, \"revision_count\": revision_count}\n",
    "\n",
    "\n",
    "def edit_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Review the article and provide a quality score.\"\"\"\n",
    "    print(\"[Editor Node]\")\n",
    "    feedback = chat([\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a senior editor. Review the article and:\n",
    "1. Start with 'SCORE: X/10' (be honest and strict)\n",
    "2. List specific strengths (2-3 points)\n",
    "3. List specific areas for improvement (2-3 points)\n",
    "Score 7+ means publishable quality.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Review this article:\\n\\n{state['draft']}\"}\n",
    "    ], max_tokens=400)\n",
    "    \n",
    "    # Parse score\n",
    "    score_match = re.search(r'SCORE:\\s*(\\d+)', feedback)\n",
    "    score = int(score_match.group(1)) if score_match else 5\n",
    "    \n",
    "    print(f\"  Quality score: {score}/10\")\n",
    "    return {\"editor_feedback\": feedback, \"quality_score\": score}\n",
    "\n",
    "\n",
    "def output_node(state: PipelineState) -> dict:\n",
    "    \"\"\"Finalize the output.\"\"\"\n",
    "    print(\"[Output Node]\")\n",
    "    print(f\"  Finalized after {state.get('revision_count', 0)} revision(s), score: {state.get('quality_score', 'N/A')}/10\")\n",
    "    return {\"final_output\": state[\"draft\"]}\n",
    "\n",
    "\n",
    "def route_after_edit(state: PipelineState) -> str:\n",
    "    \"\"\"Route to writer for revision or to output based on quality score.\"\"\"\n",
    "    score = state.get(\"quality_score\", 0)\n",
    "    revisions = state.get(\"revision_count\", 0)\n",
    "    \n",
    "    if score < 7 and revisions < 3:\n",
    "        print(f\"  -> Routing back to writer (score {score} < 7, revision {revisions} < 3)\")\n",
    "        return \"writer\"\n",
    "    else:\n",
    "        reason = f\"score {score} >= 7\" if score >= 7 else f\"max revisions reached ({revisions})\"\n",
    "        print(f\"  -> Routing to output ({reason})\")\n",
    "        return \"output\"\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "pipeline = StateGraph(PipelineState)\n",
    "\n",
    "# Add nodes\n",
    "pipeline.add_node(\"researcher\", research_node)\n",
    "pipeline.add_node(\"writer\", write_node)\n",
    "pipeline.add_node(\"editor\", edit_node)\n",
    "pipeline.add_node(\"output\", output_node)\n",
    "\n",
    "# Add edges\n",
    "pipeline.add_edge(START, \"researcher\")\n",
    "pipeline.add_edge(\"researcher\", \"writer\")\n",
    "pipeline.add_edge(\"writer\", \"editor\")\n",
    "\n",
    "# Conditional edge from editor\n",
    "pipeline.add_conditional_edges(\n",
    "    \"editor\",\n",
    "    route_after_edit,\n",
    "    {\n",
    "        \"writer\": \"writer\",\n",
    "        \"output\": \"output\"\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline.add_edge(\"output\", END)\n",
    "\n",
    "# Compile\n",
    "content_pipeline = pipeline.compile()\n",
    "\n",
    "print(\"Content pipeline compiled!\")\n",
    "print(\"\")\n",
    "print(\"Graph:\")\n",
    "print(\"  START -> researcher -> writer -> editor\")\n",
    "print(\"                           ^          |\")\n",
    "print(\"                           |     (score < 7?)\")\n",
    "print(\"                           +--- yes ---+\")\n",
    "print(\"                                  |\")\n",
    "print(\"                                  no\")\n",
    "print(\"                                  |\")\n",
    "print(\"                              output -> END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "print(\"=== Running Content Pipeline ===\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = content_pipeline.invoke({\n",
    "    \"topic\": \"How large language models are transforming software development\",\n",
    "    \"research\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"editor_feedback\": \"\",\n",
    "    \"final_output\": \"\",\n",
    "    \"quality_score\": 0,\n",
    "    \"revision_count\": 0\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "print(result[\"final_output\"])\n",
    "print(f\"\\nTotal revisions: {result['revision_count']}\")\n",
    "print(f\"Final quality score: {result['quality_score']}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Key Takeaways\n",
    "\n",
    "### What we covered\n",
    "\n",
    "1. **Multi-Agent Patterns**: Four fundamental architectures -- sequential, parallel, hierarchical, and debate -- each suited to different problem types.\n",
    "\n",
    "2. **ReAct from Scratch**: Implemented the full Thought-Action-Observation loop without any framework, showing how agents reason about tool usage step by step.\n",
    "\n",
    "3. **Agent Memory**: Built buffer and summary memory systems that allow agents to maintain context across multiple conversation turns.\n",
    "\n",
    "4. **Agent Planning**: Decomposing complex goals into sub-tasks, executing them sequentially with cumulative context, and synthesizing the results.\n",
    "\n",
    "5. **Self-Reflection**: The generate-critique-revise pattern that allows agents to iteratively improve their own output.\n",
    "\n",
    "6. **LangGraph**: Building agent workflows as state machines with nodes, edges, conditional routing, and feedback loops.\n",
    "\n",
    "7. **Debate Systems**: Multi-agent debate with pro/con agents and an impartial judge, useful for balanced analysis and bias reduction.\n",
    "\n",
    "8. **Trading Agent Modernization**: How the existing `ai_trading_agent.ipynb` could be refactored from SequentialChain to a more flexible LangGraph StateGraph.\n",
    "\n",
    "### Key Design Principles\n",
    "\n",
    "| Principle | Description |\n",
    "|-----------|-------------|\n",
    "| **Single Responsibility** | Each agent should have one clear role |\n",
    "| **Explicit State** | Use typed state objects to pass data between agents |\n",
    "| **Observability** | Log each agent's input/output for debugging |\n",
    "| **Graceful Degradation** | Set max retries/iterations to prevent infinite loops |\n",
    "| **Composability** | Design agents that can be reused in different workflows |\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "Multi-agent systems are rapidly evolving. Key areas to watch:\n",
    "- **Agent-to-agent communication protocols** (e.g., AutoGen, CrewAI)\n",
    "- **Persistent agent memory** with vector databases\n",
    "- **Autonomous agent loops** with human-in-the-loop checkpoints\n",
    "- **Tool-use standardization** (e.g., OpenAI function calling, Anthropic tool use)\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- **Paper**: Park et al. *\"Generative Agents: Interactive Simulacra of Human Behavior\"* (2023) -- Simulated town of 25 AI agents with memory, planning, and reflection\n",
    "- **Paper**: Shinn et al. *\"Reflexion: Language Agents with Verbal Reinforcement Learning\"* (2023) -- Self-reflection pattern for agent improvement\n",
    "- **Paper**: Yao et al. *\"ReAct: Synergizing Reasoning and Acting in Language Models\"* (2022) -- The foundational ReAct framework\n",
    "- **Documentation**: [LangGraph Documentation](https://langchain-ai.github.io/langgraph/) -- Official guide for building agent workflows as state machines\n",
    "- **Course**: DeepLearning.AI *\"AI Agents in LangGraph\"* -- Practical course on building production agent systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
