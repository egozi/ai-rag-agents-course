{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5a692-bc65-4332-8256-5b99359dbf96",
   "metadata": {},
   "outputs": [],
   "source": "# Module 7a: Self-Attention Mechanism\n# This notebook walks through the self-attention mechanism step by step,\n# building from simple dot-product attention to full multi-head attention.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7a74c-24a0-4351-8052-1140ceecfc98",
   "metadata": {},
   "outputs": [],
   "source": "# Our input sequence: \"Your journey starts with one step\"\n# Each token is represented as a 3-dimensional embedding vector\n\ninputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\n\nprint(f\"Input shape: {inputs.shape}\")  # [seq_len, d_model] = [6, 3]\nprint(f\"Sequence length: {inputs.shape[0]}, Embedding dim: {inputs.shape[1]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9e6325-0585-4ef8-a3fc-ba3a18cc862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query -  tensor([0.5500, 0.8700, 0.6600])\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "print('Query - ', query)\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1478c28f-5fe9-4d6f-b091-1f5e89556e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(torch.tensor([0.43, 0.15, 0.89]), torch.tensor([0.55, 0.87, 0.66]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22535871-ee01-4512-af3d-9505729f8cda",
   "metadata": {},
   "source": [
    "### normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a9452ff-2063-4b78-be90-592140d520d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)  # torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f9cd9-31df-4461-9cbc-e086fb09bd14",
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Context Vector\n# The context vector is a weighted sum of ALL input vectors,\n# using the attention weights we just computed.\n# This gives us a new representation of token 2 (\"journey\")\n# that incorporates information from all other tokens.\n\ncontext_vec_2 = torch.zeros(inputs.shape[1])\nfor i, x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i] * x_i\n\nprint(\"Context vector for token 2 ('journey'):\")\nprint(context_vec_2)\nprint(f\"\\nOriginal token 2: {inputs[1]}\")\nprint(f\"Context vector:   {context_vec_2}\")\nprint(\"\\nThe context vector is a blend of all tokens, weighted by attention!\")"
  },
  {
   "cell_type": "code",
   "id": "h5nwipl8le6",
   "source": "# Let's compute ALL context vectors at once (for all tokens, not just token 2)\n# Using matrix multiplication: attention_scores = inputs @ inputs.T\n\n# Step 1: All pairwise attention scores\nattn_scores = inputs @ inputs.T  # [6, 6]\nprint(\"All attention scores (6x6 matrix):\")\nprint(attn_scores)\n\n# Step 2: Normalize each row with softmax\nattn_weights = torch.softmax(attn_scores, dim=-1)  # each row sums to 1\nprint(\"\\nAttention weights (after softmax):\")\nprint(attn_weights)\nprint(f\"\\nRow sums: {attn_weights.sum(dim=-1)}\")  # all 1.0\n\n# Step 3: Context vectors for all tokens\ncontext_vecs = attn_weights @ inputs  # [6, 6] @ [6, 3] = [6, 3]\nprint(f\"\\nAll context vectors shape: {context_vecs.shape}\")\nprint(context_vecs)\nprint(f\"\\nVerify: context_vecs[1] matches our earlier result: {torch.allclose(context_vecs[1], context_vec_2)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fk0f1vf2fzn",
   "source": "## Visualizing Attention Weights\n\nLet's see which tokens attend to which. This is the key insight of self-attention: each token can \"look at\" every other token with different intensities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uesrbe5nm2i",
   "source": "tokens = [\"Your\", \"journey\", \"starts\", \"with\", \"one\", \"step\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(attn_weights.numpy(), cmap='Blues')\nax.set_xticks(range(len(tokens)))\nax.set_yticks(range(len(tokens)))\nax.set_xticklabels(tokens, fontsize=12)\nax.set_yticklabels(tokens, fontsize=12)\nax.set_xlabel(\"Key (attended to)\", fontsize=12)\nax.set_ylabel(\"Query (attending)\", fontsize=12)\nax.set_title(\"Simple Self-Attention Weights\", fontsize=14)\n\n# Add text annotations\nfor i in range(len(tokens)):\n    for j in range(len(tokens)):\n        ax.text(j, i, f\"{attn_weights[i, j]:.2f}\", ha=\"center\", va=\"center\", fontsize=10)\n\nplt.colorbar(im)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5yak3kwhby2",
   "source": "---\n\n## Query, Key, Value (Q, K, V) Projections\n\nThe simple attention above uses the raw input vectors for everything. In practice, transformers use **learnable weight matrices** to project inputs into three different spaces:\n\n- **Query (Q)**: \"What am I looking for?\"\n- **Key (K)**: \"What do I contain?\"\n- **Value (V)**: \"What information do I provide?\"\n\nThis separation allows the model to learn different representations for matching (Q, K) vs. information transfer (V).\n\n```\nQ = X @ W_q    (what to search for)\nK = X @ W_k    (what to match against)  \nV = X @ W_v    (what to return)\n\nAttention(Q, K, V) = softmax(Q @ K^T) @ V\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "q3mnqojb17h",
   "source": "torch.manual_seed(42)\n\nd_model = inputs.shape[1]  # 3 (input embedding dimension)\nd_k = 2  # dimension of Q, K, V projections (can differ from d_model)\n\n# Learnable weight matrices\nW_q = torch.nn.Parameter(torch.randn(d_model, d_k))\nW_k = torch.nn.Parameter(torch.randn(d_model, d_k))\nW_v = torch.nn.Parameter(torch.randn(d_model, d_k))\n\nprint(f\"Input dim: {d_model}, Projection dim: {d_k}\")\nprint(f\"W_q shape: {W_q.shape}\")\nprint(f\"W_k shape: {W_k.shape}\")\nprint(f\"W_v shape: {W_v.shape}\")\n\n# Project inputs into Q, K, V spaces\nQ = inputs @ W_q  # [6, 3] @ [3, 2] = [6, 2]\nK = inputs @ W_k  # [6, 3] @ [3, 2] = [6, 2]\nV = inputs @ W_v  # [6, 3] @ [3, 2] = [6, 2]\n\nprint(f\"\\nQ shape: {Q.shape}\")\nprint(f\"K shape: {K.shape}\")\nprint(f\"V shape: {V.shape}\")\n\nprint(f\"\\nQuery for token 2 ('journey'): {Q[1]}\")\nprint(f\"Key for token 2 ('journey'):   {K[1]}\")\nprint(f\"Value for token 2 ('journey'): {V[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r2jvoingkv",
   "source": "# Compute attention with Q, K, V\n# Step 1: Q @ K^T gives attention scores\nattn_scores_qk = Q @ K.T  # [6, 2] @ [2, 6] = [6, 6]\nprint(\"Attention scores (Q @ K^T):\")\nprint(attn_scores_qk)\n\n# Step 2: Softmax to get weights\nattn_weights_qk = torch.softmax(attn_scores_qk, dim=-1)\nprint(\"\\nAttention weights:\")\nprint(attn_weights_qk)\n\n# Step 3: Weighted sum of VALUES (not inputs!)\ncontext_vecs_qk = attn_weights_qk @ V  # [6, 6] @ [6, 2] = [6, 2]\nprint(f\"\\nContext vectors shape: {context_vecs_qk.shape}\")\nprint(\"Context vectors:\")\nprint(context_vecs_qk)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h4w82s92q7s",
   "source": "---\n\n## Scaled Dot-Product Attention\n\nThere's a problem with the raw dot product: when the dimension `d_k` is large, the dot products can become very large, pushing softmax into regions with extremely small gradients.\n\n**Solution:** Divide by `sqrt(d_k)` to keep the variance stable.\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nThis is the attention formula from \"Attention Is All You Need\" (Vaswani et al., 2017).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bgi6q5089io",
   "source": "import math\n\n# Scaled attention scores\nscale = math.sqrt(d_k)\nprint(f\"Scale factor: sqrt({d_k}) = {scale:.4f}\")\n\nscaled_attn_scores = (Q @ K.T) / scale\nprint(f\"\\nUnscaled scores (sample): {attn_scores_qk[1].detach()}\")\nprint(f\"Scaled scores (sample):   {scaled_attn_scores[1].detach()}\")\n\n# The scaled scores have smaller magnitude, leading to softer attention weights\nscaled_attn_weights = torch.softmax(scaled_attn_scores, dim=-1)\nprint(f\"\\nUnscaled weights: {attn_weights_qk[1].detach()}\")\nprint(f\"Scaled weights:   {scaled_attn_weights[1].detach()}\")\nprint(\"\\nNotice: scaled weights are more uniform (less peaky) = better gradients!\")\n\n# Final context vectors with scaling\nscaled_context = scaled_attn_weights @ V\nprint(f\"\\nScaled context vectors shape: {scaled_context.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yl6pvchavfi",
   "source": "## Exercise 1: Implement Scaled Dot-Product Attention as a Function\n\n**Your Task:** Combine everything above into a clean function that performs scaled dot-product attention.\n\n**Steps:**\n1. Compute Q, K, V from inputs using weight matrices\n2. Compute attention scores: Q @ K^T\n3. Scale by sqrt(d_k)\n4. Apply softmax\n5. Compute context vectors: weights @ V\n\n**Hints:**\n- Input shape: `[seq_len, d_model]`\n- Output shape: `[seq_len, d_v]` where d_v is the value projection dimension",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "q1pv3ca053",
   "source": "def scaled_dot_product_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute scaled dot-product attention.\n    \n    Args:\n        X: input tensor [seq_len, d_model]\n        W_q: query weight matrix [d_model, d_k]\n        W_k: key weight matrix [d_model, d_k]\n        W_v: value weight matrix [d_model, d_v]\n    \n    Returns:\n        context_vectors: [seq_len, d_v]\n        attention_weights: [seq_len, seq_len]\n    \"\"\"\n    # TODO: Implement scaled dot-product attention\n    \n    # 1. Project inputs to Q, K, V\n    Q = None  # Your code\n    K = None  # Your code\n    V = None  # Your code\n    \n    # 2. Compute attention scores\n    d_k = None  # Your code: get dimension of keys\n    attn_scores = None  # Your code: Q @ K^T / sqrt(d_k)\n    \n    # 3. Apply softmax\n    attn_weights = None  # Your code\n    \n    # 4. Compute context vectors\n    context = None  # Your code\n    \n    return context, attn_weights\n\n# Test your implementation\ncontext, weights = scaled_dot_product_attention(inputs, W_q, W_k, W_v)\nprint(f\"Context shape: {context.shape}\")\nprint(f\"Weights shape: {weights.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3r0zap17owz",
   "source": "### Solution for Exercise 1",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6djq6bit4v6",
   "source": "def scaled_dot_product_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute scaled dot-product attention - SOLUTION\n    \"\"\"\n    # 1. Project inputs to Q, K, V\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    \n    # 2. Compute scaled attention scores\n    d_k = K.shape[-1]\n    attn_scores = (Q @ K.T) / math.sqrt(d_k)\n    \n    # 3. Apply softmax\n    attn_weights = torch.softmax(attn_scores, dim=-1)\n    \n    # 4. Compute context vectors\n    context = attn_weights @ V\n    \n    return context, attn_weights\n\n# Test\ncontext, weights = scaled_dot_product_attention(inputs, W_q, W_k, W_v)\nprint(f\"Context shape: {context.shape}\")  # [6, 2]\nprint(f\"Weights shape: {weights.shape}\")  # [6, 6]\nprint(f\"\\nContext vectors:\\n{context.detach()}\")\nprint(f\"\\nAttention weights:\\n{weights.detach()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b5q6zw29jb",
   "source": "---\n\n## Causal Masking (for Autoregressive Models)\n\nIn models like GPT, each token should only attend to **previous tokens** (and itself), not future tokens. This is because during generation, future tokens don't exist yet.\n\nWe achieve this by setting attention scores for future positions to `-inf` before softmax, which makes their attention weights become 0.\n\n```\nCausal mask for sequence length 6:\n\n  Your journey starts with one step\nYour    ✓     ✗      ✗     ✗    ✗    ✗\njourney ✓     ✓      ✗     ✗    ✗    ✗  \nstarts  ✓     ✓      ✓     ✗    ✗    ✗\nwith    ✓     ✓      ✓     ✓    ✗    ✗\none     ✓     ✓      ✓     ✓    ✓    ✗\nstep    ✓     ✓      ✓     ✓    ✓    ✓\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gdbfyaso9me",
   "source": "seq_len = inputs.shape[0]\n\n# Create causal mask: upper triangle = True (positions to mask)\ncausal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\nprint(\"Causal mask (True = masked/blocked):\")\nprint(causal_mask.int())\n\n# Compute attention scores as before\nQ = inputs @ W_q\nK = inputs @ W_k\nV = inputs @ W_v\nattn_scores_causal = (Q @ K.T) / math.sqrt(d_k)\n\nprint(f\"\\nScores before masking:\\n{attn_scores_causal.detach()}\")\n\n# Apply mask: set future positions to -inf\nattn_scores_causal = attn_scores_causal.masked_fill(causal_mask, float('-inf'))\nprint(f\"\\nScores after masking:\\n{attn_scores_causal.detach()}\")\n\n# Softmax: -inf becomes 0\ncausal_weights = torch.softmax(attn_scores_causal, dim=-1)\nprint(f\"\\nCausal attention weights:\")\nprint(causal_weights.detach())\n\n# Context vectors with causal attention\ncausal_context = causal_weights @ V\nprint(f\"\\nCausal context vectors:\\n{causal_context.detach()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ivtvn1wurw9",
   "source": "# Visualize: bidirectional vs causal attention weights\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor ax, w, title in zip(axes, \n                          [weights.detach(), causal_weights.detach()],\n                          [\"Bidirectional (BERT-style)\", \"Causal (GPT-style)\"]):\n    im = ax.imshow(w.numpy(), cmap='Blues', vmin=0, vmax=0.5)\n    ax.set_xticks(range(len(tokens)))\n    ax.set_yticks(range(len(tokens)))\n    ax.set_xticklabels(tokens, fontsize=10)\n    ax.set_yticklabels(tokens, fontsize=10)\n    ax.set_xlabel(\"Key\")\n    ax.set_ylabel(\"Query\")\n    ax.set_title(title, fontsize=13)\n    for i in range(len(tokens)):\n        for j in range(len(tokens)):\n            ax.text(j, i, f\"{w[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n\nplt.suptitle(\"Bidirectional vs Causal Attention\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4o28n7bztrf",
   "source": "---\n\n## Multi-Head Attention\n\nInstead of performing one attention computation, we perform **multiple attention computations in parallel** (called \"heads\"), each with its own Q, K, V projections.\n\n**Why multiple heads?**\n- Different heads can learn to attend to different types of relationships\n- One head might focus on syntactic relationships, another on semantic ones\n- It's like having multiple \"perspectives\" on the same input\n\n**How it works:**\n1. Split the model dimension into `n_heads` smaller dimensions\n2. Each head performs independent scaled dot-product attention\n3. Concatenate all head outputs\n4. Project back to the original dimension with a final linear layer\n\n```\nMultiHead(Q, K, V) = Concat(head_1, ..., head_h) @ W_o\n\nwhere head_i = Attention(X @ W_q_i, X @ W_k_i, X @ W_v_i)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9aniencdqh8",
   "source": "torch.manual_seed(42)\n\n# For multi-head attention, let's use a larger embedding dimension\nd_model_mh = 6    # model dimension (must be divisible by n_heads)\nn_heads = 2       # number of attention heads\nd_head = d_model_mh // n_heads  # dimension per head = 3\n\nprint(f\"d_model: {d_model_mh}, n_heads: {n_heads}, d_head: {d_head}\")\n\n# Create new inputs with d_model=6 (by repeating our 3-dim inputs)\ninputs_mh = torch.cat([inputs, inputs], dim=-1)  # [6, 6]\nprint(f\"Input shape: {inputs_mh.shape}\")\n\n# Weight matrices for ALL heads combined\n# Instead of separate W_q for each head, we use one big matrix and reshape\nW_q_mh = nn.Linear(d_model_mh, d_model_mh, bias=False)\nW_k_mh = nn.Linear(d_model_mh, d_model_mh, bias=False)\nW_v_mh = nn.Linear(d_model_mh, d_model_mh, bias=False)\nW_o = nn.Linear(d_model_mh, d_model_mh, bias=False)  # output projection\n\n# Project all inputs at once\nQ_mh = W_q_mh(inputs_mh)  # [6, 6]\nK_mh = W_k_mh(inputs_mh)  # [6, 6]\nV_mh = W_v_mh(inputs_mh)  # [6, 6]\n\nprint(f\"\\nQ shape (before split): {Q_mh.shape}\")\n\n# Reshape to separate heads: [seq_len, d_model] -> [seq_len, n_heads, d_head]\nQ_heads = Q_mh.view(seq_len, n_heads, d_head)\nK_heads = K_mh.view(seq_len, n_heads, d_head)\nV_heads = V_mh.view(seq_len, n_heads, d_head)\n\n# Transpose to [n_heads, seq_len, d_head] for batch matrix multiply\nQ_heads = Q_heads.permute(1, 0, 2)  # [2, 6, 3]\nK_heads = K_heads.permute(1, 0, 2)  # [2, 6, 3]\nV_heads = V_heads.permute(1, 0, 2)  # [2, 6, 3]\n\nprint(f\"Q per head shape: {Q_heads.shape}\")\nprint(f\"Head 0 Q:\\n{Q_heads[0].detach()}\")\nprint(f\"Head 1 Q:\\n{Q_heads[1].detach()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5ghbnszl4m4",
   "source": "# Compute attention for ALL heads simultaneously using batch matrix multiply\n# attn_scores: [n_heads, seq_len, d_head] @ [n_heads, d_head, seq_len] = [n_heads, seq_len, seq_len]\nattn_scores_mh = torch.bmm(Q_heads, K_heads.transpose(1, 2)) / math.sqrt(d_head)\nattn_weights_mh = torch.softmax(attn_scores_mh, dim=-1)\n\nprint(f\"Attention scores shape: {attn_scores_mh.shape}\")\nprint(f\"Attention weights shape: {attn_weights_mh.shape}\")\n\n# Context vectors per head: [n_heads, seq_len, seq_len] @ [n_heads, seq_len, d_head] = [n_heads, seq_len, d_head]\nhead_outputs = torch.bmm(attn_weights_mh, V_heads)\nprint(f\"\\nPer-head output shape: {head_outputs.shape}\")\n\n# Concatenate heads: [n_heads, seq_len, d_head] -> [seq_len, n_heads * d_head] = [seq_len, d_model]\nhead_outputs = head_outputs.permute(1, 0, 2)  # [seq_len, n_heads, d_head]\nconcat_output = head_outputs.reshape(seq_len, d_model_mh)  # [seq_len, d_model]\nprint(f\"Concatenated shape: {concat_output.shape}\")\n\n# Final output projection\nmh_output = W_o(concat_output)\nprint(f\"Final output shape: {mh_output.shape}\")\nprint(f\"\\nMulti-head attention output:\\n{mh_output.detach()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bp3xw0f55yw",
   "source": "# Visualize attention patterns for each head\nfig, axes = plt.subplots(1, n_heads, figsize=(12, 5))\n\nfor h in range(n_heads):\n    ax = axes[h]\n    w = attn_weights_mh[h].detach().numpy()\n    im = ax.imshow(w, cmap='Blues', vmin=0, vmax=0.5)\n    ax.set_xticks(range(len(tokens)))\n    ax.set_yticks(range(len(tokens)))\n    ax.set_xticklabels(tokens, fontsize=10)\n    ax.set_yticklabels(tokens, fontsize=10)\n    ax.set_title(f\"Head {h+1}\", fontsize=13)\n    ax.set_xlabel(\"Key\")\n    ax.set_ylabel(\"Query\")\n    for i in range(len(tokens)):\n        for j in range(len(tokens)):\n            ax.text(j, i, f\"{w[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n\nplt.suptitle(\"Multi-Head Attention: Each Head Learns Different Patterns\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\nprint(\"Notice how each head attends to different positions!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ezo7wrqolp",
   "source": "## Exercise 2: Implement Multi-Head Attention as an nn.Module\n\n**Your Task:** Package multi-head attention into a reusable PyTorch module.\n\n**Steps:**\n1. Initialize weight matrices W_q, W_k, W_v, W_o as `nn.Linear` layers\n2. In `forward()`: project → split heads → compute attention → concat → output project\n3. Support optional causal masking\n\n**Hints:**\n- Use `view()` and `permute()` to reshape between `[seq_len, d_model]` and `[n_heads, seq_len, d_head]`\n- Use `torch.bmm()` for batched matrix multiplication\n- Use `masked_fill()` for causal masking",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jbykz6l0xm7",
   "source": "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_head = d_model // n_heads\n        \n        # TODO: Create linear layers for Q, K, V projections and output\n        self.W_q = None  # Your code: nn.Linear(d_model, d_model, bias=False)\n        self.W_k = None  # Your code\n        self.W_v = None  # Your code\n        self.W_o = None  # Your code\n    \n    def forward(self, x, causal=False):\n        \"\"\"\n        Args:\n            x: input tensor [seq_len, d_model]\n            causal: whether to apply causal masking\n        Returns:\n            output: [seq_len, d_model]\n            attn_weights: [n_heads, seq_len, seq_len]\n        \"\"\"\n        seq_len = x.shape[0]\n        \n        # TODO: 1. Project to Q, K, V\n        Q = None  # Your code\n        K = None  # Your code\n        V = None  # Your code\n        \n        # TODO: 2. Reshape to [n_heads, seq_len, d_head]\n        Q = None  # Your code: view then permute\n        K = None  # Your code\n        V = None  # Your code\n        \n        # TODO: 3. Compute scaled attention scores\n        attn_scores = None  # Your code\n        \n        # TODO: 4. Apply causal mask if needed\n        if causal:\n            pass  # Your code: create mask and apply with masked_fill\n        \n        # TODO: 5. Softmax\n        attn_weights = None  # Your code\n        \n        # TODO: 6. Apply attention to values\n        head_outputs = None  # Your code\n        \n        # TODO: 7. Concat heads and output projection\n        output = None  # Your code\n        \n        return output, attn_weights\n\n# Test your implementation\nmha = MultiHeadAttention(d_model=6, n_heads=2)\nout, attn = mha(inputs_mh, causal=False)\nprint(f\"Output shape: {out.shape}\")\nprint(f\"Attention shape: {attn.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "egs78h9cke",
   "source": "### Solution for Exercise 2",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "np2r0qrvjgg",
   "source": "class MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention - SOLUTION\"\"\"\n    \n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n        \n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_head = d_model // n_heads\n        \n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n        self.W_k = nn.Linear(d_model, d_model, bias=False)\n        self.W_v = nn.Linear(d_model, d_model, bias=False)\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n    \n    def forward(self, x, causal=False):\n        seq_len = x.shape[0]\n        \n        # 1. Project to Q, K, V\n        Q = self.W_q(x)\n        K = self.W_k(x)\n        V = self.W_v(x)\n        \n        # 2. Reshape: [seq_len, d_model] -> [n_heads, seq_len, d_head]\n        Q = Q.view(seq_len, self.n_heads, self.d_head).permute(1, 0, 2)\n        K = K.view(seq_len, self.n_heads, self.d_head).permute(1, 0, 2)\n        V = V.view(seq_len, self.n_heads, self.d_head).permute(1, 0, 2)\n        \n        # 3. Scaled dot-product attention scores\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(self.d_head)\n        \n        # 4. Causal mask\n        if causal:\n            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n            attn_scores = attn_scores.masked_fill(mask.unsqueeze(0), float('-inf'))\n        \n        # 5. Softmax\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        \n        # 6. Weighted sum of values\n        head_outputs = torch.bmm(attn_weights, V)  # [n_heads, seq_len, d_head]\n        \n        # 7. Concat and project: [n_heads, seq_len, d_head] -> [seq_len, d_model]\n        head_outputs = head_outputs.permute(1, 0, 2)  # [seq_len, n_heads, d_head]\n        concat = head_outputs.reshape(seq_len, self.d_model)\n        output = self.W_o(concat)\n        \n        return output, attn_weights\n\n# Test\ntorch.manual_seed(42)\nmha = MultiHeadAttention(d_model=6, n_heads=2)\n\n# Bidirectional\nout_bi, attn_bi = mha(inputs_mh, causal=False)\nprint(f\"Bidirectional output shape: {out_bi.shape}\")\n\n# Causal\nout_causal, attn_causal = mha(inputs_mh, causal=True)\nprint(f\"Causal output shape: {out_causal.shape}\")\n\n# Visualize both\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfor col, (attn_w, title) in enumerate([(attn_bi, \"Bidirectional\"), (attn_causal, \"Causal\")]):\n    for h in range(2):\n        ax = axes[h][col]\n        w = attn_w[h].detach().numpy()\n        ax.imshow(w, cmap='Blues', vmin=0, vmax=0.5)\n        ax.set_xticks(range(len(tokens)))\n        ax.set_yticks(range(len(tokens)))\n        ax.set_xticklabels(tokens, fontsize=9)\n        ax.set_yticklabels(tokens, fontsize=9)\n        ax.set_title(f\"{title} - Head {h+1}\", fontsize=11)\n        for i in range(len(tokens)):\n            for j in range(len(tokens)):\n                ax.text(j, i, f\"{w[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n\nplt.suptitle(\"Multi-Head Attention Module\", fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "vcm1llk4ano",
   "source": "---\n\n## Comparison with PyTorch's Built-in Multi-Head Attention\n\nPyTorch provides `nn.MultiheadAttention` which does everything we just built. Let's verify our understanding matches the official implementation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "auxnnmdt9op",
   "source": "# PyTorch's built-in MultiheadAttention\n# Note: PyTorch expects input as [seq_len, batch_size, d_model]\npytorch_mha = nn.MultiheadAttention(embed_dim=6, num_heads=2, batch_first=False, bias=False)\n\n# Add batch dimension: [seq_len, d_model] -> [seq_len, 1, d_model]\nx_batched = inputs_mh.unsqueeze(1)\n\n# For self-attention: query = key = value = input\nwith torch.no_grad():\n    pt_output, pt_weights = pytorch_mha(x_batched, x_batched, x_batched)\n\nprint(f\"PyTorch MHA output shape: {pt_output.shape}\")  # [6, 1, 6]\nprint(f\"PyTorch attention weights shape: {pt_weights.shape}\")  # [1, 6, 6]\nprint(f\"\\nOur implementation and PyTorch's use the same architecture!\")\nprint(f\"The difference is only in weight initialization (random).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n11593d3l3",
   "source": "---\n\n## Summary\n\nIn this notebook, we built self-attention from the ground up:\n\n1. **Simple Attention**: dot product between input vectors → softmax → weighted sum\n2. **Context Vectors**: each token gets a new representation that blends information from all tokens\n3. **Q/K/V Projections**: learnable weight matrices separate \"what to search for\" (Q), \"what to match against\" (K), and \"what to return\" (V)\n4. **Scaled Dot-Product Attention**: divide by sqrt(d_k) for stable gradients\n5. **Causal Masking**: prevent attention to future tokens (for autoregressive models like GPT)\n6. **Multi-Head Attention**: multiple parallel attention computations capture different relationships\n\n### Key Formula\n\n$$\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O$$\n\n$$\\text{where head}_i = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right) V_i$$\n\n### Next Steps\n- **Module 7b**: Build a complete Transformer block using this attention mechanism\n- Add positional encoding, layer normalization, feed-forward networks, and residual connections\n\n### References\n- Paper: Vaswani et al. \"Attention Is All You Need\" (2017)\n- Blog: Jay Alammar \"The Illustrated Transformer\"\n- Book: Sebastian Raschka \"Build a Large Language Model (From Scratch)\" (Ch. 3)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}