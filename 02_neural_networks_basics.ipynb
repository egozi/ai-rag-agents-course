{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Neural Networks & Deep Learning Basics\n",
    "\n",
    "## Overview\n",
    "\n",
    "Neural networks are computational models inspired by the structure of biological neurons in the brain. They form the backbone of modern deep learning and power everything from image recognition to natural language processing.\n",
    "\n",
    "In this notebook, we will build up an understanding of neural networks from first principles:\n",
    "\n",
    "1. **The Perceptron** - The simplest neural unit, and its limitations\n",
    "2. **Activation Functions** - Non-linearities that give networks their power\n",
    "3. **Forward Pass & Backpropagation** - How networks learn through gradient descent\n",
    "4. **Loss Functions** - How we measure prediction quality\n",
    "5. **PyTorch Introduction** - The modern framework for building neural networks\n",
    "6. **MNIST Digit Classification** - A real-world example tying everything together\n",
    "\n",
    "By the end of this module you will be able to:\n",
    "- Implement a neural network from scratch using NumPy\n",
    "- Understand forward propagation and backpropagation at the mathematical level\n",
    "- Build and train neural networks using PyTorch\n",
    "- Classify handwritten digits with >95% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup\n",
    "\n",
    "First, let's install and import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch numpy matplotlib torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Perceptron\n",
    "\n",
    "The **perceptron** is the simplest form of a neural network -- a single neuron. It takes a set of inputs, multiplies each by a weight, adds a bias, and passes the result through an activation function.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$y = f\\left(\\sum_{i=1}^{n} w_i \\cdot x_i + b\\right) = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}$ = input vector\n",
    "- $\\mathbf{w}$ = weight vector\n",
    "- $b$ = bias term\n",
    "- $f$ = activation function (e.g., step function)\n",
    "\n",
    "### How the Perceptron Learns\n",
    "\n",
    "The perceptron learning rule updates weights based on the error:\n",
    "\n",
    "$$w_i \\leftarrow w_i + \\eta \\cdot (y_{\\text{true}} - y_{\\text{pred}}) \\cdot x_i$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"A single perceptron (neuron) implemented from scratch with NumPy.\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, learning_rate=0.1):\n",
    "        self.weights = np.random.randn(n_inputs) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def step_function(self, x):\n",
    "        \"\"\"Binary step activation: returns 1 if x >= 0, else 0.\"\"\"\n",
    "        return (x >= 0).astype(int)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Forward pass: compute weighted sum + bias, apply activation.\"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.step_function(linear_output)\n",
    "\n",
    "    def train(self, X, y, n_epochs=100):\n",
    "        \"\"\"Train the perceptron using the perceptron learning rule.\"\"\"\n",
    "        errors_per_epoch = []\n",
    "        for epoch in range(n_epochs):\n",
    "            total_errors = 0\n",
    "            for xi, yi in zip(X, y):\n",
    "                prediction = self.predict(xi.reshape(1, -1))[0]\n",
    "                error = yi - prediction\n",
    "                if error != 0:\n",
    "                    total_errors += 1\n",
    "                    self.weights += self.lr * error * xi\n",
    "                    self.bias += self.lr * error\n",
    "            errors_per_epoch.append(total_errors)\n",
    "            if total_errors == 0:\n",
    "                print(f\"Converged at epoch {epoch + 1}\")\n",
    "                break\n",
    "        return errors_per_epoch\n",
    "\n",
    "\n",
    "print(\"Perceptron class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning AND and OR Gates\n",
    "\n",
    "The perceptron can learn linearly separable functions like AND and OR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the truth tables\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "y_or  = np.array([0, 1, 1, 1])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# --- AND Gate ---\n",
    "print(\"=\" * 40)\n",
    "print(\"Training Perceptron on AND gate\")\n",
    "print(\"=\" * 40)\n",
    "p_and = Perceptron(n_inputs=2, learning_rate=0.1)\n",
    "errors_and = p_and.train(X, y_and, n_epochs=100)\n",
    "predictions_and = p_and.predict(X)\n",
    "for xi, yi, pred in zip(X, y_and, predictions_and):\n",
    "    status = \"OK\" if yi == pred else \"WRONG\"\n",
    "    print(f\"  Input: {xi} -> Expected: {yi}, Predicted: {pred}  [{status}]\")\n",
    "print(f\"  Weights: {p_and.weights}, Bias: {p_and.bias:.4f}\")\n",
    "\n",
    "# --- OR Gate ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Training Perceptron on OR gate\")\n",
    "print(\"=\" * 40)\n",
    "p_or = Perceptron(n_inputs=2, learning_rate=0.1)\n",
    "errors_or = p_or.train(X, y_or, n_epochs=100)\n",
    "predictions_or = p_or.predict(X)\n",
    "for xi, yi, pred in zip(X, y_or, predictions_or):\n",
    "    status = \"OK\" if yi == pred else \"WRONG\"\n",
    "    print(f\"  Input: {xi} -> Expected: {yi}, Predicted: {pred}  [{status}]\")\n",
    "print(f\"  Weights: {p_or.weights}, Bias: {p_or.bias:.4f}\")\n",
    "\n",
    "# --- XOR Gate (will fail!) ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"Training Perceptron on XOR gate\")\n",
    "print(\"=\" * 40)\n",
    "p_xor = Perceptron(n_inputs=2, learning_rate=0.1)\n",
    "errors_xor = p_xor.train(X, y_xor, n_epochs=100)\n",
    "predictions_xor = p_xor.predict(X)\n",
    "correct = 0\n",
    "for xi, yi, pred in zip(X, y_xor, predictions_xor):\n",
    "    status = \"OK\" if yi == pred else \"WRONG\"\n",
    "    if yi == pred:\n",
    "        correct += 1\n",
    "    print(f\"  Input: {xi} -> Expected: {yi}, Predicted: {pred}  [{status}]\")\n",
    "print(f\"  Accuracy: {correct}/{len(y_xor)}\")\n",
    "print(f\"  -> The perceptron CANNOT learn XOR because it is not linearly separable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundaries\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "gate_names = [\"AND\", \"OR\", \"XOR\"]\n",
    "gate_labels = [y_and, y_or, y_xor]\n",
    "gate_models = [p_and, p_or, p_xor]\n",
    "\n",
    "for ax, name, labels, model in zip(axes, gate_names, gate_labels, gate_models):\n",
    "    # Plot data points\n",
    "    for i, (xi, yi) in enumerate(zip(X, labels)):\n",
    "        color = 'blue' if yi == 1 else 'red'\n",
    "        marker = 'o' if yi == 1 else 'x'\n",
    "        ax.scatter(xi[0], xi[1], c=color, marker=marker, s=200, zorder=5,\n",
    "                   edgecolors='black', linewidth=1.5)\n",
    "\n",
    "    # Plot decision boundary: w1*x1 + w2*x2 + b = 0 => x2 = -(w1*x1 + b) / w2\n",
    "    w1, w2 = model.weights\n",
    "    b = model.bias\n",
    "    if abs(w2) > 1e-8:\n",
    "        x1_range = np.linspace(-0.5, 1.5, 100)\n",
    "        x2_boundary = -(w1 * x1_range + b) / w2\n",
    "        ax.plot(x1_range, x2_boundary, 'g--', linewidth=2, label='Decision boundary')\n",
    "\n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.set_xlabel('$x_1$', fontsize=12)\n",
    "    ax.set_ylabel('$x_2$', fontsize=12)\n",
    "    ax.set_title(f'{name} Gate', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle('Perceptron Decision Boundaries', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Notice: AND and OR have clean linear boundaries. XOR cannot be separated by a single line.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight:** A single perceptron can only learn **linearly separable** functions. XOR requires a non-linear decision boundary, which means we need **multiple layers** of neurons -- a neural network.\n",
    "\n",
    "---\n",
    "## 3. Activation Functions\n",
    "\n",
    "Activation functions introduce **non-linearity** into neural networks. Without them, stacking multiple layers would be equivalent to a single linear transformation (since a composition of linear functions is still linear).\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "| Function | Formula | Range | Use Case |\n",
    "|----------|---------|-------|----------|\n",
    "| Sigmoid | $\\sigma(x) = \\frac{1}{1+e^{-x}}$ | (0, 1) | Binary classification output |\n",
    "| Tanh | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | Hidden layers (zero-centered) |\n",
    "| ReLU | $\\max(0, x)$ | [0, inf) | Default for hidden layers |\n",
    "| Leaky ReLU | $\\max(\\alpha x, x)$ | (-inf, inf) | Avoids dying ReLU problem |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement activation functions and their derivatives\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1.0, alpha)\n",
    "\n",
    "print(\"Activation functions defined.\")\n",
    "print(f\"  sigmoid(0) = {sigmoid(0):.4f}\")\n",
    "print(f\"  relu(-1) = {relu(-1):.4f}, relu(1) = {relu(1):.4f}\")\n",
    "print(f\"  tanh(0) = {tanh(0):.4f}\")\n",
    "print(f\"  leaky_relu(-1) = {leaky_relu(-1):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all activation functions and their derivatives side by side\n",
    "x = np.linspace(-5, 5, 500)\n",
    "\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid, sigmoid_derivative, 'tab:blue'),\n",
    "    ('ReLU', relu, relu_derivative, 'tab:orange'),\n",
    "    ('Tanh', tanh, tanh_derivative, 'tab:green'),\n",
    "    ('Leaky ReLU', leaky_relu, leaky_relu_derivative, 'tab:red'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot activations\n",
    "for name, func, _, color in activations:\n",
    "    axes[0].plot(x, func(x), label=name, color=color, linewidth=2)\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='black', linewidth=0.5)\n",
    "axes[0].set_title('Activation Functions', fontsize=14)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('f(x)', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_ylim(-1.5, 5)\n",
    "\n",
    "# Plot derivatives\n",
    "for name, _, deriv, color in activations:\n",
    "    axes[1].plot(x, deriv(x), label=f\"{name}'\", color=color, linewidth=2)\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='black', linewidth=0.5)\n",
    "axes[1].set_title('Derivatives of Activation Functions', fontsize=14)\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel(\"f'(x)\", fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].set_ylim(-0.2, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Why ReLU is popular:\")\n",
    "print(\"  1. Computationally efficient (just a max operation)\")\n",
    "print(\"  2. Does not saturate for positive values (no vanishing gradient)\")\n",
    "print(\"  3. Sigmoid/tanh derivatives approach 0 for large |x| -> vanishing gradients\")\n",
    "print(\"  4. Sparse activation: neurons output exactly 0 for negative inputs\")\n",
    "print(\"\\nThe vanishing gradient problem:\")\n",
    "print(\"  When gradients are very small (close to 0), weight updates become tiny.\")\n",
    "print(\"  In deep networks, these small gradients multiply during backpropagation,\")\n",
    "print(\"  making it nearly impossible for early layers to learn. ReLU avoids this\")\n",
    "print(\"  because its gradient is 1 for all positive inputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: 2-Layer Neural Network from Scratch (NumPy) for XOR\n",
    "\n",
    "Now that we know a single perceptron cannot solve XOR, let's build a 2-layer neural network from scratch using NumPy.\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer: 2 neurons (for the two XOR inputs)\n",
    "- Hidden layer: 4 neurons with sigmoid activation\n",
    "- Output layer: 1 neuron with sigmoid activation\n",
    "\n",
    "**Your task:** Implement the forward pass, backward pass (backpropagation), and training loop.\n",
    "\n",
    "**Hint for backpropagation:**\n",
    "- Output error: $\\delta_{out} = (y - \\hat{y}) \\cdot \\sigma'(z_{out})$\n",
    "- Hidden error: $\\delta_{hidden} = (\\delta_{out} \\cdot W_{out}^T) \\cdot \\sigma'(z_{hidden})$\n",
    "- Weight update: $W \\leftarrow W + \\eta \\cdot a^T \\cdot \\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a 2-layer neural network for XOR\n",
    "\n",
    "class NeuralNetworkXOR:\n",
    "    def __init__(self, input_size=2, hidden_size=4, output_size=1, lr=1.0):\n",
    "        np.random.seed(42)\n",
    "        self.lr = lr\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network.\n",
    "        TODO: Implement the forward pass.\n",
    "        1. Compute hidden layer pre-activation: z1 = X @ W1 + b1\n",
    "        2. Apply sigmoid activation: a1 = sigmoid(z1)\n",
    "        3. Compute output pre-activation: z2 = a1 @ W2 + b2\n",
    "        4. Apply sigmoid activation: a2 = sigmoid(z2)\n",
    "        Store z1, a1, z2, a2 as instance variables for backprop.\n",
    "        Return a2 (the prediction).\n",
    "        \"\"\"\n",
    "        self.z1 = None  # TODO\n",
    "        self.a1 = None  # TODO\n",
    "        self.z2 = None  # TODO\n",
    "        self.a2 = None  # TODO\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward pass (backpropagation).\n",
    "        TODO: Implement backpropagation.\n",
    "        1. Compute output layer delta: d2 = (y - a2) * sigmoid_derivative(z2)\n",
    "        2. Compute hidden layer delta: d1 = (d2 @ W2.T) * sigmoid_derivative(z1)\n",
    "        3. Update W2, b2 using a1 and d2\n",
    "        4. Update W1, b1 using X and d1\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        # TODO: Implement gradient computation and weight updates\n",
    "        pass\n",
    "\n",
    "    def train(self, X, y, epochs=10000):\n",
    "        \"\"\"Training loop.\n",
    "        TODO: Implement the training loop.\n",
    "        For each epoch:\n",
    "        1. Forward pass\n",
    "        2. Compute loss (MSE)\n",
    "        3. Backward pass\n",
    "        4. Record loss every 1000 epochs\n",
    "        Return list of losses.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        # TODO: Implement training loop\n",
    "        return losses\n",
    "\n",
    "\n",
    "# Test data\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"Exercise 1: Implement the forward, backward, and train methods above.\")\n",
    "print(\"Then run the cell below to test your implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Complete 2-layer neural network for XOR\n",
    "\n",
    "class NeuralNetworkXOR_Solution:\n",
    "    def __init__(self, input_size=2, hidden_size=4, output_size=1, lr=1.0):\n",
    "        np.random.seed(42)\n",
    "        self.lr = lr\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward pass (backpropagation).\"\"\"\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Output layer gradients\n",
    "        d2 = (y - self.a2) * self.sigmoid_derivative(self.z2)\n",
    "\n",
    "        # Hidden layer gradients\n",
    "        d1 = (d2 @ self.W2.T) * self.sigmoid_derivative(self.z1)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W2 += self.lr * (self.a1.T @ d2) / m\n",
    "        self.b2 += self.lr * np.sum(d2, axis=0, keepdims=True) / m\n",
    "        self.W1 += self.lr * (X.T @ d1) / m\n",
    "        self.b1 += self.lr * np.sum(d1, axis=0, keepdims=True) / m\n",
    "\n",
    "    def train(self, X, y, epochs=10000):\n",
    "        \"\"\"Training loop.\"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Compute MSE loss\n",
    "            loss = np.mean((y - output) ** 2)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "\n",
    "            # Record loss\n",
    "            if epoch % 1000 == 0:\n",
    "                losses.append(loss)\n",
    "                if epoch % 2000 == 0:\n",
    "                    print(f\"  Epoch {epoch:5d} | Loss: {loss:.6f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "# Train the network\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"Training 2-layer neural network on XOR...\")\n",
    "print(\"=\" * 40)\n",
    "nn_xor = NeuralNetworkXOR_Solution(input_size=2, hidden_size=4, output_size=1, lr=2.0)\n",
    "losses = nn_xor.train(X_xor, y_xor, epochs=10000)\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nFinal predictions:\")\n",
    "predictions = nn_xor.forward(X_xor)\n",
    "for xi, yi, pred in zip(X_xor, y_xor, predictions):\n",
    "    rounded = round(pred[0])\n",
    "    status = \"OK\" if yi[0] == rounded else \"WRONG\"\n",
    "    print(f\"  Input: {xi} -> Expected: {yi[0]}, Predicted: {pred[0]:.4f} (rounded: {rounded}) [{status}]\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(0, 10000, 1000), losses, 'b-o', linewidth=2, markersize=4)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('XOR Training Loss (2-Layer Neural Network)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSuccess! The 2-layer network can learn XOR, unlike a single perceptron.\")\n",
    "print(\"The hidden layer creates a non-linear feature space where XOR becomes separable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Forward Pass & Backpropagation\n",
    "\n",
    "### Step-by-Step Walkthrough\n",
    "\n",
    "Let's trace through a concrete example to see exactly how forward propagation and backpropagation work.\n",
    "\n",
    "**Network Architecture:** 2 inputs -> 2 hidden neurons -> 1 output\n",
    "\n",
    "#### The Computation Graph\n",
    "\n",
    "```\n",
    "x1 --w1--> [h1] --w5-->\n",
    "      \\   /              \\\n",
    "       \\ /                [o1] --> Loss\n",
    "       / \\                /\n",
    "      /   \\              /\n",
    "x2 --w3--> [h2] --w6-->\n",
    "```\n",
    "\n",
    "Each arrow represents a weight. Each node computes a weighted sum plus bias, then applies an activation function.\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "Backpropagation is simply the **chain rule** applied systematically. If we have a composition of functions:\n",
    "\n",
    "$$L = f(g(h(x)))$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial x}$$\n",
    "\n",
    "In a neural network, this means we compute gradients layer by layer, from the output back to the input -- hence \"back\" propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual step-by-step forward and backward pass\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP-BY-STEP FORWARD PASS AND BACKPROPAGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize with specific values for clarity\n",
    "x = np.array([[0.5, 0.8]])  # Input\n",
    "y_true = np.array([[1.0]])  # Target\n",
    "\n",
    "# Weights (small, fixed for demonstration)\n",
    "W1 = np.array([[0.1, 0.3],\n",
    "               [0.2, 0.4]])\n",
    "b1 = np.array([[0.01, 0.02]])\n",
    "\n",
    "W2 = np.array([[0.5],\n",
    "               [0.6]])\n",
    "b2 = np.array([[0.03]])\n",
    "\n",
    "print(\"\\n--- FORWARD PASS ---\")\n",
    "print(f\"Input x = {x}\")\n",
    "print(f\"Target y = {y_true}\")\n",
    "\n",
    "# Layer 1: Hidden\n",
    "z1 = x @ W1 + b1\n",
    "print(f\"\\nHidden layer pre-activation:\")\n",
    "print(f\"  z1 = x @ W1 + b1 = {x} @ {W1.tolist()} + {b1}\")\n",
    "print(f\"  z1 = {z1}\")\n",
    "\n",
    "a1 = sigmoid(z1)\n",
    "print(f\"  a1 = sigmoid(z1) = {a1}\")\n",
    "\n",
    "# Layer 2: Output\n",
    "z2 = a1 @ W2 + b2\n",
    "print(f\"\\nOutput layer pre-activation:\")\n",
    "print(f\"  z2 = a1 @ W2 + b2 = {z2}\")\n",
    "\n",
    "a2 = sigmoid(z2)\n",
    "print(f\"  a2 = sigmoid(z2) = {a2}  (this is our prediction)\")\n",
    "\n",
    "# Loss (MSE)\n",
    "loss = np.mean((y_true - a2) ** 2)\n",
    "print(f\"\\nMSE Loss = (y - a2)^2 = ({y_true[0,0]:.4f} - {a2[0,0]:.4f})^2 = {loss:.6f}\")\n",
    "\n",
    "print(\"\\n--- BACKWARD PASS (Backpropagation) ---\")\n",
    "print(\"Using chain rule to compute gradients...\")\n",
    "\n",
    "# dL/da2\n",
    "dL_da2 = -2 * (y_true - a2)\n",
    "print(f\"\\ndL/da2 = -2(y - a2) = {dL_da2}\")\n",
    "\n",
    "# da2/dz2\n",
    "da2_dz2 = sigmoid_derivative(z2)\n",
    "print(f\"da2/dz2 = sigmoid'(z2) = {da2_dz2}\")\n",
    "\n",
    "# dL/dz2 (delta at output)\n",
    "delta2 = dL_da2 * da2_dz2\n",
    "print(f\"delta2 = dL/dz2 = dL/da2 * da2/dz2 = {delta2}\")\n",
    "\n",
    "# Gradients for W2 and b2\n",
    "dL_dW2 = a1.T @ delta2\n",
    "dL_db2 = delta2\n",
    "print(f\"\\ndL/dW2 = a1.T @ delta2 = {dL_dW2.flatten()}\")\n",
    "print(f\"dL/db2 = delta2 = {dL_db2.flatten()}\")\n",
    "\n",
    "# Backpropagate to hidden layer\n",
    "dL_da1 = delta2 @ W2.T\n",
    "print(f\"\\ndL/da1 = delta2 @ W2.T = {dL_da1}\")\n",
    "\n",
    "da1_dz1 = sigmoid_derivative(z1)\n",
    "delta1 = dL_da1 * da1_dz1\n",
    "print(f\"delta1 = dL/da1 * sigmoid'(z1) = {delta1}\")\n",
    "\n",
    "# Gradients for W1 and b1\n",
    "dL_dW1 = x.T @ delta1\n",
    "dL_db1 = delta1\n",
    "print(f\"\\ndL/dW1 = x.T @ delta1:\")\n",
    "print(f\"  {dL_dW1}\")\n",
    "print(f\"dL/db1 = delta1 = {dL_db1}\")\n",
    "\n",
    "# Update weights\n",
    "lr = 0.5\n",
    "print(f\"\\n--- WEIGHT UPDATE (lr={lr}) ---\")\n",
    "W2_new = W2 - lr * dL_dW2\n",
    "b2_new = b2 - lr * dL_db2\n",
    "W1_new = W1 - lr * dL_dW1\n",
    "b1_new = b1 - lr * dL_db1\n",
    "\n",
    "print(f\"W2: {W2.flatten()} -> {W2_new.flatten()}\")\n",
    "print(f\"b2: {b2.flatten()} -> {b2_new.flatten()}\")\n",
    "print(f\"W1:\\n  {W1} ->\\n  {W1_new}\")\n",
    "\n",
    "# Verify: forward pass with new weights should give lower loss\n",
    "z1_new = x @ W1_new + b1_new\n",
    "a1_new = sigmoid(z1_new)\n",
    "z2_new = a1_new @ W2_new + b2_new\n",
    "a2_new = sigmoid(z2_new)\n",
    "loss_new = np.mean((y_true - a2_new) ** 2)\n",
    "\n",
    "print(f\"\\n--- VERIFICATION ---\")\n",
    "print(f\"Old prediction: {a2[0,0]:.6f}, Loss: {loss:.6f}\")\n",
    "print(f\"New prediction: {a2_new[0,0]:.6f}, Loss: {loss_new:.6f}\")\n",
    "print(f\"Loss decreased by: {loss - loss_new:.6f}\")\n",
    "print(f\"The gradient step moved us closer to the target ({y_true[0,0]}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Loss Functions\n",
    "\n",
    "Loss functions measure how far our predictions are from the true values. The choice of loss function depends on the task:\n",
    "\n",
    "- **Regression** (predicting continuous values): Mean Squared Error (MSE)\n",
    "- **Classification** (predicting categories): Cross-Entropy Loss\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "- Penalizes large errors heavily (quadratic)\n",
    "- Always non-negative, equals 0 when predictions are perfect\n",
    "\n",
    "### Binary Cross-Entropy\n",
    "\n",
    "$$\\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "- Natural pairing with sigmoid output\n",
    "- Measures the divergence between predicted probability distribution and true distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and visualize loss functions\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"Binary Cross-Entropy loss. Epsilon prevents log(0).\"\"\"\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "# Demonstrate\n",
    "y_true_val = 1.0\n",
    "y_preds = np.linspace(0.01, 0.99, 200)\n",
    "\n",
    "mse_values = [(y_true_val - yp) ** 2 for yp in y_preds]\n",
    "bce_values = [-(y_true_val * np.log(yp) + (1 - y_true_val) * np.log(1 - yp)) for yp in y_preds]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MSE plot\n",
    "axes[0].plot(y_preds, mse_values, 'b-', linewidth=2)\n",
    "axes[0].axvline(x=y_true_val, color='red', linestyle='--', label=f'True value = {y_true_val}')\n",
    "axes[0].set_xlabel('Predicted value ($\\hat{y}$)', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Mean Squared Error (Regression)', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# BCE plot\n",
    "axes[1].plot(y_preds, bce_values, 'r-', linewidth=2)\n",
    "axes[1].axvline(x=y_true_val, color='blue', linestyle='--', label=f'True label = {int(y_true_val)}')\n",
    "axes[1].set_xlabel('Predicted probability ($\\hat{y}$)', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Binary Cross-Entropy (Classification)', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].set_ylim(0, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"When to use each loss function:\")\n",
    "print(\"  MSE: Regression tasks (predicting house prices, temperatures, etc.)\")\n",
    "print(\"  Cross-Entropy: Classification tasks (spam detection, image classification, etc.)\")\n",
    "print(\"\\nKey difference:\")\n",
    "print(\"  MSE penalizes errors quadratically.\")\n",
    "print(\"  BCE penalizes confident wrong predictions very heavily (loss -> infinity as\")\n",
    "print(\"  predicted probability -> 0 when true label is 1).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. PyTorch Introduction\n",
    "\n",
    "PyTorch is a deep learning framework that provides:\n",
    "1. **Tensors** - Like NumPy arrays, but with GPU acceleration\n",
    "2. **Autograd** - Automatic differentiation (no manual backprop!)\n",
    "3. **nn.Module** - Building blocks for neural networks\n",
    "\n",
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\n--- Tensor Creation ---\")\n",
    "\n",
    "# From Python lists\n",
    "t1 = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"From list: {t1}, dtype: {t1.dtype}\")\n",
    "\n",
    "# From NumPy\n",
    "np_arr = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "t2 = torch.from_numpy(np_arr)\n",
    "print(f\"From NumPy:\\n{t2}\")\n",
    "\n",
    "# Special tensors\n",
    "t_zeros = torch.zeros(2, 3)\n",
    "t_ones = torch.ones(2, 3)\n",
    "t_rand = torch.randn(2, 3)  # Normal distribution\n",
    "print(f\"\\nZeros:\\n{t_zeros}\")\n",
    "print(f\"Random (normal):\\n{t_rand}\")\n",
    "\n",
    "print(\"\\n--- Tensor Operations ---\")\n",
    "\n",
    "a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(f\"a + b =\\n{a + b}\")\n",
    "print(f\"a * b (element-wise) =\\n{a * b}\")\n",
    "print(f\"a @ b (matrix multiply) =\\n{a @ b}\")\n",
    "print(f\"a.mean() = {a.mean():.2f}\")\n",
    "print(f\"a.shape = {a.shape}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "a_device = a.to(device)\n",
    "print(f\"\\nTensor on device: {a_device.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd: Automatic Differentiation\n",
    "\n",
    "PyTorch can automatically compute gradients for us. No more manual backpropagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd demo\n",
    "print(\"--- Autograd Demo ---\")\n",
    "\n",
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "\n",
    "# Define a function: y = x^2 + 2x + 1\n",
    "y = x**2 + 2*x + 1\n",
    "print(f\"y = x^2 + 2x + 1 = {y.item():.2f}\")\n",
    "\n",
    "# Compute gradient: dy/dx = 2x + 2\n",
    "y.backward()\n",
    "print(f\"dy/dx at x=3: {x.grad.item():.2f} (expected: 2*3 + 2 = 8)\")\n",
    "\n",
    "print(\"\\n--- Autograd with Vectors ---\")\n",
    "\n",
    "# More complex example\n",
    "w = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x_vec = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# y = w . x (dot product)\n",
    "y = torch.dot(w, x_vec)\n",
    "print(f\"w = {w.data}\")\n",
    "print(f\"x = {x_vec}\")\n",
    "print(f\"y = w . x = {y.item():.2f}\")\n",
    "\n",
    "# dy/dw = x\n",
    "y.backward()\n",
    "print(f\"dy/dw = {w.grad} (expected: {x_vec}, since dy/dw_i = x_i)\")\n",
    "\n",
    "print(\"\\n--- Autograd with a Mini Neural Network ---\")\n",
    "\n",
    "# Simulate a tiny forward + backward pass\n",
    "torch.manual_seed(42)\n",
    "x_in = torch.randn(1, 3)       # 1 sample, 3 features\n",
    "w_hidden = torch.randn(3, 2, requires_grad=True)\n",
    "w_out = torch.randn(2, 1, requires_grad=True)\n",
    "target = torch.tensor([[1.0]])\n",
    "\n",
    "# Forward\n",
    "hidden = torch.sigmoid(x_in @ w_hidden)\n",
    "output = torch.sigmoid(hidden @ w_out)\n",
    "loss = ((target - output) ** 2).mean()\n",
    "\n",
    "print(f\"Input:  {x_in.data.numpy().flatten()}\")\n",
    "print(f\"Output: {output.item():.4f}\")\n",
    "print(f\"Target: {target.item():.4f}\")\n",
    "print(f\"Loss:   {loss.item():.6f}\")\n",
    "\n",
    "# Backward - PyTorch computes ALL gradients automatically!\n",
    "loss.backward()\n",
    "print(f\"\\nGradients computed automatically:\")\n",
    "print(f\"  dL/dw_hidden shape: {w_hidden.grad.shape}\")\n",
    "print(f\"  dL/dw_hidden:\\n  {w_hidden.grad}\")\n",
    "print(f\"  dL/dw_out shape: {w_out.grad.shape}\")\n",
    "print(f\"  dL/dw_out:\\n  {w_out.grad}\")\n",
    "print(\"\\nNo manual gradient calculation needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module: Building Blocks\n",
    "\n",
    "`nn.Module` is the base class for all neural network modules in PyTorch. It handles parameter management, GPU transfer, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a neural network with nn.Module\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"A simple 2-layer neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create and inspect the network\n",
    "model = SimpleNet(input_size=2, hidden_size=4, output_size=1)\n",
    "print(\"Network architecture:\")\n",
    "print(model)\n",
    "\n",
    "print(\"\\nModel parameters:\")\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}, params={param.numel()}\")\n",
    "    total_params += param.numel()\n",
    "print(f\"  Total parameters: {total_params}\")\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.tensor([[0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "with torch.no_grad():\n",
    "    output = model(x_test)\n",
    "print(f\"\\nTest forward pass (3 samples):\")\n",
    "print(f\"  Input shape:  {x_test.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Outputs: {output.squeeze().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. MNIST Digit Classification\n",
    "\n",
    "Now let's put it all together and build a real neural network that classifies handwritten digits from the MNIST dataset.\n",
    "\n",
    "**MNIST:**\n",
    "- 70,000 grayscale images of handwritten digits (0-9)\n",
    "- Each image is 28x28 pixels = 784 input features\n",
    "- 60,000 training images, 10,000 test images\n",
    "\n",
    "**Our MLP Architecture:**\n",
    "- Input: 784 (flattened 28x28 image)\n",
    "- Hidden 1: 128 neurons, ReLU\n",
    "- Hidden 2: 64 neurons, ReLU\n",
    "- Output: 10 neurons (one per digit class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} images\")\n",
    "print(f\"Test set:     {len(test_dataset)} images\")\n",
    "print(f\"Image shape:  {train_dataset[0][0].shape} (channels x height x width)\")\n",
    "print(f\"Classes:      {train_dataset.classes}\")\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {label}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Sample Images', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron for MNIST digit classification.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden1=128, hidden2=64, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 10)\n",
    "\n",
    "        # Select activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation on output (raw logits for CrossEntropyLoss)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create model\n",
    "model_mnist = MNISTClassifier(hidden1=128, hidden2=64, activation='relu').to(device)\n",
    "print(\"MNIST Classifier Architecture:\")\n",
    "print(model_mnist)\n",
    "\n",
    "total_params = sum(p.numel() for p in model_mnist.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=5, lr=0.001):\n",
    "    \"\"\"Train the model and return training history.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / num_batches\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = 100.0 * correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"  Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Training MNIST Classifier (784 -> 128 -> 64 -> 10)...\")\n",
    "print(\"=\" * 60)\n",
    "train_losses, test_accuracies = train_model(model_mnist, train_loader, test_loader, epochs=5, lr=0.001)\n",
    "print(f\"\\nFinal test accuracy: {test_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss curve\n",
    "axes[0].plot(range(1, len(train_losses) + 1), train_losses, 'b-o', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Curve', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy curve\n",
    "axes[1].plot(range(1, len(test_accuracies) + 1), test_accuracies, 'g-o', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Test Accuracy Curve', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "\n",
    "model_mnist.eval()\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images_dev = test_images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_mnist(test_images_dev)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "predictions = predictions.cpu()\n",
    "probabilities = probabilities.cpu()\n",
    "\n",
    "# Show 16 sample predictions\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = test_images[i].squeeze()\n",
    "    pred = predictions[i].item()\n",
    "    true = test_labels[i].item()\n",
    "    conf = probabilities[i][pred].item() * 100\n",
    "\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'Pred: {pred} ({conf:.0f}%)\\nTrue: {true}', fontsize=9, color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('MNIST Predictions (green=correct, red=incorrect)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count correct/incorrect in this batch\n",
    "batch_correct = (predictions == test_labels).sum().item()\n",
    "print(f\"Batch accuracy: {batch_correct}/{len(test_labels)} = {100*batch_correct/len(test_labels):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Experiment with Hidden Layer Sizes\n",
    "\n",
    "How does the size of the hidden layers affect the model's performance? In this exercise, you will train multiple MNIST classifiers with different hidden layer sizes and compare their accuracies.\n",
    "\n",
    "**Your task:**\n",
    "1. Train models with hidden sizes: 32, 64, 128, 256\n",
    "2. Record the final test accuracy for each\n",
    "3. Plot a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compare different hidden layer sizes\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "results = {}  # Will store {size: (losses, accuracies)}\n",
    "\n",
    "for size in hidden_sizes:\n",
    "    # TODO: Create a MNISTClassifier with hidden1=size, hidden2=size//2\n",
    "    model = None  # TODO\n",
    "\n",
    "    # TODO: Train the model for 5 epochs\n",
    "    losses, accuracies = None, None  # TODO\n",
    "\n",
    "    # TODO: Store results\n",
    "    # results[size] = (losses, accuracies)\n",
    "    pass\n",
    "\n",
    "# TODO: Plot comparison of test accuracies for each hidden size\n",
    "# Create a plot with epochs on x-axis and accuracy on y-axis\n",
    "# with one line per hidden size\n",
    "\n",
    "print(\"Exercise 2: Implement the training loop above and plot the results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare different hidden layer sizes\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "results = {}\n",
    "\n",
    "for size in hidden_sizes:\n",
    "    print(f\"\\nTraining with hidden_size={size} (hidden2={size//2})...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model = MNISTClassifier(hidden1=size, hidden2=size//2, activation='relu').to(device)\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Parameters: {param_count:,}\")\n",
    "\n",
    "    losses, accuracies = train_model(model, train_loader, test_loader, epochs=5, lr=0.001)\n",
    "    results[size] = (losses, accuracies)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "epochs_range = range(1, 6)\n",
    "\n",
    "for (size, (losses, accuracies)), color in zip(results.items(), colors):\n",
    "    axes[0].plot(epochs_range, losses, '-o', color=color, linewidth=2, markersize=5,\n",
    "                 label=f'hidden={size}')\n",
    "    axes[1].plot(epochs_range, accuracies, '-o', color=color, linewidth=2, markersize=5,\n",
    "                 label=f'hidden={size}')\n",
    "\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss by Hidden Layer Size', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Test Accuracy by Hidden Layer Size', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"{'Hidden Size':<15} {'Final Accuracy':>15} {'Parameters':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for size in hidden_sizes:\n",
    "    acc = results[size][1][-1]\n",
    "    params = 784 * size + size + size * (size // 2) + (size // 2) + (size // 2) * 10 + 10\n",
    "    print(f\"{size:<15} {acc:>14.2f}% {params:>12,}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  - Larger hidden layers generally achieve higher accuracy.\")\n",
    "print(\"  - But the improvement diminishes -- doubling size does not double accuracy.\")\n",
    "print(\"  - Larger models train slower and use more memory.\")\n",
    "print(\"  - The right size depends on the task complexity and available compute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Visualize Activation Function Effects on Training\n",
    "\n",
    "Different activation functions can dramatically affect how quickly and how well a network trains. In this exercise, you will train the same MNIST architecture with three different activation functions and compare their convergence behavior.\n",
    "\n",
    "**Your task:**\n",
    "1. Train MNIST classifiers with sigmoid, ReLU, and tanh activations\n",
    "2. Use the same architecture (784 -> 128 -> 64 -> 10) and learning rate\n",
    "3. Plot the training loss curves for all three on the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Compare activation functions on MNIST\n",
    "\n",
    "activation_functions = ['sigmoid', 'relu', 'tanh']\n",
    "activation_results = {}  # Will store {activation: (losses, accuracies)}\n",
    "\n",
    "for act_fn in activation_functions:\n",
    "    # TODO: Create a MNISTClassifier with this activation function\n",
    "    model = None  # TODO\n",
    "\n",
    "    # TODO: Train the model for 5 epochs\n",
    "    losses, accuracies = None, None  # TODO\n",
    "\n",
    "    # TODO: Store results\n",
    "    # activation_results[act_fn] = (losses, accuracies)\n",
    "    pass\n",
    "\n",
    "# TODO: Plot training loss curves for all three activations on the same figure\n",
    "# Also plot accuracy curves\n",
    "\n",
    "print(\"Exercise 3: Implement the training loop above and plot the results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Compare activation functions on MNIST\n",
    "\n",
    "activation_functions = ['sigmoid', 'relu', 'tanh']\n",
    "activation_results = {}\n",
    "\n",
    "for act_fn in activation_functions:\n",
    "    print(f\"\\nTraining with activation={act_fn}...\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    model = MNISTClassifier(hidden1=128, hidden2=64, activation=act_fn).to(device)\n",
    "    losses, accuracies = train_model(model, train_loader, test_loader, epochs=5, lr=0.001)\n",
    "    activation_results[act_fn] = (losses, accuracies)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "act_colors = {'sigmoid': 'tab:blue', 'relu': 'tab:orange', 'tanh': 'tab:green'}\n",
    "epochs_range = range(1, 6)\n",
    "\n",
    "for act_fn in activation_functions:\n",
    "    losses, accuracies = activation_results[act_fn]\n",
    "    color = act_colors[act_fn]\n",
    "    axes[0].plot(epochs_range, losses, '-o', color=color, linewidth=2,\n",
    "                 markersize=6, label=act_fn.capitalize())\n",
    "    axes[1].plot(epochs_range, accuracies, '-o', color=color, linewidth=2,\n",
    "                 markersize=6, label=act_fn.capitalize())\n",
    "\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss by Activation Function', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Test Accuracy by Activation Function', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"{'Activation':<15} {'Final Accuracy':>15} {'Final Loss':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for act_fn in activation_functions:\n",
    "    losses, accuracies = activation_results[act_fn]\n",
    "    print(f\"{act_fn.capitalize():<15} {accuracies[-1]:>14.2f}% {losses[-1]:>12.4f}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  - ReLU typically converges fastest and achieves the highest accuracy.\")\n",
    "print(\"  - Sigmoid is slowest due to the vanishing gradient problem.\")\n",
    "print(\"  - Tanh is better than sigmoid (zero-centered) but still suffers from saturation.\")\n",
    "print(\"  - This is why ReLU is the default choice for hidden layers in modern networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary & References\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **The Perceptron** is the simplest neural unit. It can only learn linearly separable functions (AND, OR) but not non-linear ones (XOR).\n",
    "\n",
    "2. **Activation functions** introduce non-linearity. ReLU is the most popular choice for hidden layers because it avoids the vanishing gradient problem.\n",
    "\n",
    "3. **Backpropagation** is the systematic application of the chain rule to compute gradients layer by layer, from the output back to the input.\n",
    "\n",
    "4. **Loss functions** quantify how wrong our predictions are. MSE for regression, cross-entropy for classification.\n",
    "\n",
    "5. **PyTorch** provides tensors (GPU-accelerated arrays), autograd (automatic differentiation), and nn.Module (building blocks for networks).\n",
    "\n",
    "6. **Deeper and wider networks** can model more complex functions, but with diminishing returns and increased computational cost.\n",
    "\n",
    "7. **The choice of activation function** significantly impacts training dynamics. ReLU consistently outperforms sigmoid and tanh for deep networks.\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In the next module, we will explore:\n",
    "- Convolutional Neural Networks (CNNs) for image data\n",
    "- Recurrent Neural Networks (RNNs) for sequential data\n",
    "- Regularization techniques (dropout, batch normalization)\n",
    "- Transfer learning\n",
    "\n",
    "### References\n",
    "\n",
    "- **Book:** Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Chapters 6-8 cover feedforward networks, regularization, and optimization. Available free at [deeplearningbook.org](https://www.deeplearningbook.org/)\n",
    "\n",
    "- **Course:** fast.ai - *Practical Deep Learning for Coders*. A hands-on course that teaches deep learning from the top down. [course.fast.ai](https://course.fast.ai/)\n",
    "\n",
    "- **Tutorial:** PyTorch - *Deep Learning with PyTorch: A 60 Minute Blitz*. Official PyTorch tutorial covering tensors, autograd, and neural networks. [pytorch.org/tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "\n",
    "- **Paper:** Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). *Learning representations by back-propagating errors*. Nature, 323(6088), 533-536. The foundational paper on backpropagation.\n",
    "\n",
    "- **Paper:** Glorot, X., & Bengio, Y. (2010). *Understanding the difficulty of training deep feedforward neural networks*. Explains the vanishing gradient problem and initialization strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}