{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Module 3: NLP Fundamentals - Text Preprocessing & Classical Methods\n",
    "\n",
    "## Why NLP Matters\n",
    "\n",
    "Natural Language Processing (NLP) is the branch of AI concerned with giving computers the ability to understand, generate, and manipulate human language. From search engines and spam filters to chatbots and machine translation, NLP powers countless applications we use every day.\n",
    "\n",
    "## Classical vs. Modern Approaches\n",
    "\n",
    "In this module we focus on **classical NLP** techniques that form the foundation of the field:\n",
    "\n",
    "| Classical (this module) | Modern (later modules) |\n",
    "|------------------------|------------------------|\n",
    "| Rule-based tokenization | Subword tokenization (BPE, WordPiece) |\n",
    "| Bag-of-Words, TF-IDF | Word embeddings (Word2Vec, GloVe) |\n",
    "| N-gram language models | Transformer language models |\n",
    "| Feature engineering + classifiers | End-to-end deep learning |\n",
    "\n",
    "Understanding classical methods is essential because:\n",
    "1. They establish the **vocabulary and intuition** used throughout NLP.\n",
    "2. They are **fast, interpretable, and effective** baselines.\n",
    "3. Modern methods (embeddings, transformers) are designed to overcome their specific **limitations**, so knowing those limitations helps you appreciate why modern methods work.\n",
    "\n",
    "**What we will cover:**\n",
    "- Text preprocessing (tokenization, stop words, stemming, lemmatization)\n",
    "- Bag-of-Words and TF-IDF representations\n",
    "- N-grams\n",
    "- Text classification with classical ML\n",
    "- Limitations and the bridge to word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn matplotlib numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Before we can do anything useful with text, we need to clean and normalize it. Raw text is messy: it has inconsistent casing, punctuation, special characters, and other noise. **Preprocessing** converts raw text into a clean, standardized form that algorithms can work with.\n",
    "\n",
    "### Sample Corpus\n",
    "\n",
    "We will use a small corpus of movie reviews throughout this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The movie was absolutely fantastic! Great acting and a wonderful storyline.\",\n",
    "    \"I hated this film. The plot was terrible and the acting was wooden.\",\n",
    "    \"A masterpiece of modern cinema. The director's vision is breathtaking.\",\n",
    "    \"Worst movie I've ever seen. Complete waste of time and money.\",\n",
    "    \"The special effects were amazing, but the story felt flat and predictable.\",\n",
    "    \"An emotional rollercoaster! The performances were Oscar-worthy and the music was hauntingly beautiful.\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 0, 1]  # 1 = positive, 0 = negative\n",
    "\n",
    "for i, review in enumerate(corpus):\n",
    "    sentiment = \"positive\" if labels[i] == 1 else \"negative\"\n",
    "    print(f\"[{sentiment}] {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "### 2.1 Lowercasing and Punctuation Removal\n",
    "\n",
    "The simplest preprocessing steps are **lowercasing** (so \"The\" and \"the\" are treated identically) and **punctuation removal** (so \"fantastic!\" becomes \"fantastic\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = corpus[0]\n",
    "print(\"Original:  \", sample)\n",
    "\n",
    "# Lowercasing\n",
    "lowered = sample.lower()\n",
    "print(\"Lowered:   \", lowered)\n",
    "\n",
    "# Punctuation removal using str.translate\n",
    "no_punct = lowered.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"No punct:  \", no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization Approaches\n",
    "\n",
    "**Tokenization** is the process of splitting text into individual units (tokens). There are several approaches, each with different tradeoffs.\n",
    "\n",
    "#### Approach 1: Whitespace Splitting\n",
    "The simplest method -- just split on spaces. Fast, but does not handle punctuation attached to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The movie was absolutely fantastic! Great acting.\"\n",
    "\n",
    "# Whitespace splitting\n",
    "tokens_whitespace = text.split()\n",
    "print(\"Whitespace split:\", tokens_whitespace)\n",
    "print(\"Notice 'fantastic!' still has punctuation attached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "#### Approach 2: Regex Tokenization\n",
    "\n",
    "We can use a regular expression to extract only sequences of word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex tokenization: extract sequences of word characters\n",
    "tokens_regex = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "print(\"Regex tokens:\", tokens_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "#### Approach 3: NLTK word_tokenize\n",
    "\n",
    "NLTK's `word_tokenize` uses a sophisticated rule-based tokenizer (Punkt) that handles contractions, abbreviations, and other edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_contraction = \"I've never seen a movie that wasn't at least somewhat entertaining.\"\n",
    "\n",
    "# NLTK tokenization\n",
    "tokens_nltk = word_tokenize(text_with_contraction)\n",
    "print(\"NLTK tokens:\", tokens_nltk)\n",
    "print(\"\\nNotice how NLTK splits contractions: I've -> ['I', \\\"'ve\\\"]\")\n",
    "\n",
    "# Compare all three on the same input\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Whitespace: \", text_with_contraction.split())\n",
    "print(\"Regex:      \", re.findall(r'\\b\\w+\\b', text_with_contraction.lower()))\n",
    "print(\"NLTK:       \", word_tokenize(text_with_contraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "### 2.3 Preview: Subword Tokenization\n",
    "\n",
    "Classical tokenizers split text into whole words. Modern NLP models (BERT, GPT) use **subword tokenization** which splits words into smaller pieces:\n",
    "\n",
    "- **Byte-Pair Encoding (BPE)**: Used by GPT models. Starts with individual characters and iteratively merges the most frequent pairs.\n",
    "- **WordPiece**: Used by BERT. Similar to BPE but uses a likelihood-based merging criterion.\n",
    "- **SentencePiece**: Language-agnostic, works directly on raw text without pre-tokenization.\n",
    "\n",
    "Example: the word \"unhappiness\" might be tokenized as `[\"un\", \"##happi\", \"##ness\"]` by WordPiece.\n",
    "\n",
    "Subword tokenization solves the **out-of-vocabulary (OOV) problem** -- any word can be represented as a combination of known subword units. We will explore this in depth in later modules on transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "## 3. Stop Words, Stemming & Lemmatization\n",
    "\n",
    "### 3.1 Stop Words\n",
    "\n",
    "**Stop words** are very common words (\"the\", \"is\", \"at\", \"and\") that typically carry little meaning on their own. Removing them reduces noise and dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"NLTK has {len(stop_words)} English stop words.\")\n",
    "print(\"Examples:\", sorted(list(stop_words))[:20])\n",
    "\n",
    "# Filtering stop words\n",
    "sample_tokens = word_tokenize(corpus[0].lower())\n",
    "print(f\"\\nBefore filtering ({len(sample_tokens)} tokens): {sample_tokens}\")\n",
    "\n",
    "filtered = [w for w in sample_tokens if w not in stop_words and w.isalpha()]\n",
    "print(f\"After filtering  ({len(filtered)} tokens): {filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "### 3.2 Stemming\n",
    "\n",
    "**Stemming** reduces words to their root form by chopping off suffixes using heuristic rules. It is fast but sometimes produces non-words.\n",
    "\n",
    "The most common stemmer is the **Porter Stemmer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "stem_examples = ['running', 'runs', 'ran', 'runner', 'better', 'playing', \n",
    "                 'played', 'happiness', 'beautiful', 'beautifully']\n",
    "\n",
    "print(\"Porter Stemmer Examples:\")\n",
    "print(\"-\" * 35)\n",
    "for word in stem_examples:\n",
    "    print(f\"  {word:15s} -> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "### 3.3 Lemmatization\n",
    "\n",
    "**Lemmatization** reduces words to their dictionary form (lemma) using vocabulary and morphological analysis. It is slower but produces real words.\n",
    "\n",
    "The key advantage: with the correct **part-of-speech (POS) tag**, a lemmatizer can map \"better\" to \"good\" (adjective), which a stemmer cannot do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_examples = [\n",
    "    ('running', 'v'),   # verb\n",
    "    ('runs', 'v'),\n",
    "    ('ran', 'v'),\n",
    "    ('better', 'a'),    # adjective\n",
    "    ('playing', 'v'),\n",
    "    ('played', 'v'),\n",
    "    ('happiness', 'n'), # noun\n",
    "    ('beautiful', 'a'),\n",
    "    ('mice', 'n'),\n",
    "    ('geese', 'n'),\n",
    "]\n",
    "\n",
    "print(\"WordNet Lemmatizer Examples:\")\n",
    "print(\"-\" * 45)\n",
    "for word, pos in lemma_examples:\n",
    "    lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "    print(f\"  {word:15s} (pos={pos}) -> {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "### 3.4 Stemming vs. Lemmatization: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_words = ['running', 'better', 'mice', 'happiness', 'studies', \n",
    "                    'beautifully', 'geese', 'played', 'worse', 'feet']\n",
    "pos_tags =         ['v',       'a',      'n',    'n',         'v',\n",
    "                    'r',       'n',       'v',    'a',         'n']\n",
    "\n",
    "print(f\"{'Word':15s} {'Stemmed':15s} {'Lemmatized':15s}\")\n",
    "print(\"-\" * 45)\n",
    "for word, pos in zip(comparison_words, pos_tags):\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos=pos)\n",
    "    print(f\"{word:15s} {stemmed:15s} {lemmatized:15s}\")\n",
    "\n",
    "print(\"\\nKey takeaway: Lemmatization produces real dictionary words.\")\n",
    "print(\"Stemming is faster but can produce non-words (e.g., 'happi', 'beauti').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## 4. Bag-of-Words\n",
    "\n",
    "The **Bag-of-Words (BoW)** model represents text as a vector of word counts. It ignores word order entirely -- only the *frequency* of each word matters.\n",
    "\n",
    "### 4.1 Implement from Scratch\n",
    "\n",
    "Let us build a Bag-of-Words representation step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(documents):\n",
    "    \"\"\"Build a sorted vocabulary from a list of documents.\"\"\"\n",
    "    vocab = set()\n",
    "    for doc in documents:\n",
    "        tokens = re.findall(r'\\b\\w+\\b', doc.lower())\n",
    "        vocab.update(tokens)\n",
    "    return sorted(vocab)\n",
    "\n",
    "\n",
    "def bow_vectorize(documents, vocab):\n",
    "    \"\"\"Create count vectors for each document.\"\"\"\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    vectors = np.zeros((len(documents), len(vocab)), dtype=int)\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        tokens = re.findall(r'\\b\\w+\\b', doc.lower())\n",
    "        for token in tokens:\n",
    "            if token in word_to_idx:\n",
    "                vectors[doc_idx, word_to_idx[token]] += 1\n",
    "    return vectors\n",
    "\n",
    "\n",
    "# Build vocabulary and vectors\n",
    "vocab = build_vocabulary(corpus)\n",
    "bow_matrix = bow_vectorize(corpus, vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"BoW matrix shape: {bow_matrix.shape}  (documents x vocabulary)\")\n",
    "print(f\"\\nFirst 15 vocabulary words: {vocab[:15]}\")\n",
    "print(f\"\\nDocument 0 vector (first 15 dims): {bow_matrix[0, :15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8e9f0",
   "metadata": {},
   "source": [
    "### 4.2 Compare with sklearn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_cv = CountVectorizer()\n",
    "sklearn_bow = sklearn_cv.fit_transform(corpus)\n",
    "\n",
    "print(f\"sklearn BoW matrix shape: {sklearn_bow.shape}\")\n",
    "print(f\"sklearn vocabulary size:  {len(sklearn_cv.vocabulary_)}\")\n",
    "print(f\"Our vocabulary size:      {len(vocab)}\")\n",
    "print(f\"\\nsklearn vocabulary (first 15): {list(sklearn_cv.get_feature_names_out())[:15]}\")\n",
    "\n",
    "# They should be very similar (sklearn may filter single-char tokens)\n",
    "print(f\"\\nsklearn doc 0 vector (first 15): {sklearn_bow.toarray()[0, :15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "### 4.3 Visualize: Word Frequency Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total word frequencies across all documents\n",
    "total_counts = bow_matrix.sum(axis=0)\n",
    "word_freq = sorted(zip(vocab, total_counts), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plot top 20 words\n",
    "top_n = 20\n",
    "words = [w for w, c in word_freq[:top_n]]\n",
    "counts = [c for w, c in word_freq[:top_n]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(range(top_n), counts, color='steelblue')\n",
    "ax.set_xticks(range(top_n))\n",
    "ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "ax.set_xlabel('Word')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Top 20 Words by Frequency (Bag-of-Words)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how common stop words like 'the', 'was', 'and' dominate the counts.\")\n",
    "print(\"This is why stop word removal and TF-IDF weighting are important!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Build a Complete Text Preprocessing Pipeline\n",
    "\n",
    "Implement a function `preprocess(text)` that applies the following steps in order:\n",
    "1. Lowercase the text\n",
    "2. Remove punctuation\n",
    "3. Tokenize using NLTK `word_tokenize`\n",
    "4. Remove stop words\n",
    "5. Lemmatize each token (use POS='v' for verbs as a default -- this handles most inflections well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "      1. Lowercase\n",
    "      2. Remove punctuation\n",
    "      3. Tokenize with word_tokenize\n",
    "      4. Remove stop words\n",
    "      5. Lemmatize (default POS='v')\n",
    "    \n",
    "    Returns: list of processed tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    text = None  # TODO: lowercase the text\n",
    "    \n",
    "    # Step 2: Remove punctuation\n",
    "    text = None  # TODO: remove punctuation using str.translate\n",
    "    \n",
    "    # Step 3: Tokenize\n",
    "    tokens = None  # TODO: tokenize using word_tokenize\n",
    "    \n",
    "    # Step 4: Remove stop words (also keep only alphabetic tokens)\n",
    "    tokens = None  # TODO: filter out stop words and non-alpha tokens\n",
    "    \n",
    "    # Step 5: Lemmatize\n",
    "    tokens = None  # TODO: lemmatize each token with pos='v'\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Test it\n",
    "for doc in corpus:\n",
    "    print(preprocess(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f7",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \n",
    "    Steps:\n",
    "      1. Lowercase\n",
    "      2. Remove punctuation\n",
    "      3. Tokenize with word_tokenize\n",
    "      4. Remove stop words\n",
    "      5. Lemmatize (default POS='v')\n",
    "    \n",
    "    Returns: list of processed tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Step 3: Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Step 4: Remove stop words (also keep only alphabetic tokens)\n",
    "    tokens = [t for t in tokens if t not in stop_words and t.isalpha()]\n",
    "    \n",
    "    # Step 5: Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(t, pos='v') for t in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Test it\n",
    "print(\"Preprocessed corpus:\")\n",
    "print(\"-\" * 60)\n",
    "for i, doc in enumerate(corpus):\n",
    "    processed = preprocess(doc)\n",
    "    print(f\"Doc {i}: {processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b9",
   "metadata": {},
   "source": [
    "## 5. TF-IDF\n",
    "\n",
    "### 5.1 The Problem with Raw Counts\n",
    "\n",
    "Bag-of-Words treats all words equally. But common words like \"the\" and \"was\" appear everywhere and carry little discriminative information. **TF-IDF** (Term Frequency - Inverse Document Frequency) solves this by weighting words based on how *informative* they are.\n",
    "\n",
    "### 5.2 The Formula\n",
    "\n",
    "**TF (Term Frequency):** How often does the term appear in this document?\n",
    "\n",
    "$$\\text{TF}(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total terms in } d}$$\n",
    "\n",
    "**IDF (Inverse Document Frequency):** How rare is this term across all documents?\n",
    "\n",
    "$$\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + \\text{df}(t)}\\right) + 1$$\n",
    "\n",
    "where $N$ is the total number of documents and $\\text{df}(t)$ is the number of documents containing term $t$. The \"+1\" prevents division by zero and ensures non-negative weights.\n",
    "\n",
    "**TF-IDF:**\n",
    "\n",
    "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "Words that appear frequently in a specific document but rarely in other documents get high TF-IDF scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8ca",
   "metadata": {},
   "source": [
    "### 5.3 Implement TF-IDF from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(document_tokens):\n",
    "    \"\"\"Compute term frequency for a single tokenized document.\"\"\"\n",
    "    tf = Counter(document_tokens)\n",
    "    total_terms = len(document_tokens)\n",
    "    # Normalize by total number of terms\n",
    "    for term in tf:\n",
    "        tf[term] = tf[term] / total_terms\n",
    "    return tf\n",
    "\n",
    "\n",
    "def compute_idf(corpus_tokens):\n",
    "    \"\"\"Compute inverse document frequency for all terms in the corpus.\"\"\"\n",
    "    N = len(corpus_tokens)\n",
    "    idf = {}\n",
    "    \n",
    "    # Count how many documents contain each term\n",
    "    all_terms = set()\n",
    "    for doc_tokens in corpus_tokens:\n",
    "        all_terms.update(doc_tokens)\n",
    "    \n",
    "    for term in all_terms:\n",
    "        df = sum(1 for doc_tokens in corpus_tokens if term in doc_tokens)\n",
    "        idf[term] = math.log(N / (1 + df)) + 1\n",
    "    \n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_tfidf_from_scratch(corpus):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF matrix from a list of raw text documents.\n",
    "    Returns: (tfidf_matrix, vocabulary)\n",
    "    \"\"\"\n",
    "    # Tokenize all documents\n",
    "    corpus_tokens = [re.findall(r'\\b\\w+\\b', doc.lower()) for doc in corpus]\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = sorted(set(token for doc in corpus_tokens for token in doc))\n",
    "    word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "    \n",
    "    # Compute IDF\n",
    "    idf = compute_idf(corpus_tokens)\n",
    "    \n",
    "    # Compute TF-IDF for each document\n",
    "    tfidf_matrix = np.zeros((len(corpus), len(vocab)))\n",
    "    for doc_idx, doc_tokens in enumerate(corpus_tokens):\n",
    "        tf = compute_tf(doc_tokens)\n",
    "        for term, tf_val in tf.items():\n",
    "            col_idx = word_to_idx[term]\n",
    "            tfidf_matrix[doc_idx, col_idx] = tf_val * idf[term]\n",
    "    \n",
    "    return tfidf_matrix, vocab\n",
    "\n",
    "\n",
    "# Compute TF-IDF from scratch\n",
    "tfidf_scratch, vocab_scratch = compute_tfidf_from_scratch(corpus)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_scratch.shape}\")\n",
    "print(f\"Vocabulary size: {len(vocab_scratch)}\")\n",
    "print(f\"\\nDoc 0 - top 5 TF-IDF terms:\")\n",
    "doc0_scores = list(zip(vocab_scratch, tfidf_scratch[0]))\n",
    "doc0_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for word, score in doc0_scores[:5]:\n",
    "    print(f\"  {word:15s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e2",
   "metadata": {},
   "source": [
    "### 5.4 Compare with sklearn TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_tfidf = TfidfVectorizer(norm=None)  # norm=None to compare raw values\n",
    "sklearn_tfidf_matrix = sklearn_tfidf.fit_transform(corpus)\n",
    "\n",
    "print(f\"sklearn TF-IDF matrix shape: {sklearn_tfidf_matrix.shape}\")\n",
    "\n",
    "# Show top terms for doc 0 from sklearn\n",
    "feature_names = sklearn_tfidf.get_feature_names_out()\n",
    "doc0_sklearn = list(zip(feature_names, sklearn_tfidf_matrix.toarray()[0]))\n",
    "doc0_sklearn.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nsklearn Doc 0 - top 5 TF-IDF terms:\")\n",
    "for word, score in doc0_sklearn[:5]:\n",
    "    print(f\"  {word:15s} {score:.4f}\")\n",
    "\n",
    "print(\"\\nNote: Values differ slightly because sklearn uses raw counts\")\n",
    "print(\"for TF (not normalized by doc length) by default.\")\n",
    "print(\"The relative ranking of terms should be similar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a4",
   "metadata": {},
   "source": [
    "### 5.5 TF-IDF vs. Raw Counts: Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw counts vs TF-IDF for document 0\n",
    "doc_idx = 0\n",
    "\n",
    "# Get raw counts\n",
    "raw_counts = bow_matrix[doc_idx]\n",
    "raw_top = sorted(zip(vocab, raw_counts), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Get TF-IDF\n",
    "tfidf_scores = tfidf_scratch[doc_idx]\n",
    "tfidf_top = sorted(zip(vocab_scratch, tfidf_scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "axes[0].barh([w for w, c in raw_top], [c for w, c in raw_top], color='steelblue')\n",
    "axes[0].set_title(f'Document {doc_idx}: Raw Word Counts')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# TF-IDF\n",
    "axes[1].barh([w for w, c in tfidf_top], [c for w, c in tfidf_top], color='coral')\n",
    "axes[1].set_title(f'Document {doc_idx}: TF-IDF Scores')\n",
    "axes[1].set_xlabel('TF-IDF Score')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: TF-IDF boosts content-specific words and\")\n",
    "print(\"down-weights words that appear across many documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Implement TF-IDF from Scratch\n",
    "\n",
    "Implement the function `compute_tfidf(corpus)` that:\n",
    "1. Tokenizes each document (lowercase, extract word characters)\n",
    "2. Computes TF for each document\n",
    "3. Computes IDF across the corpus\n",
    "4. Returns a TF-IDF matrix (numpy array) and the vocabulary\n",
    "\n",
    "Then compare your results with sklearn's `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(corpus):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF matrix from scratch.\n",
    "    \n",
    "    Args:\n",
    "        corpus: list of raw text strings\n",
    "    \n",
    "    Returns:\n",
    "        tfidf_matrix: numpy array of shape (n_docs, vocab_size)\n",
    "        vocabulary: sorted list of terms\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize\n",
    "    tokenized = None  # TODO: tokenize each document (lowercase, word chars only)\n",
    "    \n",
    "    # Step 2: Build vocabulary\n",
    "    vocabulary = None  # TODO: sorted list of all unique terms\n",
    "    word_to_idx = None  # TODO: dict mapping word -> column index\n",
    "    \n",
    "    # Step 3: Compute IDF\n",
    "    N = len(corpus)\n",
    "    idf = {}  # TODO: compute IDF for each term using log(N / (1 + df)) + 1\n",
    "    \n",
    "    # Step 4: Compute TF-IDF matrix\n",
    "    tfidf_matrix = None  # TODO: fill in TF * IDF values\n",
    "    \n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "\n",
    "# Test and compare with sklearn\n",
    "my_tfidf, my_vocab = compute_tfidf(corpus)\n",
    "\n",
    "# Print results\n",
    "if my_tfidf is not None:\n",
    "    print(f\"Your TF-IDF matrix shape: {my_tfidf.shape}\")\n",
    "    print(f\"Vocabulary size: {len(my_vocab)}\")\n",
    "else:\n",
    "    print(\"TODO: Complete the implementation above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e8",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(corpus):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF matrix from scratch.\n",
    "    \n",
    "    Args:\n",
    "        corpus: list of raw text strings\n",
    "    \n",
    "    Returns:\n",
    "        tfidf_matrix: numpy array of shape (n_docs, vocab_size)\n",
    "        vocabulary: sorted list of terms\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize\n",
    "    tokenized = [re.findall(r'\\b\\w+\\b', doc.lower()) for doc in corpus]\n",
    "    \n",
    "    # Step 2: Build vocabulary\n",
    "    vocabulary = sorted(set(tok for doc in tokenized for tok in doc))\n",
    "    word_to_idx = {w: i for i, w in enumerate(vocabulary)}\n",
    "    \n",
    "    # Step 3: Compute IDF\n",
    "    N = len(corpus)\n",
    "    idf = {}\n",
    "    for term in vocabulary:\n",
    "        df = sum(1 for doc in tokenized if term in doc)\n",
    "        idf[term] = math.log(N / (1 + df)) + 1\n",
    "    \n",
    "    # Step 4: Compute TF-IDF matrix\n",
    "    tfidf_matrix = np.zeros((N, len(vocabulary)))\n",
    "    for doc_idx, doc_tokens in enumerate(tokenized):\n",
    "        tf = Counter(doc_tokens)\n",
    "        n_terms = len(doc_tokens)\n",
    "        for term, count in tf.items():\n",
    "            col = word_to_idx[term]\n",
    "            tfidf_matrix[doc_idx, col] = (count / n_terms) * idf[term]\n",
    "    \n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "\n",
    "# Compute and display\n",
    "my_tfidf, my_vocab = compute_tfidf(corpus)\n",
    "print(f\"TF-IDF matrix shape: {my_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(my_vocab)}\")\n",
    "\n",
    "# Compare with sklearn\n",
    "print(\"\\n--- Comparison: Top terms for Document 0 ---\")\n",
    "print(f\"{'Term':15s} {'Ours':>10s}  {'sklearn':>10s}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "sklearn_tv = TfidfVectorizer(norm=None)\n",
    "sklearn_mat = sklearn_tv.fit_transform(corpus).toarray()\n",
    "sklearn_vocab = sklearn_tv.get_feature_names_out()\n",
    "\n",
    "# Top 8 from our implementation\n",
    "our_top = sorted(zip(my_vocab, my_tfidf[0]), key=lambda x: x[1], reverse=True)[:8]\n",
    "for word, score in our_top:\n",
    "    # Find sklearn score for same word\n",
    "    sk_idx = list(sklearn_vocab).index(word) if word in sklearn_vocab else -1\n",
    "    sk_score = sklearn_mat[0, sk_idx] if sk_idx >= 0 else 0.0\n",
    "    print(f\"{word:15s} {score:10.4f}  {sk_score:10.4f}\")\n",
    "\n",
    "print(\"\\nNote: Rankings should be similar even if absolute values differ\")\n",
    "print(\"(sklearn uses raw counts for TF, we normalize by document length).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8aa",
   "metadata": {},
   "source": [
    "## 6. N-grams\n",
    "\n",
    "So far, our representations treat each word independently. **N-grams** capture short sequences of consecutive words, preserving some local word order.\n",
    "\n",
    "- **Unigram** (n=1): individual words -- \"the\", \"movie\", \"was\"\n",
    "- **Bigram** (n=2): pairs of words -- \"the movie\", \"movie was\"\n",
    "- **Trigram** (n=3): triples -- \"the movie was\", \"movie was great\"\n",
    "\n",
    "### 6.1 Implement N-gram Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens, n):\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "\n",
    "# Example\n",
    "sample_text = \"the movie was absolutely fantastic and the acting was wonderful\"\n",
    "tokens = sample_text.split()\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    label = {1: 'Unigrams', 2: 'Bigrams', 3: 'Trigrams'}[n]\n",
    "    print(f\"{label} (n={n}):\")\n",
    "    for ng in ngrams:\n",
    "        print(f\"  {' '.join(ng)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c2",
   "metadata": {},
   "source": [
    "### 6.2 Most Common Bigrams and Trigrams in Our Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect n-grams across the entire corpus\n",
    "all_bigrams = []\n",
    "all_trigrams = []\n",
    "\n",
    "for doc in corpus:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', doc.lower())\n",
    "    all_bigrams.extend(extract_ngrams(tokens, 2))\n",
    "    all_trigrams.extend(extract_ngrams(tokens, 3))\n",
    "\n",
    "# Most common\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for bigram, count in Counter(all_bigrams).most_common(10):\n",
    "    print(f\"  {' '.join(bigram):25s} (count: {count})\")\n",
    "\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "for trigram, count in Counter(all_trigrams).most_common(10):\n",
    "    print(f\"  {' '.join(trigram):30s} (count: {count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e4",
   "metadata": {},
   "source": [
    "## 7. Sentiment Classification: TF-IDF + Logistic Regression\n",
    "\n",
    "Now let us put everything together and build a real text classifier. We will use sklearn's built-in **20 Newsgroups** dataset (no external downloads needed) and classify posts into categories.\n",
    "\n",
    "### 7.1 Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 3 distinct categories for a manageable classification task\n",
    "categories = ['sci.space', 'rec.sport.hockey', 'comp.graphics']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train', categories=categories, remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test', categories=categories, remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(newsgroups_train.data)}\")\n",
    "print(f\"Test samples:     {len(newsgroups_test.data)}\")\n",
    "print(f\"Categories:       {newsgroups_train.target_names}\")\n",
    "\n",
    "# Show a sample\n",
    "print(f\"\\n--- Sample document (category: {newsgroups_train.target_names[newsgroups_train.target[0]]}) ---\")\n",
    "print(newsgroups_train.data[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a6",
   "metadata": {},
   "source": [
    "### 7.2 TF-IDF Vectorization + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize with TF-IDF\n",
    "tfidf_vec = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_train = tfidf_vec.fit_transform(newsgroups_train.data)\n",
    "X_test = tfidf_vec.transform(newsgroups_test.data)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "print(f\"TF-IDF feature matrix: {X_train.shape}\")\n",
    "\n",
    "# Train Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c8",
   "metadata": {},
   "source": [
    "### 7.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.set_title('Confusion Matrix: TF-IDF + Logistic Regression')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Add labels\n",
    "tick_marks = range(len(categories))\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_xticklabels(categories, rotation=45, ha='right')\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_yticklabels(categories)\n",
    "\n",
    "# Add numbers in cells\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha='center', va='center',\n",
    "                color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Bag-of-Words vs. TF-IDF Classifier Comparison\n",
    "\n",
    "Train two classifiers on the 20 Newsgroups data:\n",
    "1. **Model A**: `CountVectorizer` (Bag-of-Words) + `LogisticRegression`\n",
    "2. **Model B**: `TfidfVectorizer` (TF-IDF) + `LogisticRegression`\n",
    "\n",
    "Compare their accuracy. Which one performs better and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Compare CountVectorizer vs TfidfVectorizer\n",
    "\n",
    "# Model A: Bag-of-Words + Logistic Regression\n",
    "bow_vectorizer = None  # TODO: create CountVectorizer(max_features=10000, stop_words='english')\n",
    "X_train_bow = None     # TODO: fit_transform on training data\n",
    "X_test_bow = None      # TODO: transform test data\n",
    "\n",
    "clf_bow = None          # TODO: create and train LogisticRegression\n",
    "accuracy_bow = None     # TODO: compute accuracy on test set\n",
    "\n",
    "# Model B: TF-IDF + Logistic Regression\n",
    "tfidf_vectorizer = None  # TODO: create TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_train_tfidf = None     # TODO: fit_transform on training data\n",
    "X_test_tfidf = None      # TODO: transform test data\n",
    "\n",
    "clf_tfidf = None          # TODO: create and train LogisticRegression\n",
    "accuracy_tfidf = None     # TODO: compute accuracy on test set\n",
    "\n",
    "# Compare\n",
    "print(f\"Bag-of-Words accuracy: {accuracy_bow}\")\n",
    "print(f\"TF-IDF accuracy:       {accuracy_tfidf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a2",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Bag-of-Words + Logistic Regression\n",
    "bow_vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "X_train_bow = bow_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_bow = bow_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "clf_bow = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)\n",
    "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "\n",
    "# Model B: TF-IDF + Logistic Regression\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "clf_tfidf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# Compare\n",
    "print(f\"Bag-of-Words accuracy: {accuracy_bow:.4f}\")\n",
    "print(f\"TF-IDF accuracy:       {accuracy_tfidf:.4f}\")\n",
    "print(f\"Difference:            {accuracy_tfidf - accuracy_bow:+.4f}\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "models = ['Bag-of-Words', 'TF-IDF']\n",
    "accuracies = [accuracy_bow, accuracy_tfidf]\n",
    "colors = ['steelblue', 'coral']\n",
    "bars = ax.bar(models, accuracies, color=colors)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('BoW vs TF-IDF: Classification Accuracy')\n",
    "ax.set_ylim(0.8, 1.0)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTF-IDF typically outperforms raw BoW because it down-weights\")\n",
    "print(\"common terms that appear across many documents, giving more\")\n",
    "print(\"importance to discriminative, category-specific terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Analyze N-gram Range Effects on Classifier Performance\n",
    "\n",
    "Train TF-IDF + Logistic Regression classifiers with different `ngram_range` settings:\n",
    "- `(1, 1)` -- unigrams only\n",
    "- `(1, 2)` -- unigrams + bigrams\n",
    "- `(1, 3)` -- unigrams + bigrams + trigrams\n",
    "\n",
    "Plot the accuracies. Does including bigrams and trigrams help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: N-gram range effects\n",
    "\n",
    "ngram_ranges = [(1, 1), (1, 2), (1, 3)]\n",
    "ngram_accuracies = []\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    # TODO: Create TfidfVectorizer with the current ngram_range\n",
    "    vectorizer = None  # TODO\n",
    "    \n",
    "    # TODO: Fit and transform\n",
    "    X_tr = None  # TODO\n",
    "    X_te = None  # TODO\n",
    "    \n",
    "    # TODO: Train classifier and compute accuracy\n",
    "    model = None  # TODO\n",
    "    acc = None    # TODO\n",
    "    \n",
    "    ngram_accuracies.append(acc)\n",
    "    print(f\"ngram_range={ngram_range}: accuracy={acc}\")\n",
    "\n",
    "# TODO: Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e7",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (1, 2), (1, 3)]\n",
    "ngram_labels = ['(1,1)\\nUnigrams', '(1,2)\\n+ Bigrams', '(1,3)\\n+ Trigrams']\n",
    "ngram_accuracies = []\n",
    "ngram_feature_counts = []\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=20000, stop_words='english', ngram_range=ngram_range\n",
    "    )\n",
    "    X_tr = vectorizer.fit_transform(newsgroups_train.data)\n",
    "    X_te = vectorizer.transform(newsgroups_test.data)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_tr, y_train)\n",
    "    y_pred_ng = model.predict(X_te)\n",
    "    acc = accuracy_score(y_test, y_pred_ng)\n",
    "    \n",
    "    ngram_accuracies.append(acc)\n",
    "    ngram_feature_counts.append(X_tr.shape[1])\n",
    "    print(f\"ngram_range={str(ngram_range):8s}  features={X_tr.shape[1]:6d}  accuracy={acc:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "color1 = 'steelblue'\n",
    "bars = ax1.bar(ngram_labels, ngram_accuracies, color=color1, alpha=0.8)\n",
    "ax1.set_ylabel('Accuracy', color=color1)\n",
    "ax1.set_title('Effect of N-gram Range on Classification Accuracy')\n",
    "ax1.set_ylim(0.85, 1.0)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "# Add accuracy labels on bars\n",
    "for bar, acc in zip(bars, ngram_accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.003,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add feature count on secondary axis\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'coral'\n",
    "ax2.plot(ngram_labels, ngram_feature_counts, 'o-', color=color2, linewidth=2, markersize=8)\n",
    "ax2.set_ylabel('Number of Features', color=color2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Adding bigrams often provides a small accuracy boost by capturing\")\n",
    "print(\"  useful two-word phrases (e.g., 'space shuttle', 'graphics card').\")\n",
    "print(\"- Adding trigrams adds many more features but may not help further,\")\n",
    "print(\"  and can lead to overfitting on small datasets.\")\n",
    "print(\"- The tradeoff is always: more n-grams = more features = more memory + compute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a9",
   "metadata": {},
   "source": [
    "## 8. Limitations of Classical NLP\n",
    "\n",
    "The techniques we have covered in this module are powerful baselines, but they have fundamental limitations that motivate the move to modern deep learning approaches.\n",
    "\n",
    "### No Semantic Understanding\n",
    "\n",
    "Bag-of-Words and TF-IDF treat words as **atomic symbols**. They have no notion that \"happy\" and \"joyful\" are related, or that \"bank\" can mean a financial institution or a river bank. Two sentences can have completely different BoW representations but mean the same thing:\n",
    "\n",
    "- \"The film was excellent\" vs. \"The movie was superb\"\n",
    "- These share only \"the\" and \"was\" -- their BoW similarity is very low!\n",
    "\n",
    "### Sparsity of Representations\n",
    "\n",
    "With a vocabulary of 50,000 words, each document is represented as a 50,000-dimensional vector that is mostly zeros. This **sparsity** wastes memory, makes distances meaningless in high dimensions (the \"curse of dimensionality\"), and makes it hard for models to generalize.\n",
    "\n",
    "### Vocabulary Mismatch Problem\n",
    "\n",
    "If a word never appeared in the training data, it gets no representation at all. New words, misspellings, and domain-specific jargon are invisible to the model. This is the **out-of-vocabulary (OOV)** problem.\n",
    "\n",
    "### No Word Order\n",
    "\n",
    "\"The dog bit the man\" and \"The man bit the dog\" have identical BoW representations. N-grams partially address this but only capture very local context.\n",
    "\n",
    "### The Bridge to Word Vectors (Module 4)\n",
    "\n",
    "Word embeddings (Word2Vec, GloVe, and later Transformer-based embeddings) address all of these limitations by learning **dense, low-dimensional vector representations** where semantically similar words are close together in the vector space. In Module 4, we will explore how these representations are learned and why they revolutionized NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7ba",
   "metadata": {},
   "source": [
    "## 9. Summary & References\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Text preprocessing** (lowercasing, tokenization, stop word removal, stemming/lemmatization) is essential for cleaning raw text.\n",
    "2. **Bag-of-Words** represents documents as word count vectors -- simple but effective as a baseline.\n",
    "3. **TF-IDF** improves on BoW by weighting terms based on how informative they are (frequent in the document, rare in the corpus).\n",
    "4. **N-grams** capture local word order (bigrams, trigrams) and can improve classification.\n",
    "5. **Classical ML classifiers** (Logistic Regression) combined with TF-IDF features achieve strong baselines on text classification tasks.\n",
    "6. **Limitations**: No semantic understanding, sparse representations, vocabulary mismatch. These motivate word embeddings and deep learning.\n",
    "\n",
    "### References\n",
    "\n",
    "- **Book**: Jurafsky, D. & Martin, J.H. *Speech and Language Processing* (3rd ed. draft). Chapters 2 (Regular Expressions, Tokenization), 4 (Naive Bayes and Sentiment), 6 (Vector Semantics). Available free online: https://web.stanford.edu/~jurafsky/slp3/\n",
    "- **Book**: Bird, S., Klein, E., & Loper, E. *Natural Language Processing with Python* (NLTK Book). Available free online: https://www.nltk.org/book/\n",
    "- **Course**: Stanford CS224n: Natural Language Processing with Deep Learning, Lectures 1-2. Course materials: https://web.stanford.edu/class/cs224n/\n",
    "- **scikit-learn Documentation**: Text feature extraction -- https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}