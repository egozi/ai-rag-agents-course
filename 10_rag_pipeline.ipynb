{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "# Module 10: RAG Pipeline - Building a Complete Q&A System\n",
    "\n",
    "In this module, we connect **retrieval** (from Module 9) to **generation** to build a full Retrieval-Augmented Generation (RAG) pipeline.\n",
    "\n",
    "**What is RAG?** RAG is an architecture that enhances LLM responses by grounding them in external knowledge. Instead of relying solely on the model's training data, we:\n",
    "\n",
    "1. **Embed the user query** using the same embedding model used for our knowledge base\n",
    "2. **Search a vector database** for the most relevant chunks of text\n",
    "3. **Build a prompt** that includes the retrieved context alongside the user's question\n",
    "4. **Generate an answer** using an LLM that is grounded in the retrieved evidence\n",
    "\n",
    "This end-to-end flow transforms a basic LLM into a knowledge-grounded Q&A system that can answer questions about **your** data.\n",
    "\n",
    "**What we will build in this notebook:**\n",
    "- A complete RAG pipeline from scratch (no frameworks)\n",
    "- Multiple chain strategies: Stuff, Map-Reduce, and Refine\n",
    "- Source attribution so users know where answers come from\n",
    "- Multi-turn conversational RAG with memory\n",
    "- A brief comparison with the LangChain framework\n",
    "\n",
    "The **from-scratch implementation** is the primary focus. LangChain is shown at the end only for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0002-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb openai python-dotenv sentence-transformers langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"/home/amir/source/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Optional\n",
    "\n",
    "# Device detection for embeddings\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Initialize embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "print(f\"Embedding model loaded. Dimension: {embed_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Prepare the Knowledge Base\n",
    "\n",
    "We will create a knowledge base of ~18 detailed paragraphs covering the **History and Concepts of Artificial Intelligence**. This gives us rich, factual content to query against.\n",
    "\n",
    "After defining the documents, we will:\n",
    "1. **Chunk** them using recursive character splitting (as in Module 9)\n",
    "2. **Embed** each chunk using our sentence-transformer model\n",
    "3. **Store** the embeddings in a ChromaDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our knowledge base: detailed paragraphs about AI history and concepts\n",
    "documents = [\n",
    "    \"\"\"The concept of artificial intelligence dates back to antiquity, with myths and stories of artificial beings endowed with intelligence. However, the formal field of AI research was founded at a workshop held at Dartmouth College in the summer of 1956. The workshop was organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. They proposed that 'every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.' This optimistic vision set the stage for decades of research.\"\"\",\n",
    "\n",
    "    \"\"\"Alan Turing, often considered the father of theoretical computer science, made foundational contributions to AI. In his 1950 paper 'Computing Machinery and Intelligence,' he proposed the Turing Test as a measure of machine intelligence. The test involves a human evaluator who converses with both a human and a machine through text. If the evaluator cannot reliably distinguish the machine from the human, the machine is said to have passed the test. While the Turing Test has been debated and criticized over the decades, it remains an influential concept in AI philosophy.\"\"\",\n",
    "\n",
    "    \"\"\"The early years of AI (1956-1974) are sometimes called the 'Golden Age' of AI research. During this period, researchers developed programs that could solve algebra problems, prove logical theorems, and even speak English. The General Problem Solver (GPS), created by Herbert Simon and Allen Newell in 1957, was one of the first programs designed to imitate human problem-solving. ELIZA, created by Joseph Weizenbaum in 1966, simulated a psychotherapist and demonstrated natural language processing, though it relied on pattern matching rather than true understanding.\"\"\",\n",
    "\n",
    "    \"\"\"The first 'AI Winter' occurred roughly between 1974 and 1980. Government funding agencies like DARPA became disillusioned with AI research because early promises of human-level AI had not materialized. The Lighthill Report in the UK (1973) was particularly critical, arguing that AI had failed to achieve its ambitious goals. Funding was drastically cut, and many researchers left the field. This period demonstrated that AI progress would not be linear and that the gap between narrow demonstrations and general intelligence was far wider than initially thought.\"\"\",\n",
    "\n",
    "    \"\"\"Expert systems revived AI in the 1980s and led to a boom in commercial AI. These systems encoded human expert knowledge as if-then rules and could make decisions in specialized domains. MYCIN, developed at Stanford in the 1970s, diagnosed bacterial infections and recommended antibiotics. R1/XCON, used by Digital Equipment Corporation starting in 1980, configured computer orders and saved the company an estimated $40 million per year. By 1985, the AI industry was worth over $1 billion, with companies investing heavily in expert system technology.\"\"\",\n",
    "\n",
    "    \"\"\"The second AI Winter began in the late 1980s and lasted into the mid-1990s. Expert systems proved expensive to maintain, brittle in the face of unexpected inputs, and unable to learn from experience. The collapse of the Lisp machine market and reduced government spending contributed to widespread disillusionment. Many AI companies went bankrupt, and the term 'artificial intelligence' became something of a stigma in grant applications. Researchers rebranded their work under terms like 'machine learning,' 'knowledge-based systems,' or 'computational intelligence.'\"\"\",\n",
    "\n",
    "    \"\"\"Machine learning, a subfield of AI, focuses on algorithms that improve through experience. Rather than programming explicit rules, machine learning systems learn patterns from data. Supervised learning trains on labeled examples (e.g., classifying emails as spam or not spam), unsupervised learning finds hidden structure in unlabeled data (e.g., customer segmentation), and reinforcement learning trains agents through rewards and penalties (e.g., game playing). The shift from rule-based systems to data-driven learning was one of the most important transitions in AI history.\"\"\",\n",
    "\n",
    "    \"\"\"Neural networks, inspired by the structure of the human brain, consist of layers of interconnected nodes (neurons). Each connection has a weight that is adjusted during training. The backpropagation algorithm, popularized by Rumelhart, Hinton, and Williams in 1986, enabled efficient training of multi-layer networks by propagating error gradients backwards through the network. Despite this breakthrough, neural networks fell out of favor in the 1990s due to limited computational power and the difficulty of training deep architectures, which led to problems like vanishing gradients.\"\"\",\n",
    "\n",
    "    \"\"\"Deep learning, a subset of machine learning using neural networks with many layers, revolutionized AI starting around 2012. The breakthrough moment came when AlexNet, a deep convolutional neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, won the ImageNet competition by a large margin. This success was enabled by three factors: large datasets (ImageNet had millions of labeled images), powerful GPUs that could train deep networks efficiently, and algorithmic innovations like ReLU activation functions and dropout regularization.\"\"\",\n",
    "\n",
    "    \"\"\"Convolutional Neural Networks (CNNs) are specialized for processing grid-like data such as images. They use convolutional layers that apply learnable filters across the input, detecting features like edges, textures, and shapes. Pooling layers reduce spatial dimensions, making the network invariant to small translations. CNNs have achieved superhuman performance on many image recognition tasks. Notable architectures include LeNet (1998) for digit recognition, VGGNet (2014) for its simplicity and depth, ResNet (2015) which introduced skip connections to train very deep networks, and EfficientNet (2019) which optimized the trade-off between accuracy and computational cost.\"\"\",\n",
    "\n",
    "    \"\"\"Recurrent Neural Networks (RNNs) are designed for sequential data like text and time series. They maintain a hidden state that acts as memory, allowing information to persist across time steps. However, standard RNNs struggle with long-range dependencies due to vanishing gradients. Long Short-Term Memory (LSTM) networks, invented by Hochreiter and Schmidhuber in 1997, solved this with gating mechanisms that control information flow. Gated Recurrent Units (GRUs), introduced by Cho et al. in 2014, offered a simpler alternative with comparable performance. RNNs dominated natural language processing until the advent of transformers.\"\"\",\n",
    "\n",
    "    \"\"\"The Transformer architecture, introduced in the landmark paper 'Attention Is All You Need' by Vaswani et al. in 2017, replaced recurrence with self-attention mechanisms. Self-attention allows each token in a sequence to attend to every other token, capturing long-range dependencies efficiently. Transformers process all tokens in parallel rather than sequentially, making them much faster to train on modern hardware. The architecture consists of an encoder and decoder, each composed of multi-head attention layers and feed-forward networks with residual connections and layer normalization.\"\"\",\n",
    "\n",
    "    \"\"\"Large Language Models (LLMs) are transformer-based models trained on massive text corpora. GPT (Generative Pre-trained Transformer) by OpenAI demonstrated that pre-training on large amounts of text followed by fine-tuning on specific tasks could achieve state-of-the-art results across many NLP benchmarks. GPT-2 (2019) had 1.5 billion parameters and could generate remarkably coherent text. GPT-3 (2020) scaled to 175 billion parameters and exhibited few-shot learning abilities. GPT-4 (2023) further advanced capabilities with multimodal inputs. BERT, developed by Google in 2018, used bidirectional training and excelled at understanding tasks like question answering and sentiment analysis.\"\"\",\n",
    "\n",
    "    \"\"\"Retrieval-Augmented Generation (RAG) is a technique that enhances LLM responses by incorporating external knowledge retrieved from a database or document collection. Introduced by Lewis et al. in 2020, RAG addresses key limitations of LLMs: hallucination (generating plausible but incorrect information), knowledge cutoff (not knowing about events after training), and lack of source attribution. In a RAG pipeline, a user query is first used to retrieve relevant documents from a knowledge base, then these documents are provided as context to the LLM, which generates a grounded answer. RAG has become a standard pattern for building production AI applications.\"\"\",\n",
    "\n",
    "    \"\"\"Vector databases are specialized systems designed to store and efficiently search high-dimensional vectors (embeddings). Unlike traditional databases that match exact values, vector databases find the most similar vectors using distance metrics like cosine similarity, Euclidean distance, or dot product. Popular vector databases include Pinecone (managed cloud service), Weaviate (open-source with hybrid search), ChromaDB (lightweight and embeddable), Milvus (open-source and highly scalable), and Qdrant (Rust-based with filtering). These databases use approximate nearest neighbor (ANN) algorithms like HNSW or IVF to achieve sub-millisecond search times even with millions of vectors.\"\"\",\n",
    "\n",
    "    \"\"\"Embeddings are dense numerical representations of data (text, images, audio) in a continuous vector space. Text embeddings capture semantic meaning: words and sentences with similar meanings are mapped to nearby points in the embedding space. Early embedding models like Word2Vec (2013) and GloVe (2014) produced word-level vectors. Modern sentence embedding models like Sentence-BERT (2019) and OpenAI's text-embedding-ada-002 produce embeddings for entire sentences or paragraphs. The quality of embeddings is crucial for RAG systems because retrieval accuracy depends directly on how well the embeddings capture semantic similarity.\"\"\",\n",
    "\n",
    "    \"\"\"Prompt engineering is the practice of designing effective prompts to guide LLM behavior. Techniques include zero-shot prompting (giving no examples), few-shot prompting (providing several examples in the prompt), chain-of-thought prompting (asking the model to reason step by step), and system prompts (setting the model's role and behavior). In RAG systems, prompt engineering is critical for the generation step: the prompt must clearly instruct the model to answer based on the provided context, avoid making up information, and cite sources when possible. A well-engineered prompt can dramatically improve answer quality and reduce hallucinations.\"\"\",\n",
    "\n",
    "    \"\"\"Reinforcement Learning from Human Feedback (RLHF) is a technique used to align LLMs with human preferences. The process involves three steps: first, the model is pre-trained on large text corpora using standard language modeling objectives; second, human raters rank model outputs by quality, and a reward model is trained on these rankings; third, the language model is fine-tuned using reinforcement learning (typically PPO - Proximal Policy Optimization) to maximize the reward model's score. RLHF was a key ingredient in making ChatGPT more helpful, harmless, and honest compared to the base GPT model. Constitutional AI (CAI), developed by Anthropic, extends this idea by using AI feedback alongside human feedback.\"\"\",\n",
    "\n",
    "    \"\"\"AI safety and alignment research focuses on ensuring that AI systems behave in ways that are beneficial to humans. Key concerns include the alignment problem (ensuring AI goals match human intentions), robustness (ensuring AI performs reliably in novel situations), interpretability (understanding why AI makes certain decisions), and misuse prevention (stopping AI from being used for harmful purposes). Organizations like Anthropic, OpenAI, DeepMind, and MIRI conduct alignment research. Techniques include RLHF, constitutional AI, red-teaming (adversarial testing), mechanistic interpretability (reverse-engineering neural network computations), and scalable oversight (using AI to help humans supervise more powerful AI systems).\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base: {len(documents)} documents\")\n",
    "print(f\"Sample (first 100 chars): {documents[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0008-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Chunking the Documents\n",
    "\n",
    "We use recursive character splitting to break long documents into smaller, overlapping chunks. This ensures that each chunk fits within the embedding model's context window and that no information is lost at chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_character_split(text: str, chunk_size: int = 300, chunk_overlap: int = 50) -> list[str]:\n",
    "    \"\"\"Split text into overlapping chunks using recursive character splitting.\"\"\"\n",
    "    separators = [\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]\n",
    "    chunks = []\n",
    "\n",
    "    def _split(text: str, sep_idx: int = 0) -> list[str]:\n",
    "        if len(text) <= chunk_size:\n",
    "            return [text.strip()] if text.strip() else []\n",
    "\n",
    "        if sep_idx >= len(separators):\n",
    "            # Last resort: hard split\n",
    "            result = []\n",
    "            for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "                piece = text[i:i + chunk_size].strip()\n",
    "                if piece:\n",
    "                    result.append(piece)\n",
    "            return result\n",
    "\n",
    "        sep = separators[sep_idx]\n",
    "        if sep == \"\":\n",
    "            # Character-level split\n",
    "            result = []\n",
    "            for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "                piece = text[i:i + chunk_size].strip()\n",
    "                if piece:\n",
    "                    result.append(piece)\n",
    "            return result\n",
    "\n",
    "        parts = text.split(sep)\n",
    "        current = \"\"\n",
    "        result = []\n",
    "\n",
    "        for part in parts:\n",
    "            candidate = current + sep + part if current else part\n",
    "            if len(candidate) <= chunk_size:\n",
    "                current = candidate\n",
    "            else:\n",
    "                if current.strip():\n",
    "                    result.append(current.strip())\n",
    "                # If a single part is too large, recursively split it\n",
    "                if len(part) > chunk_size:\n",
    "                    result.extend(_split(part, sep_idx + 1))\n",
    "                    current = \"\"\n",
    "                else:\n",
    "                    # Start new chunk with overlap from the end of previous chunk\n",
    "                    if current and chunk_overlap > 0:\n",
    "                        overlap_text = current[-chunk_overlap:]\n",
    "                        current = overlap_text + sep + part\n",
    "                    else:\n",
    "                        current = part\n",
    "\n",
    "        if current.strip():\n",
    "            result.append(current.strip())\n",
    "\n",
    "        return result\n",
    "\n",
    "    return _split(text)\n",
    "\n",
    "\n",
    "# Chunk all documents and track source metadata\n",
    "all_chunks = []\n",
    "chunk_metadata = []  # Track which document each chunk came from\n",
    "\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    chunks = recursive_character_split(doc, chunk_size=300, chunk_overlap=50)\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        chunk_metadata.append({\n",
    "            \"doc_id\": doc_idx,\n",
    "            \"source\": f\"Document {doc_idx + 1}\",\n",
    "            \"preview\": doc[:80] + \"...\"\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"\\nSample chunk (index 0):\")\n",
    "print(f\"  Text: {all_chunks[0][:150]}...\")\n",
    "print(f\"  Metadata: {chunk_metadata[0]}\")\n",
    "print(f\"\\nSample chunk (index 5):\")\n",
    "print(f\"  Text: {all_chunks[5][:150]}...\")\n",
    "print(f\"  Metadata: {chunk_metadata[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Embed and Store in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all chunks\n",
    "print(\"Embedding chunks...\")\n",
    "chunk_embeddings = embed_model.encode(all_chunks, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {chunk_embeddings.shape}\")\n",
    "\n",
    "# Create ChromaDB collection\n",
    "chroma_client = chromadb.Client()  # In-memory client\n",
    "\n",
    "# Delete collection if it already exists (for re-running)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"ai_knowledge_base\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"ai_knowledge_base\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Add chunks to ChromaDB\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(all_chunks))],\n",
    "    embeddings=chunk_embeddings.tolist(),\n",
    "    documents=all_chunks,\n",
    "    metadatas=chunk_metadata\n",
    ")\n",
    "\n",
    "print(f\"\\nChromaDB collection '{collection.name}' created with {collection.count()} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0012-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Building RAG from Scratch (No Framework)\n",
    "\n",
    "Now we build the complete RAG pipeline step by step. No LangChain, no abstractions -- just Python, ChromaDB, and the OpenAI API.\n",
    "\n",
    "### Step 1: Embed the User Query\n",
    "\n",
    "We embed the user's question using the **same embedding model** that was used for the knowledge base. This ensures the query vector lives in the same space as the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0013-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> list[float]:\n",
    "    \"\"\"Embed a user query using the same model as the knowledge base.\"\"\"\n",
    "    embedding = embed_model.encode(query)\n",
    "    return embedding.tolist()\n",
    "\n",
    "# Example\n",
    "test_query = \"What is the Transformer architecture?\"\n",
    "query_vec = embed_query(test_query)\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Embedding dimension: {len(query_vec)}\")\n",
    "print(f\"First 5 values: {query_vec[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0014-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Step 2: Retrieve Top-k Relevant Chunks from ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0015-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, collection, k: int = 3) -> dict:\n",
    "    \"\"\"Retrieve the top-k most relevant chunks for a query.\"\"\"\n",
    "    query_embedding = embed_query(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example retrieval\n",
    "results = retrieve(\"What is the Transformer architecture?\", collection, k=3)\n",
    "\n",
    "print(\"Retrieved chunks:\")\n",
    "for i, (doc, meta, dist) in enumerate(zip(\n",
    "    results[\"documents\"][0],\n",
    "    results[\"metadatas\"][0],\n",
    "    results[\"distances\"][0]\n",
    ")):\n",
    "    print(f\"\\n--- Chunk {i+1} (distance: {dist:.4f}, source: {meta['source']}) ---\")\n",
    "    print(doc[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0016-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Step 3: Build the Prompt with Context + Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0017-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query: str, context_chunks: list[str]) -> str:\n",
    "    \"\"\"Build a prompt that includes retrieved context and the user's question.\"\"\"\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[Context {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the user's question based ONLY on the provided context.\n",
    "If the context does not contain enough information to answer the question, say \"I don't have enough information to answer this question.\"\n",
    "Do not make up information that is not supported by the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "sample_prompt = build_prompt(\n",
    "    \"What is the Transformer architecture?\",\n",
    "    results[\"documents\"][0]\n",
    ")\n",
    "print(sample_prompt[:500] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0018-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Step 4: Call OpenAI API to Generate the Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0019-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt: str, model: str = \"gpt-4o-mini\", temperature: float = 0.2) -> str:\n",
    "    \"\"\"Generate an answer using OpenAI's chat completions API.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example generation\n",
    "answer = generate_answer(sample_prompt)\n",
    "print(\"Question: What is the Transformer architecture?\")\n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0020-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Complete RAG Flow\n",
    "\n",
    "Now let us put all four steps together into one function and show the complete flow with retrieved context printed alongside the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0021-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, collection, k: int = 3, verbose: bool = True) -> str:\n",
    "    \"\"\"Complete RAG pipeline: retrieve, build prompt, generate answer.\"\"\"\n",
    "    # Step 1 & 2: Retrieve relevant chunks\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "    distances = results[\"distances\"][0]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nRetrieved {len(context_chunks)} chunks:\")\n",
    "        for i, (chunk, meta, dist) in enumerate(zip(context_chunks, metadatas, distances)):\n",
    "            print(f\"  [{i+1}] (distance: {dist:.4f}, {meta['source']}) {chunk[:100]}...\")\n",
    "\n",
    "    # Step 3: Build prompt\n",
    "    prompt = build_prompt(question, context_chunks)\n",
    "\n",
    "    # Step 4: Generate answer\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nAnswer:\\n{answer}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Test with several questions\n",
    "questions = [\n",
    "    \"What caused the first AI Winter?\",\n",
    "    \"How does RAG work and why is it useful?\",\n",
    "    \"What is the difference between CNNs and RNNs?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    rag_query(q, collection)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0022-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a RAG System\n",
    "\n",
    "Implement your own `rag_query_exercise` function that:\n",
    "1. Takes a question and a ChromaDB collection\n",
    "2. Retrieves the top-k relevant chunks\n",
    "3. Builds a prompt with the context\n",
    "4. Generates and returns the answer\n",
    "\n",
    "Test it with the question: *\"What is RLHF and how does it work?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0023-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query_exercise(question: str, collection, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    TODO: Implement a complete RAG pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Embed the question using embed_query()\n",
    "    2. Query the collection for top-k results\n",
    "    3. Build a prompt with the retrieved context\n",
    "    4. Generate and return the answer\n",
    "    \"\"\"\n",
    "    # TODO: Retrieve relevant chunks\n",
    "    retrieved = None\n",
    "\n",
    "    # TODO: Extract document texts from results\n",
    "    context_chunks = None\n",
    "\n",
    "    # TODO: Build the prompt\n",
    "    prompt = None\n",
    "\n",
    "    # TODO: Generate and return the answer\n",
    "    answer = None\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Test\n",
    "# answer = rag_query_exercise(\"What is RLHF and how does it work?\", collection)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0024-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0025-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query_exercise(question: str, collection, k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve context, build prompt, generate answer.\n",
    "    \"\"\"\n",
    "    # Step 1 & 2: Retrieve relevant chunks\n",
    "    query_embedding = embed_query(question)\n",
    "    retrieved = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Extract document texts\n",
    "    context_chunks = retrieved[\"documents\"][0]\n",
    "\n",
    "    # Step 3: Build the prompt\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[Context {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "    )\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
    "If the context does not contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 4: Generate the answer\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Test\n",
    "answer = rag_query_exercise(\"What is RLHF and how does it work?\", collection)\n",
    "print(\"Question: What is RLHF and how does it work?\")\n",
    "print(f\"\\nAnswer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0026-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Chain Strategies\n",
    "\n",
    "When we retrieve multiple chunks, how should we combine them and send them to the LLM? There are three common strategies:\n",
    "\n",
    "| Strategy | Approach | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **Stuff** | Concatenate all chunks into one prompt | Simple, one API call | Limited by context window |\n",
    "| **Map-Reduce** | Summarize each chunk, then combine summaries | Handles many chunks | Multiple API calls, slower |\n",
    "| **Refine** | Iteratively refine answer with each chunk | Preserves detail | Sequential, slowest |\n",
    "\n",
    "Let us implement each one from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0027-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### 4.1 Stuff Method\n",
    "\n",
    "The simplest approach: concatenate all retrieved chunks into a single prompt.\n",
    "\n",
    "**Pros:** Simple, one API call, preserves all context.  \n",
    "**Cons:** Can exceed the LLM's context window if too many chunks are retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0028-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuff_chain(question: str, collection, k: int = 5) -> str:\n",
    "    \"\"\"Stuff method: concatenate all chunks into one prompt.\"\"\"\n",
    "    # Retrieve chunks\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "\n",
    "    # Stuff all chunks into a single context\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[Passage {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are a knowledgeable AI assistant. Answer the question thoroughly based ONLY on the provided passages.\n",
    "If the passages do not contain enough information, say so.\n",
    "\n",
    "Passages:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer:\"\"\"\n",
    "\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Test\n",
    "question = \"Explain the evolution of neural networks from simple models to deep learning.\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "answer = stuff_chain(question, collection, k=5)\n",
    "print(f\"[Stuff Method] Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0029-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### 4.2 Map-Reduce Method\n",
    "\n",
    "Two-phase approach:\n",
    "1. **Map:** Summarize each retrieved chunk independently (in parallel, conceptually)\n",
    "2. **Reduce:** Combine all summaries into a final answer\n",
    "\n",
    "**Pros:** Can handle a large number of chunks without exceeding context limits.  \n",
    "**Cons:** Multiple API calls (one per chunk + one final), slower and more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0030-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_chain(question: str, collection, k: int = 5) -> str:\n",
    "    \"\"\"Map-Reduce method: summarize each chunk, then combine summaries.\"\"\"\n",
    "    # Retrieve chunks\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "\n",
    "    # MAP phase: extract relevant information from each chunk\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(context_chunks):\n",
    "        map_prompt = f\"\"\"Given the following passage and a question, extract ONLY the information from the passage that is relevant to answering the question.\n",
    "If the passage contains no relevant information, respond with \"No relevant information.\"\n",
    "\n",
    "Passage: {chunk}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant information:\"\"\"\n",
    "\n",
    "        summary = generate_answer(map_prompt)\n",
    "        summaries.append(summary)\n",
    "        print(f\"  Map step {i+1}/{len(context_chunks)} complete.\")\n",
    "\n",
    "    # REDUCE phase: combine all summaries into a final answer\n",
    "    combined_summaries = \"\\n\\n\".join(\n",
    "        f\"[Summary {i+1}]: {s}\" for i, s in enumerate(summaries)\n",
    "    )\n",
    "\n",
    "    reduce_prompt = f\"\"\"You are a knowledgeable AI assistant. Based on the following summaries extracted from multiple sources, provide a comprehensive answer to the question.\n",
    "Synthesize the information and avoid repetition.\n",
    "\n",
    "Summaries:\n",
    "{combined_summaries}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Comprehensive answer:\"\"\"\n",
    "\n",
    "    answer = generate_answer(reduce_prompt)\n",
    "    print(\"  Reduce step complete.\")\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Test\n",
    "question = \"Explain the evolution of neural networks from simple models to deep learning.\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Running Map-Reduce chain...\")\n",
    "answer = map_reduce_chain(question, collection, k=5)\n",
    "print(f\"\\n[Map-Reduce Method] Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0031-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### 4.3 Refine Method\n",
    "\n",
    "Iterative approach: start with the first chunk, generate an initial answer, then refine it with each subsequent chunk.\n",
    "\n",
    "**Pros:** Can incorporate nuanced details from each chunk progressively.  \n",
    "**Cons:** Strictly sequential (each step depends on the previous), slowest of the three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0032-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_chain(question: str, collection, k: int = 5) -> str:\n",
    "    \"\"\"Refine method: iteratively refine the answer with each chunk.\"\"\"\n",
    "    # Retrieve chunks\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "\n",
    "    # Initial answer from the first chunk\n",
    "    initial_prompt = f\"\"\"You are a knowledgeable AI assistant. Answer the question based on the provided context.\n",
    "If the context is insufficient, provide what you can and note what is missing.\n",
    "\n",
    "Context: {context_chunks[0]}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    current_answer = generate_answer(initial_prompt)\n",
    "    print(f\"  Initial answer generated from chunk 1.\")\n",
    "\n",
    "    # Refine with each subsequent chunk\n",
    "    for i, chunk in enumerate(context_chunks[1:], start=2):\n",
    "        refine_prompt = f\"\"\"You are a knowledgeable AI assistant. You have an existing answer to a question, and now you have additional context.\n",
    "Refine the existing answer by incorporating any new relevant information from the additional context.\n",
    "If the new context is not relevant, keep the existing answer unchanged.\n",
    "Do not remove correct information from the existing answer.\n",
    "\n",
    "Existing answer: {current_answer}\n",
    "\n",
    "Additional context: {chunk}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Refined answer:\"\"\"\n",
    "\n",
    "        current_answer = generate_answer(refine_prompt)\n",
    "        print(f\"  Refined with chunk {i}/{len(context_chunks)}.\")\n",
    "\n",
    "    return current_answer\n",
    "\n",
    "\n",
    "# Test\n",
    "question = \"Explain the evolution of neural networks from simple models to deep learning.\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Running Refine chain...\")\n",
    "answer = refine_chain(question, collection, k=5)\n",
    "print(f\"\\n[Refine Method] Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0033-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Comparing the Three Strategies\n",
    "\n",
    "Let us run the same question through all three methods and compare the outputs side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0034-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_question = \"What are the key milestones in AI history?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Comparison Question: {comparison_question}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Stuff\n",
    "print(\"\\n--- STUFF METHOD ---\")\n",
    "stuff_answer = stuff_chain(comparison_question, collection, k=5)\n",
    "print(stuff_answer)\n",
    "\n",
    "# Map-Reduce\n",
    "print(\"\\n--- MAP-REDUCE METHOD ---\")\n",
    "mr_answer = map_reduce_chain(comparison_question, collection, k=5)\n",
    "print(mr_answer)\n",
    "\n",
    "# Refine\n",
    "print(\"\\n--- REFINE METHOD ---\")\n",
    "refine_answer = refine_chain(comparison_question, collection, k=5)\n",
    "print(refine_answer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Observation: All three methods produce similar content but differ in\")\n",
    "print(\"structure and detail. Stuff is fastest, Map-Reduce handles scale,\")\n",
    "print(\"and Refine produces the most refined, iteratively improved answers.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0035-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Stuff and Map-Reduce from Scratch\n",
    "\n",
    "Implement both the **Stuff** and **Map-Reduce** chain strategies yourself. Then run the same question through both and compare the quality of answers.\n",
    "\n",
    "Question to test: *\"What are vector databases and why are they important for AI?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0036-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuff_chain_exercise(question: str, collection, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    TODO: Implement the Stuff chain strategy.\n",
    "\n",
    "    Steps:\n",
    "    1. Retrieve top-k chunks\n",
    "    2. Concatenate all chunks into a single context string\n",
    "    3. Build a prompt with the combined context + question\n",
    "    4. Generate and return the answer\n",
    "    \"\"\"\n",
    "    # TODO: Retrieve chunks\n",
    "    results = None\n",
    "\n",
    "    # TODO: Build combined context\n",
    "    context = None\n",
    "\n",
    "    # TODO: Build prompt and generate answer\n",
    "    answer = None\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def map_reduce_chain_exercise(question: str, collection, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    TODO: Implement the Map-Reduce chain strategy.\n",
    "\n",
    "    Steps:\n",
    "    1. Retrieve top-k chunks\n",
    "    2. MAP: For each chunk, extract relevant information (one API call per chunk)\n",
    "    3. REDUCE: Combine all extracted summaries into a final answer (one API call)\n",
    "    \"\"\"\n",
    "    # TODO: Retrieve chunks\n",
    "    results = None\n",
    "\n",
    "    # TODO: MAP phase - summarize each chunk\n",
    "    summaries = None\n",
    "\n",
    "    # TODO: REDUCE phase - combine summaries into final answer\n",
    "    answer = None\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Test\n",
    "# test_q = \"What are vector databases and why are they important for AI?\"\n",
    "# print(\"Stuff:\", stuff_chain_exercise(test_q, collection))\n",
    "# print(\"\\nMap-Reduce:\", map_reduce_chain_exercise(test_q, collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0037-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0038-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuff_chain_exercise(question: str, collection, k: int = 5) -> str:\n",
    "    \"\"\"Stuff method: put all chunks into one prompt.\"\"\"\n",
    "    # Retrieve\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "\n",
    "    # Build combined context\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[Passage {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "    )\n",
    "\n",
    "    # Build prompt and generate\n",
    "    prompt = f\"\"\"Answer the question based ONLY on the provided passages.\n",
    "\n",
    "Passages:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def map_reduce_chain_exercise(question: str, collection, k: int = 5) -> str:\n",
    "    \"\"\"Map-Reduce method: summarize each chunk, then combine.\"\"\"\n",
    "    # Retrieve\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "\n",
    "    # MAP: extract relevant info from each chunk\n",
    "    summaries = []\n",
    "    for chunk in context_chunks:\n",
    "        map_prompt = f\"\"\"Extract information relevant to the question from this passage.\n",
    "If nothing is relevant, say \"No relevant information.\"\n",
    "\n",
    "Passage: {chunk}\n",
    "Question: {question}\n",
    "\n",
    "Relevant information:\"\"\"\n",
    "        summary = generate_answer(map_prompt)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # REDUCE: combine summaries\n",
    "    combined = \"\\n\\n\".join(f\"[{i+1}]: {s}\" for i, s in enumerate(summaries))\n",
    "    reduce_prompt = f\"\"\"Synthesize the following extracted information into a comprehensive answer.\n",
    "\n",
    "Extracted information:\n",
    "{combined}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    answer = generate_answer(reduce_prompt)\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Test and compare\n",
    "test_q = \"What are vector databases and why are they important for AI?\"\n",
    "print(f\"Question: {test_q}\\n\")\n",
    "\n",
    "print(\"--- STUFF ---\")\n",
    "print(stuff_chain_exercise(test_q, collection))\n",
    "\n",
    "print(\"\\n--- MAP-REDUCE ---\")\n",
    "print(map_reduce_chain_exercise(test_q, collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0039-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Source Attribution\n",
    "\n",
    "A production RAG system should tell users **where** each answer came from. This builds trust and lets users verify the information.\n",
    "\n",
    "We will modify our RAG pipeline to:\n",
    "1. Number each retrieved context passage\n",
    "2. Instruct the LLM to cite sources using bracket notation like [1], [2]\n",
    "3. Display the cited sources below the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0040-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query_with_sources(question: str, collection, k: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    RAG pipeline with source attribution.\n",
    "    Returns a dict with 'answer' and 'sources'.\n",
    "    \"\"\"\n",
    "    # Retrieve\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "    # Build numbered context\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "    )\n",
    "\n",
    "    # Prompt instructs the model to cite sources\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided numbered sources.\n",
    "IMPORTANT: Cite your sources by including the source number in square brackets (e.g., [1], [2]) after the relevant statements in your answer.\n",
    "Every factual claim should have at least one citation.\n",
    "If the sources do not contain enough information, say so.\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (with citations):\"\"\"\n",
    "\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "    # Build source references\n",
    "    sources = []\n",
    "    for i, (chunk, meta) in enumerate(zip(context_chunks, metadatas)):\n",
    "        sources.append({\n",
    "            \"id\": i + 1,\n",
    "            \"source\": meta[\"source\"],\n",
    "            \"text\": chunk[:150] + \"...\"\n",
    "        })\n",
    "\n",
    "    return {\"answer\": answer, \"sources\": sources}\n",
    "\n",
    "\n",
    "def display_answer_with_sources(result: dict):\n",
    "    \"\"\"Pretty-print an answer with its sources.\"\"\"\n",
    "    print(\"Answer:\")\n",
    "    print(result[\"answer\"])\n",
    "    print(\"\\n--- Sources ---\")\n",
    "    for src in result[\"sources\"]:\n",
    "        print(f\"  [{src['id']}] ({src['source']}) {src['text']}\")\n",
    "\n",
    "\n",
    "# Test\n",
    "result = rag_query_with_sources(\"What is the Turing Test?\", collection, k=3)\n",
    "display_answer_with_sources(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0041-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "result = rag_query_with_sources(\n",
    "    \"What are the different types of machine learning?\",\n",
    "    collection,\n",
    "    k=4\n",
    ")\n",
    "display_answer_with_sources(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0042-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 3: Add Source Attribution\n",
    "\n",
    "Implement `rag_with_citations` that:\n",
    "1. Retrieves relevant chunks\n",
    "2. Includes source numbers in the prompt\n",
    "3. Instructs the LLM to cite sources with [1], [2], etc.\n",
    "4. Returns both the answer and a list of source references\n",
    "\n",
    "Test with: *\"How do expert systems work and why did they fall out of favor?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0043-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_citations(question: str, collection, k: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    TODO: Implement RAG with source attribution.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "        - 'answer': str with inline [1], [2] citations\n",
    "        - 'sources': list of dicts with 'id', 'source', 'text'\n",
    "    \"\"\"\n",
    "    # TODO: Retrieve chunks\n",
    "    results = None\n",
    "\n",
    "    # TODO: Build numbered context for the prompt\n",
    "    context = None\n",
    "\n",
    "    # TODO: Build prompt that instructs the LLM to cite sources\n",
    "    prompt = None\n",
    "\n",
    "    # TODO: Generate answer\n",
    "    answer = None\n",
    "\n",
    "    # TODO: Build source references list\n",
    "    sources = None\n",
    "\n",
    "    return {\"answer\": answer, \"sources\": sources}\n",
    "\n",
    "\n",
    "# Test\n",
    "# result = rag_with_citations(\"How do expert systems work and why did they fall out of favor?\", collection)\n",
    "# display_answer_with_sources(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0044-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0045-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_citations(question: str, collection, k: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    RAG with source attribution using inline citations.\n",
    "    \"\"\"\n",
    "    # Retrieve chunks\n",
    "    results = retrieve(question, collection, k=k)\n",
    "    context_chunks = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "    # Build numbered context\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "    )\n",
    "\n",
    "    # Build prompt with citation instructions\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the question using ONLY the provided numbered sources.\n",
    "Cite sources inline using bracket notation like [1], [2], [3] after relevant statements.\n",
    "Every factual claim must include at least one citation.\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer with inline citations:\"\"\"\n",
    "\n",
    "    # Generate answer\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "    # Build source references\n",
    "    sources = []\n",
    "    for i, (chunk, meta) in enumerate(zip(context_chunks, metadatas)):\n",
    "        sources.append({\n",
    "            \"id\": i + 1,\n",
    "            \"source\": meta[\"source\"],\n",
    "            \"text\": chunk[:150] + \"...\"\n",
    "        })\n",
    "\n",
    "    return {\"answer\": answer, \"sources\": sources}\n",
    "\n",
    "\n",
    "# Test\n",
    "result = rag_with_citations(\n",
    "    \"How do expert systems work and why did they fall out of favor?\",\n",
    "    collection,\n",
    "    k=4\n",
    ")\n",
    "display_answer_with_sources(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0046-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Multi-turn Conversation\n",
    "\n",
    "Real Q&A systems need to handle follow-up questions. A user might ask:\n",
    "1. \"What is deep learning?\"\n",
    "2. \"When did it become popular?\" (refers to \"deep learning\" from Q1)\n",
    "3. \"Who were the key researchers?\" (still about deep learning)\n",
    "\n",
    "To support this, we:\n",
    "- Maintain a **conversation history** (list of previous Q&A pairs)\n",
    "- Include conversation history in the prompt so the LLM can resolve references\n",
    "- Still retrieve fresh context for each new question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0047-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationRAG:\n",
    "    \"\"\"RAG system with multi-turn conversation memory.\"\"\"\n",
    "\n",
    "    def __init__(self, collection, embed_model, openai_client, k: int = 3, max_history: int = 10):\n",
    "        self.collection = collection\n",
    "        self.embed_model = embed_model\n",
    "        self.client = openai_client\n",
    "        self.k = k\n",
    "        self.max_history = max_history\n",
    "        self.history = []  # List of {\"role\": \"user\"/\"assistant\", \"content\": ...}\n",
    "\n",
    "    def _format_history(self) -> str:\n",
    "        \"\"\"Format conversation history for the prompt.\"\"\"\n",
    "        if not self.history:\n",
    "            return \"No previous conversation.\"\n",
    "        lines = []\n",
    "        for entry in self.history[-self.max_history:]:\n",
    "            role = \"User\" if entry[\"role\"] == \"user\" else \"Assistant\"\n",
    "            lines.append(f\"{role}: {entry['content']}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def ask(self, question: str, verbose: bool = True) -> str:\n",
    "        \"\"\"Ask a question with conversation context.\"\"\"\n",
    "        # Retrieve relevant chunks for the current question\n",
    "        query_embedding = self.embed_model.encode(question).tolist()\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=self.k,\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "        context_chunks = results[\"documents\"][0]\n",
    "\n",
    "        # Build context string\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"[Source {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "        )\n",
    "\n",
    "        # Build the full prompt with history\n",
    "        history_str = self._format_history()\n",
    "\n",
    "        prompt = f\"\"\"You are a helpful AI assistant engaged in a conversation. Use the provided sources to answer the current question.\n",
    "Consider the conversation history to understand context and resolve pronouns or references (e.g., \"it\", \"they\", \"that\").\n",
    "Answer based ONLY on the provided sources. If the sources are insufficient, say so.\n",
    "\n",
    "Conversation History:\n",
    "{history_str}\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        # Generate\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful conversational AI assistant. Answer questions based on provided sources and conversation history.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "\n",
    "        # Update conversation history\n",
    "        self.history.append({\"role\": \"user\", \"content\": question})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"User: {question}\")\n",
    "            print(f\"\\nAssistant: {answer}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Conversation history cleared.\")\n",
    "\n",
    "\n",
    "# Demo: multi-turn conversation\n",
    "print(\"=\" * 60)\n",
    "print(\"Multi-turn Conversation Demo\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "conv = ConversationRAG(collection, embed_model, client, k=3)\n",
    "\n",
    "# Turn 1\n",
    "conv.ask(\"What is deep learning?\")\n",
    "\n",
    "# Turn 2 - references \"it\" from Turn 1\n",
    "conv.ask(\"When did it become popular and what triggered its success?\")\n",
    "\n",
    "# Turn 3 - references \"the researchers\" from Turn 2\n",
    "conv.ask(\"Who were the key researchers involved?\")\n",
    "\n",
    "# Turn 4 - follow-up about a related topic\n",
    "conv.ask(\"How did this lead to modern large language models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0048-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Exercise 4: Multi-turn Conversational RAG\n",
    "\n",
    "Implement a `ConversationRAGExercise` class that:\n",
    "1. Maintains a list of previous Q&A pairs as conversation history\n",
    "2. Includes the conversation history in each prompt\n",
    "3. Retrieves fresh context for each question\n",
    "4. Has an `ask(question)` method that returns the answer\n",
    "\n",
    "Test it with a 3-4 turn conversation about AI safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0049-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationRAGExercise:\n",
    "    \"\"\"\n",
    "    TODO: Implement a multi-turn conversational RAG system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, collection, embed_model, openai_client, k: int = 3):\n",
    "        self.collection = collection\n",
    "        self.embed_model = embed_model\n",
    "        self.client = openai_client\n",
    "        self.k = k\n",
    "        # TODO: Initialize conversation history\n",
    "        self.history = None\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        TODO: Implement the ask method.\n",
    "\n",
    "        Steps:\n",
    "        1. Retrieve relevant chunks for the question\n",
    "        2. Format conversation history\n",
    "        3. Build a prompt with history + context + question\n",
    "        4. Generate the answer\n",
    "        5. Add the Q&A pair to history\n",
    "        6. Return the answer\n",
    "        \"\"\"\n",
    "        # TODO: Retrieve relevant chunks\n",
    "        context_chunks = None\n",
    "\n",
    "        # TODO: Format history string\n",
    "        history_str = None\n",
    "\n",
    "        # TODO: Build prompt with history + context + question\n",
    "        prompt = None\n",
    "\n",
    "        # TODO: Generate answer\n",
    "        answer = None\n",
    "\n",
    "        # TODO: Update history\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = None\n",
    "\n",
    "\n",
    "# Test\n",
    "# conv = ConversationRAGExercise(collection, embed_model, client)\n",
    "# conv.ask(\"What is AI safety?\")\n",
    "# conv.ask(\"What techniques are used to address it?\")\n",
    "# conv.ask(\"Which organizations are leading this research?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0050-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0051-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationRAGExercise:\n",
    "    \"\"\"Multi-turn conversational RAG system.\"\"\"\n",
    "\n",
    "    def __init__(self, collection, embed_model, openai_client, k: int = 3):\n",
    "        self.collection = collection\n",
    "        self.embed_model = embed_model\n",
    "        self.client = openai_client\n",
    "        self.k = k\n",
    "        self.history = []\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ask a question with full conversation context.\"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        query_embedding = self.embed_model.encode(question).tolist()\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=self.k,\n",
    "            include=[\"documents\"]\n",
    "        )\n",
    "        context_chunks = results[\"documents\"][0]\n",
    "\n",
    "        # Format history\n",
    "        if self.history:\n",
    "            history_str = \"\\n\".join(\n",
    "                f\"{'User' if h['role'] == 'user' else 'Assistant'}: {h['content']}\"\n",
    "                for h in self.history\n",
    "            )\n",
    "        else:\n",
    "            history_str = \"No previous conversation.\"\n",
    "\n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"[Source {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)\n",
    "        )\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"You are a helpful conversational AI assistant. Use the sources to answer the question.\n",
    "Use the conversation history to resolve references like \"it\", \"they\", \"that topic\".\n",
    "Answer based ONLY on the sources provided.\n",
    "\n",
    "Conversation History:\n",
    "{history_str}\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        # Generate answer\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful conversational assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "\n",
    "        # Update history\n",
    "        self.history.append({\"role\": \"user\", \"content\": question})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        print(f\"User: {question}\")\n",
    "        print(f\"\\nAssistant: {answer}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Conversation history cleared.\")\n",
    "\n",
    "\n",
    "# Demo conversation about AI safety\n",
    "print(\"=\" * 60)\n",
    "print(\"Multi-turn Conversation: AI Safety\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "conv_exercise = ConversationRAGExercise(collection, embed_model, client)\n",
    "\n",
    "# Turn 1\n",
    "conv_exercise.ask(\"What is AI safety and alignment?\")\n",
    "\n",
    "# Turn 2 - \"it\" refers to AI safety\n",
    "conv_exercise.ask(\"What techniques are used to address it?\")\n",
    "\n",
    "# Turn 3 - \"this research\" refers to AI safety research\n",
    "conv_exercise.ask(\"Which organizations are leading this research?\")\n",
    "\n",
    "# Turn 4 - deeper follow-up\n",
    "conv_exercise.ask(\"How does RLHF specifically help with alignment?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0052-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Using LangChain (Comparison)\n",
    "\n",
    "Now that we have built everything from scratch, let us see how LangChain simplifies the same pipeline. This section is a brief comparison -- our from-scratch version gives us more control and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0053-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0054-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain documents from our knowledge base\n",
    "lc_docs = [\n",
    "    Document(page_content=doc, metadata={\"source\": f\"Document {i+1}\"})\n",
    "    for i, doc in enumerate(documents)\n",
    "]\n",
    "\n",
    "# Split using LangChain's splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "lc_chunks = text_splitter.split_documents(lc_docs)\n",
    "print(f\"LangChain created {len(lc_chunks)} chunks\")\n",
    "\n",
    "# Create a Chroma vector store using OpenAI embeddings\n",
    "lc_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "lc_vectorstore = Chroma.from_documents(\n",
    "    documents=lc_chunks,\n",
    "    embedding=lc_embeddings,\n",
    "    collection_name=\"ai_knowledge_base_lc\"\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Same as our stuff method\n",
    "    retriever=lc_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"LangChain RetrievalQA chain created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0055-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with LangChain\n",
    "lc_question = \"What is the Transformer architecture and why was it important?\"\n",
    "\n",
    "lc_result = qa_chain.invoke({\"query\": lc_question})\n",
    "\n",
    "print(f\"Question: {lc_question}\\n\")\n",
    "print(f\"Answer:\\n{lc_result['result']}\")\n",
    "print(f\"\\nSource documents used: {len(lc_result['source_documents'])}\")\n",
    "for i, doc in enumerate(lc_result['source_documents']):\n",
    "    print(f\"  [{i+1}] ({doc.metadata.get('source', 'N/A')}) {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0056-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "### From-Scratch vs LangChain: Comparison\n",
    "\n",
    "| Aspect | From Scratch | LangChain |\n",
    "|--------|-------------|----------|\n",
    "| **Lines of code** | ~30-50 for basic RAG | ~10-15 |\n",
    "| **Customization** | Full control over every step | Limited to available chain types and parameters |\n",
    "| **Prompt engineering** | Write your own prompts | Uses built-in prompt templates |\n",
    "| **Debugging** | Easy -- you see every step | Harder -- abstraction layers hide details |\n",
    "| **Chain strategies** | Implement exactly what you need | `chain_type` parameter: \"stuff\", \"map_reduce\", \"refine\" |\n",
    "| **Learning value** | High -- understand the full pipeline | Lower -- learn the API, not the concepts |\n",
    "| **Production readiness** | Requires more engineering | Provides many utilities out of the box |\n",
    "\n",
    "**Recommendation:** Start by building from scratch to understand the concepts (as we did in this notebook). Then use LangChain or similar frameworks when you need to move fast in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0057-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary and References\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "In this module, you built a **complete RAG system from scratch**:\n",
    "\n",
    "1. **Knowledge Base Preparation**: Chunked documents and stored embeddings in ChromaDB\n",
    "2. **Core RAG Pipeline**: Query embedding, vector search, context building, LLM generation\n",
    "3. **Chain Strategies**: Implemented Stuff (simple), Map-Reduce (scalable), and Refine (iterative) approaches\n",
    "4. **Source Attribution**: Added numbered citations so users can verify answers\n",
    "5. **Multi-turn Conversations**: Added memory so follow-up questions work naturally\n",
    "6. **LangChain Comparison**: Saw how frameworks simplify the code but trade off control\n",
    "\n",
    "The from-scratch approach gives you deep understanding of every component. When building production systems, you can choose to use frameworks like LangChain for convenience or keep your custom implementation for maximum control.\n",
    "\n",
    "### References\n",
    "\n",
    "- **Documentation**: [LangChain - Question Answering](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- **Course**: [DeepLearning.AI - Building and Evaluating Advanced RAG](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)\n",
    "- **Blog**: [LlamaIndex - Building RAG from Scratch](https://docs.llamaindex.ai/en/stable/optimizing/building_rag_from_scratch/)\n",
    "- **Paper**: Gao et al., \"Retrieval-Augmented Generation for Large Language Models: A Survey\" (2023) - [arXiv:2312.10997](https://arxiv.org/abs/2312.10997)\n",
    "- **Paper**: Lewis et al., \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (2020) - [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)\n",
    "- **ChromaDB Documentation**: [https://docs.trychroma.com/](https://docs.trychroma.com/)\n",
    "- **Sentence Transformers**: [https://www.sbert.net/](https://www.sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0058-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}