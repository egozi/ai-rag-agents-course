{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: RAG Foundations - Vector Databases & Retrieval\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is one of the most impactful patterns in modern AI engineering. Instead of relying solely on what an LLM memorized during training, RAG allows us to **ground** model responses in real, up-to-date, domain-specific information.\n",
    "\n",
    "## Why RAG?\n",
    "\n",
    "Large Language Models have three fundamental limitations:\n",
    "\n",
    "1. **Knowledge cutoff** - LLMs only know what was in their training data. They cannot answer questions about events or documents created after their training date.\n",
    "2. **Hallucination** - When an LLM does not know an answer, it may confidently generate plausible-sounding but incorrect information.\n",
    "3. **No domain-specific knowledge** - General-purpose LLMs lack deep expertise in your company's proprietary data, internal documents, or niche domains.\n",
    "\n",
    "RAG solves all three problems by **retrieving** relevant documents and **providing them as context** to the LLM before generation.\n",
    "\n",
    "## RAG Architecture Overview\n",
    "\n",
    "```\n",
    "                         RAG Pipeline\n",
    "                         ============\n",
    "\n",
    "  User Query\n",
    "      |\n",
    "      v\n",
    " +-----------+     +-----------+     +----------------+     +-----------+\n",
    " |           |     |  Vector   |     |                |     |           |\n",
    " |   Embed   |---->|  Database |---->|   Augment      |---->|  Generate |\n",
    " |   Query   |     |  Search   |     |   Prompt with  |     |  Answer   |\n",
    " |           |     |  (Top-K)  |     |   Retrieved    |     |  (LLM)    |\n",
    " +-----------+     +-----------+     |   Documents    |     +-----------+\n",
    "                         ^           +----------------+           |\n",
    "                         |                                        v\n",
    "                   +------------+                           Final Response\n",
    "                   | Document   |                           (Grounded in\n",
    "                   | Embeddings |                            real data)\n",
    "                   | (offline)  |\n",
    "                   +------------+\n",
    "```\n",
    "\n",
    "### What we will cover in this notebook:\n",
    "1. Why RAG matters (with a live demo)\n",
    "2. Document loading and chunking strategies\n",
    "3. Embedding documents for semantic search\n",
    "4. Vector databases with ChromaDB\n",
    "5. Distance metrics explained\n",
    "6. Metadata filtering\n",
    "7. Landscape of vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup\n",
    "\n",
    "First, let's install the required packages and load our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb openai cohere python-dotenv datasets sentence-transformers matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"/home/amir/source/.env\")\n",
    "\n",
    "# Core libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vector database\n",
    "import chromadb\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Why RAG? Seeing the Problem First-Hand\n",
    "\n",
    "Let's demonstrate the core problem RAG solves. We will:\n",
    "1. Ask an LLM about information it **cannot** know (fictional company data)\n",
    "2. Show how providing context fixes the answer\n",
    "\n",
    "Even without calling an LLM API, we can illustrate this principle clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating the RAG problem\n",
    "# Imagine asking an LLM: \"What were NovaTech Solutions' Q3 2025 revenue figures?\"\n",
    "# The LLM would either hallucinate numbers or say it doesn't know.\n",
    "\n",
    "# Without context (what a naive LLM might do):\n",
    "query = \"What were NovaTech Solutions' Q3 2025 revenue figures?\"\n",
    "naive_response = (\n",
    "    \"I don't have specific information about NovaTech Solutions' Q3 2025 \"\n",
    "    \"revenue figures. My training data may not include this information. \"\n",
    "    \"Please check their latest earnings report or SEC filings.\"\n",
    ")\n",
    "print(\"QUERY:\", query)\n",
    "print(\"\\nNAIVE LLM RESPONSE (no RAG):\")\n",
    "print(naive_response)\n",
    "\n",
    "# Now, with context provided (RAG approach):\n",
    "context_document = \"\"\"\n",
    "NovaTech Solutions Q3 2025 Earnings Report:\n",
    "- Total Revenue: $4.2 billion (up 23% YoY)\n",
    "- Cloud Services Revenue: $2.8 billion (up 31% YoY)\n",
    "- Enterprise Software Revenue: $1.1 billion (up 12% YoY)\n",
    "- Professional Services Revenue: $0.3 billion (down 5% YoY)\n",
    "- Net Income: $680 million\n",
    "- Adjusted EBITDA: $1.1 billion\n",
    "\"\"\"\n",
    "\n",
    "augmented_response = (\n",
    "    \"Based on NovaTech Solutions' Q3 2025 earnings report, their total revenue \"\n",
    "    \"was $4.2 billion, representing a 23% year-over-year increase. Cloud Services \"\n",
    "    \"led the growth at $2.8 billion (up 31% YoY), followed by Enterprise Software \"\n",
    "    \"at $1.1 billion (up 12% YoY). Net income was $680 million.\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nRAG-AUGMENTED RESPONSE (with retrieved context):\")\n",
    "print(augmented_response)\n",
    "print(\"\\n--- Retrieved Context ---\")\n",
    "print(context_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** The LLM's response quality is dramatically improved when we provide relevant context. The entire RAG pipeline exists to **automatically find and provide that context** for any given query.\n",
    "\n",
    "The rest of this notebook focuses on building that retrieval mechanism step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Document Loading\n",
    "\n",
    "The first step in any RAG pipeline is loading the source documents. In production you would load from PDFs, web pages, databases, APIs, etc. Here we use a curated set of sample documents about AI/ML topics for portability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base - 15 paragraphs about AI/ML topics\n",
    "# In production, these would come from files, databases, APIs, etc.\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc_01\",\n",
    "        \"text\": \"Transformer architectures have revolutionized natural language processing since their introduction in the 2017 paper 'Attention Is All You Need' by Vaswani et al. The key innovation is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input when producing each part of the output. Unlike recurrent neural networks (RNNs) that process tokens sequentially, transformers can process all tokens in parallel, making them significantly faster to train on modern GPU hardware. The original transformer uses an encoder-decoder structure, but subsequent models have used encoder-only (BERT) or decoder-only (GPT) variants.\",\n",
    "        \"category\": \"architectures\",\n",
    "        \"source\": \"textbook\",\n",
    "        \"year\": 2023\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_02\",\n",
    "        \"text\": \"Retrieval-Augmented Generation (RAG) combines the strengths of retrieval-based and generative AI systems. In a RAG pipeline, a user query is first used to search a knowledge base for relevant documents. These documents are then provided as context to a large language model, which generates a response grounded in the retrieved information. RAG significantly reduces hallucination compared to pure generative approaches, because the model can reference actual source documents rather than relying solely on parametric knowledge stored during training.\",\n",
    "        \"category\": \"rag\",\n",
    "        \"source\": \"research_paper\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_03\",\n",
    "        \"text\": \"Vector databases are purpose-built storage systems optimized for storing, indexing, and querying high-dimensional vector embeddings. Unlike traditional databases that search by exact matches or keyword overlap, vector databases find items by semantic similarity. Popular vector databases include Pinecone (cloud-native), ChromaDB (lightweight, open-source), FAISS (Facebook's library for efficient similarity search), Weaviate (open-source with hybrid search), and Qdrant (Rust-based, high performance). The choice of vector database depends on scale, deployment requirements, and feature needs.\",\n",
    "        \"category\": \"infrastructure\",\n",
    "        \"source\": \"blog_post\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_04\",\n",
    "        \"text\": \"Embedding models convert text into dense numerical vectors that capture semantic meaning. Words or sentences with similar meanings will have vectors that are close together in the embedding space. Modern embedding models like OpenAI's text-embedding-ada-002, Cohere's embed-v3, and open-source alternatives like all-MiniLM-L6-v2 from Sentence Transformers can produce high-quality embeddings for similarity search. The dimensionality of embeddings varies: MiniLM produces 384-dimensional vectors, while OpenAI's ada-002 produces 1536-dimensional vectors.\",\n",
    "        \"category\": \"embeddings\",\n",
    "        \"source\": \"textbook\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_05\",\n",
    "        \"text\": \"Fine-tuning is the process of taking a pre-trained model and further training it on a smaller, task-specific dataset. This allows the model to adapt its general knowledge to perform well on specific domains or tasks. Common fine-tuning approaches include full fine-tuning (updating all parameters), LoRA (Low-Rank Adaptation, which adds small trainable matrices), and QLoRA (quantized LoRA, which reduces memory requirements). Fine-tuning a 7B parameter model with LoRA can be done on a single consumer GPU with 16GB of VRAM, making it accessible to individual practitioners.\",\n",
    "        \"category\": \"training\",\n",
    "        \"source\": \"tutorial\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_06\",\n",
    "        \"text\": \"Chunking is a critical preprocessing step in RAG pipelines. Long documents must be split into smaller pieces (chunks) before embedding, because embedding models have token limits and because smaller chunks lead to more precise retrieval. Common chunking strategies include: fixed-size chunking (splitting every N characters with optional overlap), sentence-based chunking (splitting on sentence boundaries), and recursive chunking (trying larger separators first, then falling back to smaller ones). The optimal chunk size depends on the use case, but 256-512 tokens is a common starting point.\",\n",
    "        \"category\": \"rag\",\n",
    "        \"source\": \"tutorial\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_07\",\n",
    "        \"text\": \"Large Language Models (LLMs) are neural networks trained on massive text corpora to predict the next token in a sequence. Models like GPT-4, Claude, Llama, and Gemini have demonstrated remarkable capabilities in understanding and generating human language. These models typically have billions of parameters and are trained on trillions of tokens of text data. The training process involves two main phases: pre-training on general text data (unsupervised), and alignment through techniques like RLHF (Reinforcement Learning from Human Feedback) or DPO (Direct Preference Optimization).\",\n",
    "        \"category\": \"architectures\",\n",
    "        \"source\": \"textbook\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_08\",\n",
    "        \"text\": \"Cosine similarity is the most commonly used distance metric for comparing text embeddings. It measures the cosine of the angle between two vectors, ranging from -1 (opposite) to 1 (identical direction). Cosine similarity is preferred over Euclidean distance for text because it is invariant to vector magnitude, focusing purely on direction. This means two documents about the same topic will have high cosine similarity regardless of their length. Other useful metrics include dot product (which factors in magnitude) and Euclidean distance (L2 norm of the difference vector).\",\n",
    "        \"category\": \"embeddings\",\n",
    "        \"source\": \"textbook\",\n",
    "        \"year\": 2023\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_09\",\n",
    "        \"text\": \"Prompt engineering is the practice of crafting effective prompts to guide LLM behavior. Key techniques include: zero-shot prompting (no examples), few-shot prompting (providing examples in the prompt), chain-of-thought (asking the model to reason step by step), and system prompts (setting the model's role and behavior guidelines). In RAG systems, prompt engineering is crucial for the generation step. The retrieved documents must be formatted clearly in the prompt, and the model should be instructed to base its answer only on the provided context to minimize hallucination.\",\n",
    "        \"category\": \"techniques\",\n",
    "        \"source\": \"tutorial\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_10\",\n",
    "        \"text\": \"Attention mechanisms allow neural networks to focus on the most relevant parts of the input when generating each part of the output. In the transformer architecture, multi-head attention splits the input into multiple parallel attention operations (heads), each learning to attend to different types of relationships. Self-attention computes attention scores between all pairs of positions in a sequence, creating a weighted representation where each token incorporates information from all other tokens. The computational cost of self-attention is O(n^2) with respect to sequence length, which has motivated research into efficient attention variants like sparse attention and linear attention.\",\n",
    "        \"category\": \"architectures\",\n",
    "        \"source\": \"research_paper\",\n",
    "        \"year\": 2023\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_11\",\n",
    "        \"text\": \"Evaluation of RAG systems requires measuring both retrieval quality and generation quality. Retrieval metrics include precision@k (fraction of top-k results that are relevant), recall@k (fraction of relevant documents found in top-k), and Mean Reciprocal Rank (MRR). Generation metrics include faithfulness (does the answer stick to retrieved context?), relevance (does the answer address the question?), and completeness (does the answer cover all aspects?). Automated evaluation frameworks like RAGAS provide standardized metrics for RAG evaluation.\",\n",
    "        \"category\": \"rag\",\n",
    "        \"source\": \"research_paper\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_12\",\n",
    "        \"text\": \"Transfer learning is the technique of leveraging knowledge gained from one task to improve performance on a different but related task. In NLP, this typically involves using a model pre-trained on a large general corpus and then adapting it to a specific downstream task. BERT popularized this approach by showing that a single pre-trained model could be fine-tuned to achieve state-of-the-art results on a wide range of NLP benchmarks. The success of transfer learning depends on the similarity between the pre-training domain and the target domain.\",\n",
    "        \"category\": \"training\",\n",
    "        \"source\": \"textbook\",\n",
    "        \"year\": 2023\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_13\",\n",
    "        \"text\": \"Semantic search goes beyond keyword matching to understand the intent and meaning behind a query. Traditional search engines rely on BM25 and TF-IDF, which match documents based on word overlap. Semantic search uses embedding models to represent both queries and documents as vectors, then finds the nearest neighbors in vector space. This means a search for 'automobile' will also find documents about 'cars' and 'vehicles', even if those exact words do not appear in the query. Hybrid search combines both approaches: semantic similarity for understanding meaning and keyword matching for precision on specific terms.\",\n",
    "        \"category\": \"rag\",\n",
    "        \"source\": \"blog_post\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_14\",\n",
    "        \"text\": \"Quantization reduces the memory footprint and computational cost of neural networks by representing weights with lower precision numbers. Common quantization levels include FP16 (16-bit floating point), INT8 (8-bit integer), and INT4 (4-bit integer). A 70B parameter model requires about 140GB in FP16 but only 35GB in INT4, making it possible to run on consumer hardware. Techniques like GPTQ and AWQ provide post-training quantization with minimal quality loss. The GGUF format, used by llama.cpp, has become a popular standard for distributing quantized models.\",\n",
    "        \"category\": \"infrastructure\",\n",
    "        \"source\": \"tutorial\",\n",
    "        \"year\": 2024\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_15\",\n",
    "        \"text\": \"AI agents are systems that use LLMs as reasoning engines to autonomously plan and execute multi-step tasks. An agent typically has access to tools (APIs, databases, code execution) and uses a loop of observation, reasoning, and action. Popular agent frameworks include LangChain's AgentExecutor, AutoGPT, and CrewAI. The ReAct (Reasoning + Acting) pattern is a common approach where the agent alternates between thinking about what to do and taking actions. Key challenges in agent design include error recovery, cost control, and preventing infinite loops.\",\n",
    "        \"category\": \"techniques\",\n",
    "        \"source\": \"blog_post\",\n",
    "        \"year\": 2024\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"\\nCategories: {sorted(set(d['category'] for d in documents))}\")\n",
    "print(f\"Sources: {sorted(set(d['source'] for d in documents))}\")\n",
    "print(f\"\\nSample document (first 150 chars):\")\n",
    "print(f\"  [{documents[0]['id']}] {documents[0]['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on real-world document loading:** In production, you would use document loaders to ingest content from various sources:\n",
    "\n",
    "- **PDFs**: `PyPDFLoader`, `UnstructuredPDFLoader` (via LangChain or custom)\n",
    "- **Web pages**: `BeautifulSoup`, `Trafilatura`, `WebBaseLoader`\n",
    "- **Databases**: Direct SQL queries or ORM exports\n",
    "- **APIs**: REST/GraphQL endpoints returning text data\n",
    "- **Office docs**: `python-docx`, `openpyxl`\n",
    "\n",
    "LangChain provides over 100 document loaders: https://python.langchain.com/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Chunking Strategies\n",
    "\n",
    "Before we can embed documents, we often need to split them into smaller **chunks**. Why?\n",
    "\n",
    "1. **Embedding models have token limits** (typically 256-512 tokens)\n",
    "2. **Smaller chunks = more precise retrieval** (a full 10-page document would dilute the relevant information)\n",
    "3. **LLM context windows are limited** (we want to pack the most relevant information)\n",
    "\n",
    "Let's implement three common chunking strategies from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A longer sample document for demonstrating chunking\n",
    "long_document = \"\"\"Artificial intelligence has a long and fascinating history that spans several decades of research and development. The field was formally founded at the Dartmouth Conference in 1956, where researchers like John McCarthy, Marvin Minsky, Allen Newell, and Herbert Simon gathered to discuss the possibility of creating intelligent machines.\n",
    "\n",
    "In the early years, AI research focused on symbolic approaches, where knowledge was represented using rules and logic. Expert systems became popular in the 1980s, encoding domain knowledge from human experts into rule-based systems. These systems found commercial success in medical diagnosis, financial analysis, and manufacturing.\n",
    "\n",
    "The field experienced several periods of reduced funding and interest, known as AI winters. The first AI winter occurred in the 1970s when early promises of AI went unfulfilled. The second AI winter happened in the late 1980s and early 1990s when expert systems proved too expensive to maintain and too brittle to handle real-world complexity.\n",
    "\n",
    "The modern era of AI began with the resurgence of neural networks, particularly deep learning. The breakthrough came in 2012 when AlexNet, a deep convolutional neural network, won the ImageNet competition by a large margin. This demonstrated that deep neural networks, trained on large datasets with GPU acceleration, could achieve superhuman performance on visual recognition tasks.\n",
    "\n",
    "Natural language processing saw its own revolution with the introduction of the transformer architecture in 2017. The paper 'Attention Is All You Need' by Vaswani et al. introduced the self-attention mechanism, which allowed models to process entire sequences in parallel rather than sequentially. This led to a series of increasingly powerful language models.\n",
    "\n",
    "BERT, introduced by Google in 2018, demonstrated the power of pre-training on large text corpora followed by fine-tuning on specific tasks. GPT-2 and GPT-3 from OpenAI showed that scaling up language models led to emergent capabilities, including few-shot learning where the model could perform tasks with just a few examples in the prompt.\n",
    "\n",
    "The release of ChatGPT in November 2022 brought AI into the mainstream consciousness. Built on GPT-3.5 and later GPT-4, ChatGPT demonstrated that large language models could engage in natural conversations, write code, analyze data, and assist with a wide range of tasks. This triggered an explosion of interest and investment in AI.\n",
    "\n",
    "Today, AI research is advancing rapidly across multiple fronts. Multi-modal models can process text, images, audio, and video. Retrieval-augmented generation grounds LLM responses in factual data. AI agents can autonomously plan and execute complex tasks using tools. The field continues to evolve at an unprecedented pace, raising both excitement about possibilities and important questions about safety and ethics.\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(long_document)} characters\")\n",
    "print(f\"Approximate words: {len(long_document.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Fixed-Size Chunking\n",
    "\n",
    "The simplest approach: split every N characters, with optional overlap to preserve context across chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_size_chunk(text, chunk_size=300, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into fixed-size chunks with overlap.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Number of overlapping characters between consecutive chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap  # step back by overlap amount\n",
    "    return chunks\n",
    "\n",
    "fixed_chunks = fixed_size_chunk(long_document, chunk_size=300, overlap=50)\n",
    "print(f\"Fixed-size chunking: {len(fixed_chunks)} chunks\")\n",
    "for i, chunk in enumerate(fixed_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how fixed-size chunks can cut right through the middle of a sentence or even a word. This is the main downside of this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Sentence-Based Chunking\n",
    "\n",
    "Split on sentence boundaries to keep complete thoughts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_chunk(text, max_sentences=3):\n",
    "    \"\"\"\n",
    "    Split text into chunks of N sentences each.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to chunk\n",
    "        max_sentences: Maximum sentences per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation followed by a space\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), max_sentences):\n",
    "        chunk = ' '.join(sentences[i:i + max_sentences])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "sentence_chunks = sentence_chunk(long_document, max_sentences=3)\n",
    "print(f\"Sentence-based chunking: {len(sentence_chunks)} chunks\")\n",
    "for i, chunk in enumerate(sentence_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Recursive Character Chunking\n",
    "\n",
    "This strategy tries to split on natural boundaries in order of preference: paragraphs first, then sentences, then words. This is the approach used by LangChain's `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunk(text, chunk_size=500, separators=None):\n",
    "    \"\"\"\n",
    "    Recursively split text using a hierarchy of separators.\n",
    "    Tries the most meaningful separator first (paragraph breaks),\n",
    "    then falls back to smaller separators.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        separators: Ordered list of separators to try\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    \n",
    "    # Base case: text is small enough\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text] if text.strip() else []\n",
    "    \n",
    "    # Try each separator in order\n",
    "    for sep in separators:\n",
    "        if sep in text:\n",
    "            parts = text.split(sep)\n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for part in parts:\n",
    "                # If adding this part would exceed chunk_size\n",
    "                test_chunk = current_chunk + sep + part if current_chunk else part\n",
    "                \n",
    "                if len(test_chunk) <= chunk_size:\n",
    "                    current_chunk = test_chunk\n",
    "                else:\n",
    "                    # Save current chunk and start new one\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                    \n",
    "                    # If the part itself is too large, recurse with next separator\n",
    "                    if len(part) > chunk_size:\n",
    "                        remaining_seps = separators[separators.index(sep) + 1:]\n",
    "                        if remaining_seps:\n",
    "                            sub_chunks = recursive_chunk(part, chunk_size, remaining_seps)\n",
    "                            chunks.extend(sub_chunks)\n",
    "                            current_chunk = \"\"\n",
    "                        else:\n",
    "                            # Last resort: hard split\n",
    "                            chunks.append(part[:chunk_size].strip())\n",
    "                            current_chunk = part[chunk_size:]\n",
    "                    else:\n",
    "                        current_chunk = part\n",
    "            \n",
    "            if current_chunk.strip():\n",
    "                chunks.append(current_chunk.strip())\n",
    "            \n",
    "            return chunks\n",
    "    \n",
    "    # Fallback: hard split\n",
    "    return [text[i:i+chunk_size].strip() for i in range(0, len(text), chunk_size) if text[i:i+chunk_size].strip()]\n",
    "\n",
    "recursive_chunks = recursive_chunk(long_document, chunk_size=500)\n",
    "print(f\"Recursive chunking: {len(recursive_chunks)} chunks\")\n",
    "for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Chunk Size Distributions\n",
    "\n",
    "Let's compare the three strategies by looking at their chunk size distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "strategies = {\n",
    "    \"Fixed-Size (300 chars, 50 overlap)\": fixed_chunks,\n",
    "    \"Sentence-Based (3 per chunk)\": sentence_chunks,\n",
    "    \"Recursive (500 char limit)\": recursive_chunks\n",
    "}\n",
    "\n",
    "colors = [\"#2196F3\", \"#4CAF50\", \"#FF9800\"]\n",
    "\n",
    "for ax, (name, chunks), color in zip(axes, strategies.items(), colors):\n",
    "    sizes = [len(c) for c in chunks]\n",
    "    ax.bar(range(len(sizes)), sizes, color=color, alpha=0.7)\n",
    "    ax.set_title(name, fontsize=10)\n",
    "    ax.set_xlabel(\"Chunk Index\")\n",
    "    ax.set_ylabel(\"Characters\")\n",
    "    ax.axhline(y=np.mean(sizes), color='red', linestyle='--', alpha=0.7, label=f\"Mean: {np.mean(sizes):.0f}\")\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Chunk Size Distribution by Strategy\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"{'Strategy':<40} {'Count':>6} {'Mean':>8} {'Std':>8} {'Min':>6} {'Max':>6}\")\n",
    "print(\"-\" * 76)\n",
    "for name, chunks in strategies.items():\n",
    "    sizes = [len(c) for c in chunks]\n",
    "    print(f\"{name:<40} {len(sizes):>6} {np.mean(sizes):>8.1f} {np.std(sizes):>8.1f} {min(sizes):>6} {max(sizes):>6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement and Compare Chunking Strategies\n",
    "\n",
    "Implement three chunking functions and compare their behavior on the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement the three chunking strategies\n",
    "\n",
    "exercise_text = \"\"\"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Instead of being explicitly programmed with rules, these systems identify patterns in data and make decisions with minimal human intervention.\n",
    "\n",
    "Supervised learning is the most common type of machine learning. In supervised learning, the model is trained on labeled data, where each input comes with the correct output. The model learns to map inputs to outputs and can then make predictions on new, unseen data. Common algorithms include linear regression, decision trees, and neural networks.\n",
    "\n",
    "Unsupervised learning works with unlabeled data. The model tries to find hidden patterns or structures in the data without being told what to look for. Clustering algorithms like K-means group similar data points together. Dimensionality reduction techniques like PCA and t-SNE help visualize high-dimensional data.\n",
    "\n",
    "Reinforcement learning is inspired by behavioral psychology. An agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions and learns to maximize cumulative reward over time. This approach has achieved remarkable results in game playing, robotics, and resource optimization.\"\"\"\n",
    "\n",
    "# TODO: Implement fixed_size_chunk_ex that splits by character count with overlap\n",
    "def fixed_size_chunk_ex(text, size=200, overlap=30):\n",
    "    result = None\n",
    "    return result\n",
    "\n",
    "# TODO: Implement sentence_chunk_ex that splits on sentence boundaries\n",
    "def sentence_chunk_ex(text, sentences_per_chunk=2):\n",
    "    result = None\n",
    "    return result\n",
    "\n",
    "# TODO: Implement recursive_chunk_ex that tries \\n\\n, then \\n, then '. ', then ' '\n",
    "def recursive_chunk_ex(text, size=300):\n",
    "    result = None\n",
    "    return result\n",
    "\n",
    "# Compare results\n",
    "# for name, func in [(\"Fixed\", fixed_size_chunk_ex), (\"Sentence\", sentence_chunk_ex), (\"Recursive\", recursive_chunk_ex)]:\n",
    "#     chunks = func(exercise_text)\n",
    "#     print(f\"\\n{name}: {len(chunks)} chunks\")\n",
    "#     for i, c in enumerate(chunks):\n",
    "#         print(f\"  Chunk {i} ({len(c)} chars): {c[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Exercise 1\n",
    "\n",
    "def fixed_size_chunk_ex(text, size=200, overlap=30):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "def sentence_chunk_ex(text, sentences_per_chunk=2):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = ' '.join(sentences[i:i + sentences_per_chunk]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def recursive_chunk_ex(text, size=300):\n",
    "    return recursive_chunk(text, chunk_size=size, separators=[\"\\n\\n\", \"\\n\", \". \", \" \"])\n",
    "\n",
    "# Compare results\n",
    "for name, func in [(\"Fixed\", fixed_size_chunk_ex), (\"Sentence\", sentence_chunk_ex), (\"Recursive\", recursive_chunk_ex)]:\n",
    "    chunks = func(exercise_text)\n",
    "    print(f\"\\n{name}: {len(chunks)} chunks\")\n",
    "    for i, c in enumerate(chunks):\n",
    "        preview = c[:80] + \"...\" if len(c) > 80 else c\n",
    "        print(f\"  Chunk {i} ({len(c)} chars): {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Embedding Documents\n",
    "\n",
    "Now that we have our documents (and know how to chunk them), we need to convert them into numerical vectors that capture their semantic meaning. We will use **sentence-transformers** with the `all-MiniLM-L6-v2` model, which is:\n",
    "\n",
    "- **Free** - no API key required\n",
    "- **Fast** - small model (80MB), runs on CPU\n",
    "- **Good quality** - 384-dimensional embeddings, strong performance on semantic similarity\n",
    "\n",
    "> **Alternatives:** For production systems, you might use OpenAI's `text-embedding-3-small` (API, 1536 dims), Cohere's `embed-english-v3.0` (API, 1024 dims), or larger local models like `all-mpnet-base-v2` (768 dims)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"Model loaded: all-MiniLM-L6-v2\")\n",
    "print(f\"Max sequence length: {embedding_model.max_seq_length}\")\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed our documents\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nEmbedded {len(embeddings)} documents\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dtype: {embeddings.dtype}\")\n",
    "print(f\"\\nFirst embedding (first 10 dimensions): {embeddings[0][:10]}\")\n",
    "print(f\"Embedding norm (L2): {np.linalg.norm(embeddings[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore embedding properties\n",
    "\n",
    "# 1. Embeddings are normalized (unit vectors) for this model\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(\"Embedding norms (should be ~1.0 for normalized models):\")\n",
    "print(f\"  Mean: {norms.mean():.4f}, Std: {norms.std():.6f}\")\n",
    "\n",
    "# 2. Semantic similarity: compare two related documents\n",
    "# doc_01 (transformers) vs doc_09 (attention mechanisms) - should be similar\n",
    "sim_related = np.dot(embeddings[0], embeddings[9])  # cosine sim (since normalized)\n",
    "# doc_01 (transformers) vs doc_13 (quantization) - less related\n",
    "sim_unrelated = np.dot(embeddings[0], embeddings[13])\n",
    "\n",
    "print(f\"\\nSimilarity between 'Transformers' and 'Attention mechanisms': {sim_related:.4f}\")\n",
    "print(f\"Similarity between 'Transformers' and 'Quantization':          {sim_unrelated:.4f}\")\n",
    "print(f\"\\nThe embedding space captures that transformers and attention are more related!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Vector Databases: ChromaDB\n",
    "\n",
    "A vector database stores embeddings and enables efficient similarity search. **ChromaDB** is an excellent choice for learning and prototyping because:\n",
    "\n",
    "- Simple Python API\n",
    "- Works in-memory or with persistent storage\n",
    "- Built-in embedding support\n",
    "- Metadata filtering\n",
    "- Open source\n",
    "\n",
    "### Core ChromaDB Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|------------|\n",
    "| **Client** | Connection to ChromaDB (in-memory or persistent) |\n",
    "| **Collection** | A named group of documents + embeddings (like a table) |\n",
    "| **Document** | The original text content |\n",
    "| **Embedding** | The vector representation of a document |\n",
    "| **Metadata** | Key-value pairs associated with each document |\n",
    "| **Query** | Semantic search to find similar documents |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChromaDB client (in-memory for this tutorial)\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# For persistent storage, you would use:\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "print(\"ChromaDB client created (in-memory mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection\n",
    "# We can use cosine similarity (default), L2 distance, or inner product\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"ai_ml_knowledge_base\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # use cosine similarity\n",
    ")\n",
    "\n",
    "print(f\"Collection created: {collection.name}\")\n",
    "print(f\"Distance metric: cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to the collection\n",
    "collection.add(\n",
    "    ids=[doc[\"id\"] for doc in documents],\n",
    "    documents=[doc[\"text\"] for doc in documents],\n",
    "    embeddings=embeddings.tolist(),\n",
    "    metadatas=[{\"category\": doc[\"category\"], \"source\": doc[\"source\"], \"year\": doc[\"year\"]} for doc in documents]\n",
    ")\n",
    "\n",
    "print(f\"Added {collection.count()} documents to the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic search: query the collection\n",
    "def search(query_text, n_results=3):\n",
    "    \"\"\"Search the collection for documents similar to the query.\"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query_text]).tolist()\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "query = \"How do embedding models work?\"\n",
    "results = search(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nTop {len(results['ids'][0])} results:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(len(results['ids'][0])):\n",
    "    doc_id = results['ids'][0][i]\n",
    "    distance = results['distances'][0][i]\n",
    "    similarity = 1 - distance  # cosine distance to similarity\n",
    "    text = results['documents'][0][i]\n",
    "    metadata = results['metadatas'][0][i]\n",
    "    \n",
    "    print(f\"\\n[{i+1}] ID: {doc_id} | Similarity: {similarity:.4f} | Category: {metadata['category']}\")\n",
    "    print(f\"    {text[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try several different queries\n",
    "test_queries = [\n",
    "    \"What is RAG and how does it work?\",\n",
    "    \"How can I make a model smaller to run on my laptop?\",\n",
    "    \"What are the best ways to evaluate a search system?\",\n",
    "    \"How do transformer neural networks process language?\",\n",
    "    \"What tools can AI agents use?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search(query, n_results=2)\n",
    "    top_id = results['ids'][0][0]\n",
    "    top_sim = 1 - results['distances'][0][0]\n",
    "    top_cat = results['metadatas'][0][0]['category']\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  -> Top match: {top_id} (similarity: {top_sim:.4f}, category: {top_cat})\")\n",
    "    print(f\"     {results['documents'][0][0][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a ChromaDB Collection and Perform Semantic Search\n",
    "\n",
    "Create a new collection with a different set of documents and perform searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Build your own ChromaDB collection\n",
    "\n",
    "# Here are some programming-related documents\n",
    "programming_docs = [\n",
    "    {\"id\": \"prog_01\", \"text\": \"Python is a high-level, interpreted programming language known for its clear syntax and readability. It supports multiple paradigms including procedural, object-oriented, and functional programming. Python's extensive standard library and package ecosystem (via pip) make it popular for web development, data science, automation, and AI.\", \"topic\": \"python\"},\n",
    "    {\"id\": \"prog_02\", \"text\": \"JavaScript is the language of the web, running in every modern browser. With Node.js, it can also run on servers. Key features include event-driven programming, closures, prototypal inheritance, and async/await for handling asynchronous operations. TypeScript adds static typing on top of JavaScript.\", \"topic\": \"javascript\"},\n",
    "    {\"id\": \"prog_03\", \"text\": \"Rust is a systems programming language focused on safety, speed, and concurrency. Its ownership system prevents memory errors at compile time without needing a garbage collector. Rust is used for operating systems, game engines, web assembly, and performance-critical applications.\", \"topic\": \"rust\"},\n",
    "    {\"id\": \"prog_04\", \"text\": \"Docker containers package applications with all their dependencies into standardized units. Unlike virtual machines, containers share the host OS kernel, making them lightweight and fast to start. Docker Compose allows defining multi-container applications, while Kubernetes orchestrates containers at scale.\", \"topic\": \"devops\"},\n",
    "    {\"id\": \"prog_05\", \"text\": \"Git is a distributed version control system that tracks changes in source code. Key concepts include commits (snapshots), branches (parallel development lines), merges (combining branches), and remotes (shared repositories). GitHub, GitLab, and Bitbucket provide cloud hosting for Git repositories.\", \"topic\": \"tools\"},\n",
    "    {\"id\": \"prog_06\", \"text\": \"SQL (Structured Query Language) is used to manage and query relational databases. Core operations include SELECT for reading, INSERT for adding, UPDATE for modifying, and DELETE for removing data. JOINs combine data from multiple tables. PostgreSQL and MySQL are popular open-source relational databases.\", \"topic\": \"databases\"},\n",
    "]\n",
    "\n",
    "# TODO: Create a new collection called 'programming_kb'\n",
    "prog_collection = None\n",
    "\n",
    "# TODO: Embed the programming documents using embedding_model.encode()\n",
    "prog_embeddings = None\n",
    "\n",
    "# TODO: Add documents, embeddings, and metadata to the collection\n",
    "\n",
    "# TODO: Search with these 3 queries and print results\n",
    "search_queries = [\n",
    "    \"How do I manage code versions and collaborate with others?\",\n",
    "    \"What language is best for building web applications?\",\n",
    "    \"How can I deploy my application reliably?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Exercise 2\n",
    "\n",
    "# Create a new collection\n",
    "prog_collection = chroma_client.create_collection(\n",
    "    name=\"programming_kb\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Embed the documents\n",
    "prog_texts = [doc[\"text\"] for doc in programming_docs]\n",
    "prog_embeddings = embedding_model.encode(prog_texts)\n",
    "\n",
    "# Add to collection\n",
    "prog_collection.add(\n",
    "    ids=[doc[\"id\"] for doc in programming_docs],\n",
    "    documents=prog_texts,\n",
    "    embeddings=prog_embeddings.tolist(),\n",
    "    metadatas=[{\"topic\": doc[\"topic\"]} for doc in programming_docs]\n",
    ")\n",
    "\n",
    "print(f\"Collection '{prog_collection.name}' created with {prog_collection.count()} documents\\n\")\n",
    "\n",
    "# Search\n",
    "search_queries = [\n",
    "    \"How do I manage code versions and collaborate with others?\",\n",
    "    \"What language is best for building web applications?\",\n",
    "    \"How can I deploy my application reliably?\"\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    query_emb = embedding_model.encode([query]).tolist()\n",
    "    results = prog_collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=2,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    for i in range(len(results['ids'][0])):\n",
    "        sim = 1 - results['distances'][0][i]\n",
    "        topic = results['metadatas'][0][i]['topic']\n",
    "        print(f\"  [{i+1}] {results['ids'][0][i]} (sim: {sim:.4f}, topic: {topic})\")\n",
    "        print(f\"      {results['documents'][0][i][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Distance Metrics Deep Dive\n",
    "\n",
    "Understanding distance metrics is essential for building effective retrieval systems. Let's implement them from scratch and compare their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Cosine similarity: measures the angle between two vectors.\n",
    "    Range: [-1, 1] where 1 = identical direction, 0 = orthogonal, -1 = opposite\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "def dot_product(a, b):\n",
    "    \"\"\"Dot product: combines direction AND magnitude.\n",
    "    Range: (-inf, inf) - higher is more similar\n",
    "    \"\"\"\n",
    "    return np.dot(a, b)\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Euclidean distance (L2): straight-line distance between two points.\n",
    "    Range: [0, inf) - lower is more similar\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "# Compare metrics on our embeddings\n",
    "# Select three documents: two related (transformers, attention) and one unrelated (quantization)\n",
    "emb_transformers = embeddings[0]     # doc_01: Transformer architectures\n",
    "emb_attention = embeddings[9]        # doc_10: Attention mechanisms\n",
    "emb_quantization = embeddings[13]    # doc_14: Quantization\n",
    "\n",
    "pairs = [\n",
    "    (\"Transformers vs Attention\", emb_transformers, emb_attention),\n",
    "    (\"Transformers vs Quantization\", emb_transformers, emb_quantization),\n",
    "    (\"Attention vs Quantization\", emb_attention, emb_quantization),\n",
    "]\n",
    "\n",
    "print(f\"{'Pair':<35} {'Cosine Sim':>12} {'Dot Product':>12} {'Euclidean':>12}\")\n",
    "print(\"-\" * 73)\n",
    "for name, a, b in pairs:\n",
    "    cs = cosine_similarity(a, b)\n",
    "    dp = dot_product(a, b)\n",
    "    ed = euclidean_distance(a, b)\n",
    "    print(f\"{name:<35} {cs:>12.4f} {dp:>12.4f} {ed:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Full similarity matrix using cosine similarity\n",
    "n = len(embeddings)\n",
    "sim_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        sim_matrix[i][j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(sim_matrix, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "\n",
    "# Labels\n",
    "labels = [d['id'].replace('doc_', '') for d in documents]\n",
    "short_labels = [\n",
    "    \"Transformers\", \"RAG\", \"Vector DBs\", \"Embeddings\", \"Fine-tuning\",\n",
    "    \"Chunking\", \"LLMs\", \"Cosine Sim\", \"Prompting\", \"Attention\",\n",
    "    \"RAG Eval\", \"Transfer Learn\", \"Semantic Search\", \"Quantization\", \"AI Agents\"\n",
    "]\n",
    "\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_xticklabels(short_labels, rotation=45, ha='right', fontsize=8)\n",
    "ax.set_yticklabels(short_labels, fontsize=8)\n",
    "\n",
    "plt.colorbar(im, label='Cosine Similarity')\n",
    "ax.set_title('Document Similarity Matrix', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Metric\n",
    "\n",
    "| Metric | Use When | Key Property |\n",
    "|--------|----------|-------------|\n",
    "| **Cosine Similarity** | Text embeddings, when you care about direction not magnitude | Invariant to vector length |\n",
    "| **Dot Product** | When embeddings encode importance in their magnitude | Faster than cosine (no normalization) |\n",
    "| **Euclidean Distance** | When absolute position in space matters (e.g., spatial data) | Sensitive to magnitude |\n",
    "\n",
    "**Rule of thumb:** For text similarity search, **cosine similarity** is almost always the right choice. If your embeddings are already normalized (unit vectors), cosine similarity equals the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Metadata Filtering\n",
    "\n",
    "Real-world RAG systems need more than just semantic similarity. **Metadata filtering** lets you constrain search results based on structured attributes like category, date, source, or access permissions.\n",
    "\n",
    "ChromaDB supports `where` clauses for metadata filtering and `where_document` for text content filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with metadata filters\n",
    "query = \"How do neural networks learn?\"\n",
    "query_emb = embedding_model.encode([query]).tolist()\n",
    "\n",
    "# 1. No filter (baseline)\n",
    "results_all = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"=== No filter ===\")\n",
    "for i in range(len(results_all['ids'][0])):\n",
    "    sim = 1 - results_all['distances'][0][i]\n",
    "    cat = results_all['metadatas'][0][i]['category']\n",
    "    print(f\"  [{results_all['ids'][0][i]}] sim: {sim:.4f}, category: {cat}\")\n",
    "\n",
    "# 2. Filter by category\n",
    "results_arch = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=3,\n",
    "    where={\"category\": \"architectures\"},\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Filter: category = 'architectures' ===\")\n",
    "for i in range(len(results_arch['ids'][0])):\n",
    "    sim = 1 - results_arch['distances'][0][i]\n",
    "    cat = results_arch['metadatas'][0][i]['category']\n",
    "    print(f\"  [{results_arch['ids'][0][i]}] sim: {sim:.4f}, category: {cat}\")\n",
    "\n",
    "# 3. Filter by year\n",
    "results_2024 = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=3,\n",
    "    where={\"year\": {\"$gte\": 2024}},\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Filter: year >= 2024 ===\")\n",
    "for i in range(len(results_2024['ids'][0])):\n",
    "    sim = 1 - results_2024['distances'][0][i]\n",
    "    year = results_2024['metadatas'][0][i]['year']\n",
    "    cat = results_2024['metadatas'][0][i]['category']\n",
    "    print(f\"  [{results_2024['ids'][0][i]}] sim: {sim:.4f}, year: {year}, category: {cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced filtering: combining conditions\n",
    "query = \"Best practices and tutorials\"\n",
    "query_emb = embedding_model.encode([query]).tolist()\n",
    "\n",
    "# Filter: source is tutorial AND year >= 2024\n",
    "results_filtered = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=5,\n",
    "    where={\n",
    "        \"$and\": [\n",
    "            {\"source\": \"tutorial\"},\n",
    "            {\"year\": {\"$gte\": 2024}}\n",
    "        ]\n",
    "    },\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"=== Filter: source='tutorial' AND year >= 2024 ===\")\n",
    "for i in range(len(results_filtered['ids'][0])):\n",
    "    sim = 1 - results_filtered['distances'][0][i]\n",
    "    meta = results_filtered['metadatas'][0][i]\n",
    "    print(f\"  [{results_filtered['ids'][0][i]}] sim: {sim:.4f}, source: {meta['source']}, year: {meta['year']}\")\n",
    "    print(f\"    {results_filtered['documents'][0][i][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Also show what ChromaDB filter operators are available\n",
    "print(\"\\nChromaDB filter operators:\")\n",
    "print(\"  $eq   - equals (default)\")\n",
    "print(\"  $ne   - not equals\")\n",
    "print(\"  $gt   - greater than\")\n",
    "print(\"  $gte  - greater than or equal\")\n",
    "print(\"  $lt   - less than\")\n",
    "print(\"  $lte  - less than or equal\")\n",
    "print(\"  $in   - in list\")\n",
    "print(\"  $nin  - not in list\")\n",
    "print(\"  $and  - logical AND\")\n",
    "print(\"  $or   - logical OR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Retrieval Quality Across Queries\n",
    "\n",
    "Try several queries and manually assess whether the top results are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Evaluate retrieval quality\n",
    "\n",
    "evaluation_queries = [\n",
    "    \"What is the difference between RAG and fine-tuning?\",\n",
    "    \"How do I choose a vector database?\",\n",
    "    \"What are the latest developments in language models?\",\n",
    "    \"How can I reduce hallucination in AI systems?\",\n",
    "    \"What hardware do I need to run large models?\"\n",
    "]\n",
    "\n",
    "# TODO: For each query, retrieve top-3 results and assess relevance\n",
    "# Print the query, the top-3 document IDs, their similarity scores,\n",
    "# and a brief note on whether they seem relevant\n",
    "\n",
    "relevance_scores = None  # TODO: store your relevance assessments\n",
    "\n",
    "# Expected output format:\n",
    "# Query: \"What is the difference between RAG and fine-tuning?\"\n",
    "#   [1] doc_02 (sim: 0.65) - RELEVANT (directly about RAG)\n",
    "#   [2] doc_05 (sim: 0.52) - RELEVANT (directly about fine-tuning)\n",
    "#   [3] doc_06 (sim: 0.48) - RELEVANT (about chunking, a RAG component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Exercise 3\n",
    "\n",
    "evaluation_queries = [\n",
    "    \"What is the difference between RAG and fine-tuning?\",\n",
    "    \"How do I choose a vector database?\",\n",
    "    \"What are the latest developments in language models?\",\n",
    "    \"How can I reduce hallucination in AI systems?\",\n",
    "    \"What hardware do I need to run large models?\"\n",
    "]\n",
    "\n",
    "# Ground truth: manually identified relevant doc IDs for each query\n",
    "expected_relevant = {\n",
    "    0: [\"doc_02\", \"doc_05\", \"doc_06\"],       # RAG vs fine-tuning\n",
    "    1: [\"doc_03\"],                              # vector databases\n",
    "    2: [\"doc_07\", \"doc_01\"],                    # language models\n",
    "    3: [\"doc_02\", \"doc_09\", \"doc_11\"],          # hallucination\n",
    "    4: [\"doc_14\", \"doc_05\"],                    # hardware / quantization\n",
    "}\n",
    "\n",
    "total_relevant_found = 0\n",
    "total_results = 0\n",
    "\n",
    "for idx, query in enumerate(evaluation_queries):\n",
    "    query_emb = embedding_model.encode([query]).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_emb,\n",
    "        n_results=3,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    for i in range(len(results['ids'][0])):\n",
    "        doc_id = results['ids'][0][i]\n",
    "        sim = 1 - results['distances'][0][i]\n",
    "        cat = results['metadatas'][0][i]['category']\n",
    "        is_relevant = doc_id in expected_relevant.get(idx, [])\n",
    "        relevance_tag = \"RELEVANT\" if is_relevant else \"NOT RELEVANT\"\n",
    "        \n",
    "        if is_relevant:\n",
    "            total_relevant_found += 1\n",
    "        total_results += 1\n",
    "        \n",
    "        print(f\"  [{i+1}] {doc_id} (sim: {sim:.4f}, cat: {cat}) - {relevance_tag}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Overall precision: {total_relevant_found}/{total_results} = {total_relevant_found/total_results:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Metadata Filtering\n",
    "\n",
    "Practice using metadata filters to narrow down search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Metadata filtering\n",
    "\n",
    "# TODO 1: Retrieve all documents in the 'rag' category about search or retrieval\n",
    "# Hint: use where={\"category\": \"rag\"}\n",
    "rag_results = None\n",
    "\n",
    "# TODO 2: Retrieve only documents from 'textbook' sources published in 2023\n",
    "textbook_2023 = None\n",
    "\n",
    "# TODO 3: Retrieve documents from either 'tutorial' or 'blog_post' sources\n",
    "# Hint: use $in operator: {\"source\": {\"$in\": [\"tutorial\", \"blog_post\"]}}\n",
    "informal_sources = None\n",
    "\n",
    "# Print results for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Exercise 4\n",
    "\n",
    "query = \"search and retrieval techniques\"\n",
    "query_emb = embedding_model.encode([query]).tolist()\n",
    "\n",
    "# 1. RAG category only\n",
    "rag_results = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=5,\n",
    "    where={\"category\": \"rag\"},\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"=== RAG category results ===\")\n",
    "for i in range(len(rag_results['ids'][0])):\n",
    "    sim = 1 - rag_results['distances'][0][i]\n",
    "    print(f\"  [{rag_results['ids'][0][i]}] sim: {sim:.4f} - {rag_results['documents'][0][i][:80]}...\")\n",
    "\n",
    "# 2. Textbook sources from 2023\n",
    "textbook_query = \"fundamental concepts in AI\"\n",
    "textbook_emb = embedding_model.encode([textbook_query]).tolist()\n",
    "\n",
    "textbook_2023 = collection.query(\n",
    "    query_embeddings=textbook_emb,\n",
    "    n_results=5,\n",
    "    where={\n",
    "        \"$and\": [\n",
    "            {\"source\": \"textbook\"},\n",
    "            {\"year\": 2023}\n",
    "        ]\n",
    "    },\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Textbook sources from 2023 ===\")\n",
    "for i in range(len(textbook_2023['ids'][0])):\n",
    "    meta = textbook_2023['metadatas'][0][i]\n",
    "    sim = 1 - textbook_2023['distances'][0][i]\n",
    "    print(f\"  [{textbook_2023['ids'][0][i]}] sim: {sim:.4f}, source: {meta['source']}, year: {meta['year']}\")\n",
    "\n",
    "# 3. Informal sources (tutorial or blog_post)\n",
    "informal_sources = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=5,\n",
    "    where={\"source\": {\"$in\": [\"tutorial\", \"blog_post\"]}},\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Tutorial & Blog Post sources ===\")\n",
    "for i in range(len(informal_sources['ids'][0])):\n",
    "    meta = informal_sources['metadatas'][0][i]\n",
    "    sim = 1 - informal_sources['distances'][0][i]\n",
    "    print(f\"  [{informal_sources['ids'][0][i]}] sim: {sim:.4f}, source: {meta['source']}, category: {meta['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Other Vector Databases\n",
    "\n",
    "ChromaDB is excellent for prototyping and small-to-medium workloads. For production at scale, you may need specialized solutions:\n",
    "\n",
    "| Database | Type | Best For | Key Features |\n",
    "|----------|------|----------|--------------|\n",
    "| **ChromaDB** | Open-source, embedded | Prototyping, small apps | Simple API, Python-native, in-memory or persistent |\n",
    "| **Pinecone** | Cloud-managed | Production SaaS | Fully managed, auto-scaling, hybrid search |\n",
    "| **FAISS** | Library (Facebook) | High-performance local | Extremely fast, GPU support, many index types |\n",
    "| **Weaviate** | Open-source, self-hosted | Hybrid search | GraphQL API, built-in vectorization, BM25 + vector |\n",
    "| **Qdrant** | Open-source, self-hosted | Performance-critical | Written in Rust, advanced filtering, gRPC API |\n",
    "| **Milvus** | Open-source, distributed | Large-scale production | Distributed architecture, billion-scale vectors |\n",
    "| **pgvector** | PostgreSQL extension | Existing Postgres users | Integrates with existing SQL database, familiar tooling |\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "- **Just learning/prototyping?** Use **ChromaDB** (simplest setup)\n",
    "- **Already using PostgreSQL?** Add **pgvector** extension\n",
    "- **Need a managed service?** Use **Pinecone** (zero ops)\n",
    "- **Need maximum local speed?** Use **FAISS** (raw performance)\n",
    "- **Need hybrid search (keyword + semantic)?** Use **Weaviate** or **Qdrant**\n",
    "- **Billions of vectors?** Use **Milvus** (distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Bridge to the RAG Pipeline\n",
    "\n",
    "We have now built the **retrieval** half of RAG. Here is where we stand in the full pipeline:\n",
    "\n",
    "```\n",
    "                    What we built in this notebook\n",
    "                    ==============================\n",
    "\n",
    " Documents  -->  Chunk  -->  Embed  -->  Store in     -->  Query &\n",
    "                                         Vector DB         Retrieve\n",
    " [done]         [done]      [done]       [done]           [done]\n",
    "\n",
    "\n",
    "                    What comes next (Module 10)\n",
    "                    ============================\n",
    "\n",
    " Retrieved   -->  Format      -->  Send to    -->  Generate\n",
    " Documents        as Context       LLM             Grounded Answer\n",
    " [from above]     [prompt eng]     [API call]      [final output]\n",
    "```\n",
    "\n",
    "In Module 10, we will:\n",
    "1. Build a complete RAG pipeline that connects retrieval to generation\n",
    "2. Use prompt engineering to instruct the LLM to answer based on retrieved context\n",
    "3. Handle edge cases (no relevant documents found, conflicting information)\n",
    "4. Evaluate the full system end-to-end\n",
    "\n",
    "The key insight is that **retrieval quality directly determines generation quality**. If we retrieve the wrong documents, even the best LLM will produce poor answers. That is why this module focused deeply on the retrieval foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RAG solves critical LLM limitations**: knowledge cutoff, hallucination, and lack of domain expertise\n",
    "2. **Chunking matters**: How you split documents significantly affects retrieval quality. Recursive chunking is generally the best default strategy.\n",
    "3. **Embedding models convert text to vectors**: Sentence-transformers (e.g., all-MiniLM-L6-v2) provide free, local, high-quality embeddings\n",
    "4. **Vector databases enable semantic search**: ChromaDB is excellent for learning and prototyping; production systems may need Pinecone, FAISS, or Weaviate\n",
    "5. **Cosine similarity is the standard metric** for text embedding comparison\n",
    "6. **Metadata filtering** combines semantic search with structured constraints for more precise retrieval\n",
    "\n",
    "### References\n",
    "\n",
    "- **Paper**: Lewis et al. [\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"](https://arxiv.org/abs/2005.11401) (2020) - The foundational RAG paper\n",
    "- **Paper**: Vaswani et al. [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (2017) - The transformer paper\n",
    "- **Docs**: [ChromaDB Documentation](https://docs.trychroma.com/) - Vector database used in this notebook\n",
    "- **Docs**: [Pinecone Documentation](https://docs.pinecone.io/) - Cloud vector database\n",
    "- **Docs**: [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki) - Facebook's similarity search library\n",
    "- **Docs**: [Sentence Transformers](https://www.sbert.net/) - Embedding models used in this notebook\n",
    "- **Course**: DeepLearning.AI [\"LangChain: Chat with Your Data\"](https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/) - Covers RAG with LangChain\n",
    "- **Course**: DeepLearning.AI [\"Building and Evaluating Advanced RAG Applications\"](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/) - Advanced RAG techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}